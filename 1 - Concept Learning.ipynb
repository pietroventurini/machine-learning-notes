{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pietroventurini/machine-learning-notes/blob/master/1%20-%20Concept%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtyYH5ccVQ6n"
   },
   "source": [
    "# Concept Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtyYH5ccVQ6n"
   },
   "source": [
    "Concept learning can be formulated as a problem of searching through a predefined space of potential *hypotheses* for the hypothesis that best fits the training examples. In other words, we want to infer a boolean-valued function from training examples of its input and output.\n",
    "\n",
    "Consider the following set of examples, where `EnjoySport` indicates wether or not someone enjoys his favorite water sport on that day.\n",
    "\n",
    "| Example | Sky   | AirTemp | Humidity | Wind   | Water | Forecast | EnjoySport |\n",
    "|---------|-------|---------|----------|--------|-------|----------|------------|\n",
    "| 1       | Sunny | Warm    | Normal   | Strong | Warm  | Same     | Yes        |\n",
    "| 2       | Sunny | Warm    | High     | Strong | Warm  | Same     | Yes        |\n",
    "| 3       | Rainy | Cold    | High     | Strong | Warm  | Change   | No         |\n",
    "| 4       | Sunny | Warm    | High     | Strong | Cool  | Change   | Yes        |\n",
    "\n",
    "Let each hypotesis be a vector of the six attributes *Sky, AirTemp, Humidity, Wind, Water, Forecast*. For each of them the hypotesis will either:\n",
    "- indicate by `?` that any value is acceptable for that attribute\n",
    "- specify a single required value\n",
    "- indicate by `Ø` that no value is acceptable\n",
    "\n",
    "For instance, according to the hypotesys $\\langle ?, Cold,High,?,?,?\\rangle$, the positive example would be only the third one.\n",
    "\n",
    "### Prototypical concept learning task\n",
    "\n",
    "Given:\n",
    "- **Instance** $X$: the possible days, represented by the six attributes\n",
    "- **Hypoteses** $H$: each described by a conjunction of constraints on the attributes\n",
    "- **Target concept** $c$: the function $\\text{EnjoySport}: X\\rightarrow\\{0,1\\}$\n",
    "- **Training examples**: positive/negative examples of the target function, each of the form $\\langle x,c(x) \\rangle$.\n",
    "\n",
    "Determine:\n",
    "- A hypotesis $h \\in H$ such that $h(x)=c(x) \\; \\forall x\\in X$\n",
    "\n",
    "### Inductive learning hypotesis\n",
    "In ***inductive learning*** our assumption is that the best hypothesis regarding unseen instances is the hypothesis that best fits the observed training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VduMlu78eJhv"
   },
   "source": [
    "## Concept learning as search\n",
    "The space of hypotesis ismplicitly defined by the hypothesis representation. The goal of this search is to find the hypothesis that best fits the training examples.\n",
    "In our example, the instance space $X$ contains exactly\n",
    "$3\\cdot2\\cdot2\\cdot2\\cdot2\\cdot2 = 96$ distinct instances. The number of *semantically distinct* hypotheses is  $1 + (4\\cdot3\\cdot3\\cdot3\\cdot3\\cdot3) = 973$ (by considering the placeholder `?` and that every hypothesis containing one or more `Ø` symbols represents the empty set of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDPGvXVFeMSm"
   },
   "source": [
    "## General-to-specific ordering of hypoteses\n",
    "Many algorithms for concept learning rely on the general-to-specific ordering of hypoteses. Consider this two hypoteses:\n",
    "$$h_1=\\langle Sunny, ?, ?, Strong, ?, ?\\rangle$$\n",
    "$$h_2=\\langle Sunny, ?, ?, ?, ?, ?\\rangle$$\n",
    "\n",
    "Any instance classified positive by $h_1$ will also be classified positive by $h_2$. Therefore, we say that $h_2$ is more general than $h_1$.\n",
    "\n",
    "**Definition:** Let $h_j$ and $h_k$ be boolean-valued functions defined over $X$. Then $h_j$ is ***more general-than-or-equal-to*** $h_k$ (written $h_j \\ge_g h_k$) if and only if\n",
    "\n",
    "$$(\\forall x \\in X)\\left[ (h_k(x)=1)\\rightarrow (h_j(x)=1)\\right]$$\n",
    "\n",
    "\n",
    "**Definition:** $h_j$ is ***strictly more general-than-or-equal-to*** $h_k$ (written $h_j >_g h_k$) if and only if\n",
    "\n",
    "$$(h_j\\ge_g h_k) \\land (h_j \\not \\ge_g h_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WrDt8xMeQwa"
   },
   "source": [
    "## FIND-S Algorithm\n",
    "1. Initialize $h$ to the most specific hypothesis in $H$: $h\\leftarrow \\langle \\emptyset,...,\\emptyset\\rangle$\n",
    "2. For each positive training instance $x$:  \n",
    "  2.1 For each attribute constraint $a_i$ in $h$: *If* the constraint $a_i$ is satisfied by $x$ *then* do nothing *else* replace $a_i$ in $h$ by the next more general constraint that is satisfied by $x$.\n",
    "3. Output hypothesis $h$\n",
    "\n",
    "FIND-S is guaranteed to output the most specific hypothesis within $H$\n",
    "that is consistent with the positive training examples. Anyway, there are several questions left unanswered:\n",
    "\n",
    "1. Has the learner learned the correct target concept? Although it will find an hypothesis consistent with the training data, it can't determine if it is the *only* hypothesis in $H$ consistent with the training data.  \n",
    "2. Why we should prefer the most specific hypothesis?  \n",
    "3. Are the training examples consistent? We would prefer an algorithm that could at least detect when the training data is inconsistent and, preferably, accommodate such errors.  \n",
    "4. If there are many maximally specific consistent hypotheses, which one should we choose?\n",
    "\n",
    "Given $D$, the FIND-S algorithm outputs the hypothesis $h=\\langle Sunny, Warm, ?, Strong, ?, ?\\rangle$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCpDezwTeTST"
   },
   "source": [
    "# CANDIDATE-ELIMINATION Algorithm\n",
    "The key idea in the CANDIDATE-ELIMINATION algorithm is to output a description of the set of all hypotheses consistent with the training examples.\n",
    "\n",
    "**Definition:** A hypothesis $h$ is ***consistent*** with a set of training examples $D$ if and only if $h(x)=c(x)$ for each training example $\\langle x,c(x)\\rangle$ in $D$. \n",
    "\n",
    "$$ Consistent(h,D)\\equiv (\\forall \\langle x, c(x)\\rangle \\in D) \\; h(x)=c(x)$$\n",
    "\n",
    "The CANDIDATE-ELIMINATION algorithm represents the set of all hypotheses consistent with the observed training examples. This subset is called the *version space* with respect to the hypothesis space $H$ and the training examples $D$.\n",
    "\n",
    "**Definition:** The ***version space***, denoted $VS_{H,D}$, with respect to hypothesis space $H$ and training examples $D$, is the subset of hypotheses from $H$ consistent with the training examples in $D$.\n",
    "\n",
    "$$VS_{H,D} \\equiv \\{h\\in H \\vert Consistent(h,D)\\}$$\n",
    "\n",
    "The CANDIDATE-ELIMINATION algorithm represents the version space by storing only its most general members (labeled G) and its most specific (labeled S). Given these two sets $S$ and $G$, we can enumerate all members of the version space as needed by generating the hypotheses that lie between these two sets in the general-to-specific partial ordering.\n",
    "\n",
    "**Definition:** the ***general boundary*** $G$, with respect to hypothesis space $H$ and training data $D$, is the set of maximally general members of H consistent with $D$.\n",
    "\n",
    "$$G \\equiv \\{g \\in H\\; \\vert\\; Consistent(g,D) \\land (\\neg \\exists g'\\in H)[(g'>_g g) \\land Consistent(g',D)]\\}$$\n",
    "\n",
    "**Definition:** the ***specific boundary*** $S$, with respect to hypothesis space $H$ and training data $D$, is the set of minimally general (i.e., maximally specific) members of H consistent with $D$.\n",
    "\n",
    "$$S \\equiv \\{s \\in H\\; \\vert\\; Consistent(s,D) \\land (\\neg \\exists s'\\in H)[(s>_g s') \\land Consistent(s',D)]\\}$$\n",
    "\n",
    "**Version space representation theorem:** let $X$ be an arbitrary set of instances and let $H$ be a set of boolean-valued hypotheses defined over $X$. Let $c: X \\rightarrow \\{0, 1\\}$ be an arbitrary target concept defined over $X$, and let $D$ be an arbitrary set of training examples $\\{\\langle x, c(x)\\rangle\\}$. For all $X$, $H$, $c$, and $D$ such that $S$ and $G$ are well defined,\n",
    "\n",
    "$$VS_{H,D}=\\{h\\in H \\vert (\\exists s\\in S)(\\exists g \\in G)(g\\ge_g h \\ge_g s)\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sios65zBeW60"
   },
   "source": [
    "## LIST-THEN-ELIMINATE Algorithm\n",
    "One obvious way to represent the version space is simpy to list all of its members. The *LIST-THEN-ELIMINATE* algorithm first initializes the version space containing all the hypotheses in $H$ and then eliminates any hypothesis found inconsistent with any training example. It is guaranteed to output all hypotheses consistent with the training data, but it requires to enumerate all hypotheses in $H$, that is an unrealistic requirement.\n",
    "\n",
    "1. $VersionSpace \\leftarrow$ a list containing every hypothesis in $H$.\n",
    "2. For each training example $\\langle x,c(x)\\rangle$ remove from $VersionSpace$ any hypothesis $h$ for which $h(x)\\neq c(x)$\n",
    "3. Output the list of hypotheses in $VersionSpace$\n",
    "\n",
    "<img src=\"images/concept-learning/version-space.png\" width=\"550\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNHyl9eDlTvq"
   },
   "source": [
    "## CANDIDATE-ELIMINATION Learning Algorithm\n",
    "The candidate-elimination algorithm computes the version space containing all hypotheses from $H$ that are consistent with an observed sequence of training examples.\n",
    "```\n",
    "Initialize G to the set of maximally general hypotheses in H (G := {?,?,?,?,?,?})\n",
    "Initialize S to the set of maximally specific hypotheses in H (S := {Ø,Ø,Ø,Ø,Ø,Ø})\n",
    "\n",
    "For each training example d, do:\n",
    "  If d is a positive example:\n",
    "    Remove from G any hypothesis inconsistent with d\n",
    "    For each hypothesis s in S that is not consistent with d:\n",
    "      - Remove s from S\n",
    "      - Add to S all minimal generalizations h of s such that h is consistent with d, and some member of G is more general than h\n",
    "      - Remove from S any hypothesis that is more general than another hypothesis in S\n",
    "  If d is a negative example:\n",
    "    - Remove from S any hypothesis inconsistent with d\n",
    "    For each hypothesis g in G that is not consistent with d:\n",
    "      - Remove g from G\n",
    "      - Add to G all minimal specializations h of g such that h is consistent with d, and some member of S is more specific than h\n",
    "      - Remove from G any hypothesis that is less general than another hypothesis in G\n",
    "```\n",
    "\n",
    "As each training example is considered, the $S$ and $G$ boundary sets are generalized and specialized, respectively, to eliminate from the version space any hypotheses found inconsistent with the new training example. Any hypothesis more general than $S$ will, by definition, cover any example that $S$ covers and thus will cover any past positive example. In a dual fashion, the $G$ boundary summarizes the information from previously encountered negative examples. Any hypothesis more specific than $G$ is assured to be consistent with past negative examples.\n",
    "An illustrative example can be found at page 33 of *Machine Learning - Thomas Mitchell (McGraw Hill, 1997)*.\n",
    "\n",
    "### Convergence of the CANDIDATE-ELIMINATION algorithm\n",
    "The Candidate-Elimination Algorithm will converge toward the hypothesis that correctly describes the target concept if (necessary but not sufficient condition):\n",
    "1. there are no errors in the training examples;\n",
    "2. there is an hypothesis in $H$ that correctly describes the target concept.\n",
    "\n",
    "The target concept is exactly learned when the $S$ and $G$ boundary sets converge to a single identical hypothesis.\n",
    "\n",
    "The **optimal query strategy** for a concept learner (to ask to an oracle) is to generate instances that satisfy exactly **half** of the hypothesis in the currentversion space.\n",
    "\n",
    "If this is possible, we can find the target concept with $\\left\\lceil {\\log_2|VS|} \\right \\rceil$ queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rU0N5eKFFPB9"
   },
   "source": [
    "## Classification of new instances\n",
    "To classify a new instance as positive, it is sufficient to verify that the instance is classified as positive by **every** member of $S$.\n",
    "\n",
    "To classify a new instance as negative, it is sufficient to verify that the instance is classified as negative by **every** member of $G$.\n",
    "\n",
    "If there are some hypotheses that classify the instance as positive and some others that classify it as negative, we can take, for example, a ***majority vote***, selecting the most frequent outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbpIaf32fPpD"
   },
   "source": [
    "## Biased and Unbiased learner\n",
    "Suppose we wish to assure that the hypothesis space contains the unknown target\n",
    "concept. The obvious solution is to enrich the hypothesis space to include\n",
    "every possible hypothesis. If we restrict the hypothesis space to include only conjunctions of attribute values, the hypothesis space is unable to represent\n",
    "even simple disjunctive target concepts such as \"*Sky = Sunny or Sky = Cloudy*\". The choice of such a representation for the hypotheses represents the **inductive bias** of the learner.\n",
    "\n",
    "The solution to the problem of assuring that the target concept is in the hypothesis space $H$ is to provide a hypothesis space capable of representing every teachable concept (the set of all subsets of $X$, i.e. the *power set* of $X$, with dimension $2^{|X|}$).\n",
    "\n",
    "However, while this hypothesis space eliminates any problems of expressibility, it unfortunately raises a new, equally difficult problem: our concept learning algorithm is now completely unable to generalize beyond the observed examples (the $S$ boundary will always be simply the disjunction of the observed positive examples, while the $G$ boundary will always be the negated disjunction of the observed negative examples). \n",
    "\n",
    "**Property of inductive inference:** A learner that makes no a priori assumptions regarding the identity of the target concept has no rational basis for classifying any unseen instances.\n",
    "\n",
    "Our previous learners were able to correctly classify new instances (if there are no errors in the training set and the target concept is included in $H$) because of the assumption that the target concept can be represented as conjunction of attributes. If this assumption is incorrect, at least some instances will be erroneously classified.\n",
    "\n",
    "- The **inductive bias** of the Candidate-Elimination Algorithm using hypothesis space $H$ is that the target concept $c$ is included in $H$.\n",
    "- The **inductive bias** of the Find-S Algorithm using hypothesis space $H$ is that the target concept $c$ is included inHand that the solution is a maximally specific hypothesis (stronger inductive bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RSGoojorb2u"
   },
   "source": [
    "# Overfitting\n",
    "**Definition:** Given a hypothesis space $H$, a hypothesis $h \\in H$ is said to overfit the training data if there exists some alternative hypothesis $h' \\in H$ such that $h$ has smaller error than $h'$ over the training examples, but $h'$ has a smaller error than $h$ over the entire distribution of instances."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1 - Concept Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
