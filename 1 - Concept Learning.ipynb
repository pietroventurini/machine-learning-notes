{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pietroventurini/machine-learning-notes/blob/master/1%20-%20Concept%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtyYH5ccVQ6n"
   },
   "source": [
    "# Concept Learning\n",
    "### Contents\n",
    "\n",
    "1. **Introduction**  \n",
    "    1.1 Prototypical concept learning task  \n",
    "    1.2 The inductive learning hypothesis  \n",
    "    1.3 Concept learning as search  \n",
    "    1.4 General-to-specific ordering of hypotheses  \n",
    "2. **Concept learning as search**  \n",
    "    2.1. Find-S algorithm  \n",
    "    2.3. Candidate elimination algorithm  \n",
    "    2.4. List-then-eliminate algorithm    \n",
    "5. **Biased and unbiased learners** \n",
    "6. **Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtyYH5ccVQ6n"
   },
   "source": [
    "## Introduction\n",
    "**Concept learning** consists into inferring a boolean-valued function from training examples of its input and output. In other words, it can be formulated as the problem of searching through a predefined space of potential hypotheses for the hypothesis that best fits the training examples. A **hypothesis** is described by a conjuction of constraints on the attributes of the problem's instances.\n",
    "\n",
    "Consider the following set of examples, where `EnjoySport` indicates whether or not someone enjoys his favorite water sport on that day. The task is to learn to predict the value of `EnjoySport` for an arbitrary day, based on the values of its other attributes.\n",
    "\n",
    "| Example | Sky   | AirTemp | Humidity | Wind   | Water | Forecast | EnjoySport |\n",
    "|---------|-------|---------|----------|--------|-------|----------|------------|\n",
    "| 1       | Sunny | Warm    | Normal   | Strong | Warm  | Same     | Yes        |\n",
    "| 2       | Sunny | Warm    | High     | Strong | Warm  | Same     | Yes        |\n",
    "| 3       | Rainy | Cold    | High     | Strong | Warm  | Change   | No         |\n",
    "| 4       | Sunny | Warm    | High     | Strong | Cool  | Change   | Yes        |\n",
    "\n",
    "Let each hypothesis be a vector of the six attributes *[Sky, AirTemp, Humidity, Wind, Water, Forecast]*. For each of them the hypothesis will either:\n",
    "- indicate by `?` that any value is acceptable for that attribute\n",
    "- specify a single required value\n",
    "- indicate by `Ø` that no value is acceptable\n",
    "\n",
    "An instance $x$ for which all the constraints of a certain hypothesis are satisfied, will be classified as positive, meaning $h(x)=1$. For instance, according to  hypothesis $\\langle ?, Cold,High,?,?,?\\rangle$, only the third example would be classified as positive. The most general hypothesis is $\\langle?,?,?,?,?,?\\rangle$, while the most specific one is $\\langle\\emptyset,\\emptyset,\\emptyset,\\emptyset,\\emptyset,\\emptyset\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtyYH5ccVQ6n"
   },
   "source": [
    "### Prototypical concept learning task\n",
    "\n",
    "**Given**:\n",
    "- **Instances** $X$: the possible days, represented by the six attributes,\n",
    "- **Hypotheses** $H$: each described by a conjunction of constraints on the attributes,\n",
    "- **Target concept** $c$: the function $\\text{EnjoySport}: X\\rightarrow\\{0,1\\}$,\n",
    "- **Training examples**: positive/negative examples of the target function, each of the form $\\langle x,c(x) \\rangle$.\n",
    "\n",
    "**Determine**:\n",
    "- A hypothesis $h \\in H$ such that $h(x)=c(x) \\; \\forall x\\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtyYH5ccVQ6n"
   },
   "source": [
    "### The inductive learning hypothesis\n",
    "In ***inductive learning*** our assumption is that the best hypothesis regarding unseen instances is the hypothesis that best fits the observed training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VduMlu78eJhv"
   },
   "source": [
    "# Concept learning as search\n",
    "The space of hypothesis, thus the space of all hypotheses that the program can ever learn, is implicitly defined by the hypothesis representation we choose. The goal of this search is to find the hypothesis that best fits the training examples.\n",
    "\n",
    "In our example, the instance space $X$ contains exactly\n",
    "$3\\cdot2\\cdot2\\cdot2\\cdot2\\cdot2 = 96$ distinct instances. The number of *semantically distinct* hypotheses is  $1 + (4\\cdot3\\cdot3\\cdot3\\cdot3\\cdot3) = 973$ (by considering the placeholder `?` and that every hypothesis containing one or more `Ø` symbols represents the empty set of instances, therefore it classifies every instance as negative. Practical learning tasks can involve infinite hypothesis spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDPGvXVFeMSm"
   },
   "source": [
    "### General-to-specific ordering of hypotheses\n",
    "Many algorithms for concept learning rely on the general-to-specific ordering of hypotheses. Consider this two hypotheses:\n",
    "$$h_1=\\langle Sunny, ?, ?, Strong, ?, ?\\rangle$$\n",
    "$$h_2=\\langle Sunny, ?, ?, ?, ?, ?\\rangle$$\n",
    "\n",
    "Any instance classified positive by $h_1$ will also be classified positive by $h_2$. Therefore, we say that $h_2$ is more general than $h_1$.\n",
    "\n",
    "**Definition:** Let $h_j$ and $h_k$ be boolean-valued functions defined over $X$. Then $h_j$ is ***more general-than-or-equal-to*** $h_k$ (written $h_j \\ge_g h_k$) if and only if\n",
    "\n",
    "$$(\\forall x \\in X)\\left[ (h_k(x)=1)\\rightarrow (h_j(x)=1)\\right]$$\n",
    "\n",
    "\n",
    "**Definition:** $h_j$ is ***strictly more general-than-or-equal-to*** $h_k$ (written $h_j >_g h_k$) if and only if\n",
    "\n",
    "$$(h_j\\ge_g h_k) \\land (h_k \\not \\ge_g h_j)$$\n",
    "\n",
    "#### Example\n",
    "Consider the following three hypotheses from the EnjoySports example:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\begin{split}\n",
    "        &h_1 = \\langle Sunny, ?, ?, Strong, ?, ?\\rangle \\\\\n",
    "        &h_2 = \\langle Sunny, ?, ?, ?, ?, ?\\rangle \\\\\n",
    "        &h_3 = \\langle Sunny, ?, ?, ?, Cool, ?\\rangle\n",
    "    \\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "and the instances:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\begin{split}\n",
    "        &x_1 = \\langle Sunny, Warm, High, Strong, Cool, Same\\rangle \\\\\n",
    "        &x_2 = \\langle Sunny, Warm, High, Light, Warm, Same\\rangle \\\\\n",
    "    \\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "It holds that $h_2 \\ge_g h_1$ and $h_2\\ge_g h_3$. Neither $h_1$ nor $h_3$ is more general than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WrDt8xMeQwa"
   },
   "source": [
    "## FIND-S Algorithm\n",
    "Find-S begins with the most specific possible hypothesis in $H$, then generalize this hypothesis each time it fails to cover an observed positive training example.\n",
    "\n",
    "1. Initialize $h$ to the most specific hypothesis in $H$: $h\\leftarrow \\langle \\emptyset,...,\\emptyset\\rangle$\n",
    "2. For each positive training instance $x$:  \n",
    "    - For each attribute constraint $a_i$ in $h$:  \n",
    "        - *If* the constraint $a_i$ is satisfied by $x$ *then* do nothing *else* replace $a_i$ in $h$ by the next more general constraint that is satisfied by $x$.\n",
    "3. Output hypothesis $h$\n",
    "\n",
    "FIND-S is guaranteed to output the most specific hypothesis within $H$\n",
    "that is consistent with the positive training examples. At each iteration, the current hypothesis is the most specific one which his consistent with the instances observed up to that point. \n",
    "\n",
    "Anyway, there are several questions left unanswered:\n",
    "\n",
    "1. Has the learner learned the correct target concept? Although it will find a hypothesis consistent with the training data, it can't determine if it is the *only* hypothesis in $H$ consistent with the training data.  \n",
    "2. Why we should prefer the most specific hypothesis?  \n",
    "3. Are the training examples consistent? We would prefer an algorithm that could at least detect when the training data is inconsistent and, preferably, accommodate such errors.  \n",
    "4. If there are many maximally specific consistent hypotheses, which one should we choose?\n",
    "\n",
    "Given $D$, the FIND-S algorithm outputs the hypothesis $h=\\langle Sunny, Warm, ?, Strong, ?, ?\\rangle$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCpDezwTeTST"
   },
   "source": [
    "## CANDIDATE-ELIMINATION Algorithm\n",
    "Differently from Find-S, which outputs just one of possibly many hypotheses consistent with the training set, CANDIDATE-ELIMINATION outputs a description of the set of all hypotheses consistent with the training examples.\n",
    "\n",
    "**Definition:** A hypothesis $h$ is ***consistent*** with a set of training examples $D$ if and only if $h(x)=c(x)$ for each training example $\\langle x,c(x)\\rangle$ in $D$. \n",
    "\n",
    "$$ Consistent(h,D)\\equiv (\\forall \\langle x, c(x)\\rangle \\in D) \\; h(x)=c(x)$$\n",
    "\n",
    "The CANDIDATE-ELIMINATION algorithm represents the set of all hypotheses consistent with the observed training examples. This subset is called the **version space** with respect to the hypothesis space $H$ and the training examples $D$.\n",
    "\n",
    "**Definition:** The ***version space***, denoted $VS_{H,D}$, with respect to hypothesis space $H$ and training examples $D$, is the subset of hypotheses from $H$ consistent with the training examples in $D$.\n",
    "\n",
    "$$VS_{H,D} \\equiv \\{h\\in H \\vert Consistent(h,D)\\}$$\n",
    "\n",
    "The CANDIDATE-ELIMINATION algorithm represents the version space by storing only its most general members (labeled `G`) and its most specific (labeled `S`). Given these two sets $S$ and $G$, we can enumerate all members of the version space as needed by generating the hypotheses that lie between these two sets in the general-to-specific partial ordering.\n",
    "\n",
    "**Definition:** the ***general boundary*** $G$, with respect to hypothesis space $H$ and training data $D$, is the set of maximally general members of $H$ consistent with $D$.\n",
    "\n",
    "$$G \\equiv \\{g \\in H\\; \\vert\\; Consistent(g,D) \\land (\\neg \\exists g'\\in H)[(g'>_g g) \\land Consistent(g',D)]\\}$$\n",
    "\n",
    "**Definition:** the ***specific boundary*** $S$, with respect to hypothesis space $H$ and training data $D$, is the set of minimally general (i.e., maximally specific) members of $H$ consistent with $D$.\n",
    "\n",
    "$$S \\equiv \\{s \\in H\\; \\vert\\; Consistent(s,D) \\land (\\neg \\exists s'\\in H)[(s>_g s') \\land Consistent(s',D)]\\}$$\n",
    "\n",
    "**Version space representation theorem:** let $X$ be an arbitrary set of instances and let $H$ be a set of boolean-valued hypotheses defined over $X$. Let $c: X \\rightarrow \\{0, 1\\}$ be an arbitrary target concept defined over $X$, and let $D$ be an arbitrary set of training examples $\\{\\langle x, c(x)\\rangle\\}$. For all $X$, $H$, $c$, and $D$ such that $S$ and $G$ are well defined,\n",
    "\n",
    "$$VS_{H,D}=\\{h\\in H \\vert (\\exists s\\in S)(\\exists g \\in G)(g\\ge_g h \\ge_g s)\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sios65zBeW60"
   },
   "source": [
    "### LIST-THEN-ELIMINATE Algorithm\n",
    "One obvious way to represent the version space is simpy to list all of its members. The *LIST-THEN-ELIMINATE* algorithm first initializes the version space containing all the hypotheses in $H$ and then eliminates any hypothesis found inconsistent with any training example. It is guaranteed to output all hypotheses consistent with the training data, but it requires to enumerate all hypotheses in $H$, that is an unrealistic requirement.\n",
    "\n",
    "1. $VersionSpace \\leftarrow$ a list containing every hypothesis in $H$.\n",
    "2. For each training example $\\langle x,c(x)\\rangle$ remove from $VersionSpace$ any hypothesis $h$ for which $h(x)\\neq c(x)$\n",
    "3. Output the list of hypotheses in $VersionSpace$\n",
    "\n",
    "<img src=\"images/concept-learning/version-space.png\" width=\"550\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNHyl9eDlTvq"
   },
   "source": [
    "## CANDIDATE-ELIMINATION Learning Algorithm\n",
    "The candidate-elimination algorithm computes the version space containing all hypotheses from $H$ that are consistent with an observed sequence of training examples.\n",
    "```python\n",
    "Initialize G to the set of maximally general hypotheses in H (G := {?,?,?,?,?,?})\n",
    "Initialize S to the set of maximally specific hypotheses in H (S := {Ø,Ø,Ø,Ø,Ø,Ø})\n",
    "\n",
    "for each training example d, do:\n",
    "  if d is a positive example:\n",
    "    Remove from G any hypothesis inconsistent with d\n",
    "    For each hypothesis s in S that is not consistent with d:\n",
    "      - Remove s from S\n",
    "      - Add to S all minimal generalizations h of s such that h is consistent with d, and some member of G is more general than h\n",
    "      - Remove from S any hypothesis that is more general than another hypothesis in S\n",
    "  if d is a negative example:\n",
    "    - Remove from S any hypothesis inconsistent with d\n",
    "    for each hypothesis g in G that is not consistent with d:\n",
    "      - Remove g from G\n",
    "      - Add to G all minimal specializations h of g such that h is consistent with d, and some member of S is more specific than h\n",
    "      - Remove from G any hypothesis that is more specific than another hypothesis in G\n",
    "```\n",
    "\n",
    "As each training example is considered, the $S$ and $G$ boundary sets are generalized and specialized, respectively, to eliminate from the version space any hypotheses found inconsistent with the new training example. Positive training examples may force the $S$ boundary of the version space to become increasingly general. Negative training examples play the complimentary role of forcing the $G$ boundary to become increasingly specific. Any hypothesis more general than $S$ will, by definition, cover any example that $S$ covers and thus will cover any past positive example. Any hypothesis more specific than $G$ is assured to be consistent with past negative examples.\n",
    "\n",
    "An illustrative example can be found at page 33 of *Machine Learning - Thomas Mitchell (McGraw Hill, 1997)*.\n",
    "\n",
    "### Convergence of the CANDIDATE-ELIMINATION algorithm\n",
    "The Candidate-Elimination Algorithm will converge toward the hypothesis that correctly describes the target concept if (necessary but not sufficient condition):\n",
    "1. there are no errors in the training examples;\n",
    "2. there is a hypothesis in $H$ that correctly describes the target concept.\n",
    "\n",
    "The target concept is exactly learned when the $S$ and $G$ boundary sets converge to a single identical hypothesis.\n",
    "\n",
    "The **optimal query strategy** for a concept learner (to ask to an oracle) is to generate instances that satisfy exactly **half** of the hypothesis in the currentversion space.\n",
    "\n",
    "If this is possible, we can find the target concept with $\\left\\lceil {\\log_2|VS|} \\right \\rceil$ queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rU0N5eKFFPB9"
   },
   "source": [
    "## Classification of new instances\n",
    "To classify a new instance as positive, it is sufficient to verify that the instance is classified as positive by **every** member of $S$.\n",
    "\n",
    "To classify a new instance as negative, it is sufficient to verify that the instance is classified as negative by **every** member of $G$.\n",
    "\n",
    "If there are some hypotheses that classify the instance as positive and some others that classify it as negative, we can take, for example, a ***majority vote***, selecting the most frequent outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbpIaf32fPpD"
   },
   "source": [
    "## Biased and Unbiased learner\n",
    "Suppose we wish to assure that the hypothesis space contains the unknown target\n",
    "concept. The obvious solution is to enrich the hypothesis space to include\n",
    "every possible hypothesis. If we restrict the hypothesis space to include only conjunctions of attribute values, the hypothesis space is unable to represent\n",
    "even simple disjunctive target concepts such as \"*Sky = Sunny or Sky = Cloudy*\". The choice of such a representation for the hypotheses represents the **inductive bias** of the learner.\n",
    "\n",
    "The solution to the problem of assuring that the target concept is in the hypothesis space $H$ is to provide a hypothesis space capable of representing every teachable concept (the set of all subsets of $X$, i.e. the *power set* of $X$, with dimension $2^{|X|}$).\n",
    "\n",
    "However, while this hypothesis space eliminates any problems of expressibility, it unfortunately raises a new, equally difficult problem: our concept learning algorithm is now completely unable to generalize beyond the observed examples (the $S$ boundary will always be simply the disjunction of the observed positive examples, while the $G$ boundary will always be the negated disjunction of the observed negative examples). \n",
    "\n",
    "**Property of inductive inference:** A learner that makes no a priori assumptions regarding the identity of the target concept has no rational basis for classifying any unseen instances.\n",
    "\n",
    "Our previous learners were able to correctly classify new instances (if there are no errors in the training set and the target concept is included in $H$) because of the assumption that the target concept can be represented as conjunction of attributes. If this assumption is incorrect, at least some instances will be erroneously classified.\n",
    "\n",
    "- The **inductive bias** of the Candidate-Elimination Algorithm using hypothesis space $H$ is that the target concept $c$ is included in $H$.\n",
    "- The **inductive bias** of the Find-S Algorithm using hypothesis space $H$ is that the target concept $c$ is included in $H$ and that the solution is a maximally specific hypothesis (stronger inductive bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RSGoojorb2u"
   },
   "source": [
    "# Overfitting\n",
    "Given an hypothesis, we call *training error*, and denote it with $error_{train}(h)$, the error on the training set, while we call *generalization error*, and denote it with $error_{D}(h)$, the error  made by hypothesis $h$ on the entire distribution $D$.\n",
    "\n",
    "**Definition:** Given a hypothesis space $H$, a hypothesis $h \\in H$ is said to overfit the training data if there exists some alternative hypothesis $h' \\in H$ such that $h$ has smaller error than $h'$ over the training examples, but $h'$ has a smaller error than $h$ over the entire distribution of instances.\n",
    "\n",
    "$$error_{train}(h) < error_{train}(h') \\quad \\wedge \\quad error_{D}(h) > error_{D}(h')$$\n",
    "$$accuracy_{train}(h) > accuracy_{train}(h') \\quad \\wedge \\quad accuracy_{D}(h) < accuracy_{D}(h')$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1 - Concept Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
