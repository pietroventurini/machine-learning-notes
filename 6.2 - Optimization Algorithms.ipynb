{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "6.2 - Optimization Algorithms.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pietroventurini/machine-learning-notes/blob/main/6.2%20-%20Optimization%20Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olive-patio"
      },
      "source": [
        "# Optimization Algorithms\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. [**Prerequisites**](#Prerequisites)  \n",
        "    1. [Exponentially Weighted Average](#Exponentially-Weighted-Average)\n",
        "2. [**Optimization Algorithms**](#Optimization-Algorithms)  \n",
        "    1. [Momentum](#Momentum)  \n",
        "    2. [Nesterov Accelerated Gradient](#Nesterov-Accelerated-Gradient)  \n",
        "    3. [RMSProp](#RMSProp)  \n",
        "    4. [Adam](#Adam) \n",
        "3. [**Common Practices**](#Common-Practices)\n",
        "    1. [Dynamic learning rate](#Dynamic-learning-rate)  \n",
        "    2. [Tuning hyperparameters](#Tuning-hyperparameters)  "
      ],
      "id": "olive-patio"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "removable-organizer"
      },
      "source": [
        "<a name='Prerequisites'></a>\n",
        "# Prerequisites\n",
        "\n",
        "In order to understand the optimization algorithms that we'll present, we need first to understand how exponentially weighted average works."
      ],
      "id": "removable-organizer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mental-accident"
      },
      "source": [
        "<a name='Exponentially-Weighted-Average'></a>\n",
        "## Exponentially Weighted Average\n",
        "\n",
        "Consider a set of data $\\left\\{x_1,\\dots, x_m \\right\\}$ describing the daily temperature in a city over a time span. The time series may look a little bit noisy, so, in order to highlight the trends, we could compute an exponentially weighted moving average, which can be done using the following recursive formulation:\n",
        "\n",
        "$$v_t = \\beta v_{t-1} + (1-\\beta)x_t$$\n",
        "\n",
        "which can be initialized with $v_0=0$. We can think at the term $v_t$ as approximately averaging over $\\frac{1}{1-\\beta}$ days temperature, for instance, if $\\beta=0.9$, that is like averaging over the last 10 days temperature. If we average over a larger window (e.g. $\\beta=0.98$, which corresponds approximatively to 50 days), the exponentially weighted average adapts more slowly to changes in the data, and that is because, when $\\beta$ is very close to one, we are giving more importance to $v_{t-1}$ rather than to the current data $x_t$.\n",
        "\n",
        "The recursive formulation is an efficient way to update the moving average, without the need to keep in memory all the previous data."
      ],
      "id": "mental-accident"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rural-algorithm"
      },
      "source": [
        "### Bias correction\n",
        "\n",
        "Assume $\\beta=0.98$ and consider what would happen with the first bunch of averages:\n",
        "\n",
        "- $v_0 = 0$\n",
        "- $v_1 = \\beta v_0 + (1 - \\beta)x_1 = 0.02 x_1$\n",
        "- $v_2 = \\beta v_1 + (1 - \\beta)x_2 = 0.98\\cdot 0.02 x_1 + 0.02 x_2 = 0.0196x_1 + 0.02x_2$\n",
        "\n",
        "We can see that, in the moment we combine the first two measurements, their weighted average is far less than any of the two values (Since we are computing a linear combination weighted by very small coefficients). In order to adress this issue and compute a more accurate estimate, we can rescale $v_t$ by $\\frac{1}{1-\\beta^t}$. So, when $t=2$ we would have $1-\\beta^2 = 1 - 0.98^2 = 0.0396$ and we would compute, instead of $v_2$:\n",
        "\n",
        "$$\\frac{v_2}{1-\\beta^2} = \\frac{0.0196x_1 + 0.02x_2}{0.0396}$$\n",
        "\n",
        "As you might have noticed, the sum of the coefficients at the numerator is equal to the denominator. This operation, sometimes called _bias correction_ can help exponentially weighted moving average being more accurate, especially with the initial estimates."
      ],
      "id": "rural-algorithm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concrete-wallet"
      },
      "source": [
        "<a name='Optimization-Algorithms'></a>\n",
        "# Optimization Algorithms\n",
        "\n",
        "We can use variants of the stochastic or mini-batch gradient descent that may help to speed up learning and escape local minima. One of these techniques is the gradient descent with momentum. Two other popular optimization algorithms that have been shown to work well on a wide range of neural network architectures are RMSProp and Adam."
      ],
      "id": "concrete-wallet"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "authentic-tulsa"
      },
      "source": [
        "<a name='Momentum'></a>\n",
        "## Momentum\n",
        "\n",
        "There exist several heuristics that can be used to avoid getting stuck in local minima and may help accelerate the learning. For example we can include a **weight momentum** in the weight update. The basic idea is to compute an **exponentially weighted average** (governed by a parameter $\\beta$) of the gradients and to use it to update our weights instead. Assume we are performing mini-batch gradient descent, and consider the update step for the $l$-th layer (we'll omit the superscript denoting the layer in the next lines). First we compute the derivatives $\\mathrm{d}W$ and $\\mathrm{d}\\mathbf{b}$ of the loss function with respect to that layer's weights, on the current mini-batch. Then we update the weights in this way:\n",
        "\n",
        "$$V_{\\mathrm{d}W} = \\beta V_{\\mathrm{d}W} + (1-\\beta) \\mathrm{d}W$$\n",
        "\n",
        "$$V_{\\mathrm{d}b} = \\beta V_{\\mathrm{d}b} + (1-\\beta) \\mathrm{d}b$$\n",
        "\n",
        "$$W = W - \\alpha V_{\\mathrm{d}W}$$\n",
        "\n",
        "$$b = b - \\alpha V_{\\mathrm{d}b}$$\n",
        "\n",
        "In this way we can smooth out the steps of gradient descent and follow a more straightforward path. Weight momentum can also be interpreted by making an analogy with physics: the current value of the weights represents the position of a physical object rolling down a surface (defined by the cost function). The term $\\mathrm{d}W$ can be thought as an acceleration and the term $V_{\\mathrm{d}W}$ as a velocity. The hyperparameter $\\beta$, which is smaller than $1$, represents a _friction_ and prevents the object from speeding up without limit, but rather than taking each step independently from the previously taken ones, the object can gain momentum from and, eventually, escape a local minimum.\n",
        "\n",
        "In practice $V_{\\mathrm{d}W}$ is a matrix of the same dimension of $W$, $V_{\\mathrm{d}b}$ is a vector of the same dimension of $b$, and they are both initialized to zero. Tipically, $\\beta$ is initialized to some value around $0.9$."
      ],
      "id": "authentic-tulsa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "breathing-bracket"
      },
      "source": [
        "<a name='Nesterov-Accelerated-Gradient'></a>\n",
        "## Nesterov Accelerated Gradient\n",
        "\n",
        "Nesterov Accelerated Gradient is slighlty different than momentum in the sense that we kind of \"look into the future\" to see how much momentum is required. The update equations become:\n",
        "\n",
        "$$\\delta^{(k+1)} = -\\eta \\nabla L(W^{(k)} + \\alpha \\delta^{(k)}) + \\alpha\\delta^{(k)}$$\n",
        "\n",
        "$$W^{(k+1)} = W^{(k)} + \\delta^{(k+1)}$$\n",
        "\n",
        "The same holds also for $b$. With Nesterov momentum, first we move in the previous accumulated gradient computed the iteration before (from $W^{(k)}$ to $W^{(k)} + \\alpha\\delta^{(k)}$, then we compute the gradient in that point ($-\\eta \\nabla L(W^{(k)} + \\alpha \\delta^{(k)})$) and finally make a correction.\n",
        "\n",
        "<img src=\"https://github.com/pietroventurini/machine-learning-notes/blob/main/images/neural_networks/Nesterov.png?raw=1\" style=\"width:40em; display: block; margin-left: auto; margin-right: auto;\" />"
      ],
      "id": "breathing-bracket"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monetary-briefs"
      },
      "source": [
        "<a name='RMSProp'></a>\n",
        "## RMSProp\n",
        "\n",
        "Root Mean Square Propagation ([RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)) is quite similar to weight momentum, but it divides the gradient by a running weighted average. The main reason behind this method is that the fact that the magnitude of the gradient for different weights may change during the learning makes it difficult to choose a single global learning rate.\n",
        "\n",
        "$$S_{\\mathrm{d}W} = \\beta_2 S_{\\mathrm{d}W} + (1-\\beta_2) \\mathrm{d}W^2$$\n",
        "\n",
        "$$S_{\\mathrm{d}b} = \\beta_2 S_{\\mathrm{d}b} + (1-\\beta_2) \\mathrm{d}b^2$$\n",
        "\n",
        "$$W = W - \\alpha \\frac{\\mathrm{d}W}{\\sqrt{S_{\\mathrm{d}W}}+\\varepsilon}$$\n",
        "\n",
        "$$b = b - \\alpha \\frac{\\mathrm{d}b}{\\sqrt{S_{\\mathrm{d}b}}+\\varepsilon}$$\n",
        "\n",
        "Note that the squaring operation is computed element-wise, and we used different letters $S_{\\mathrm{d}W}$ and $\\beta_2$ than with momentum, because with Adam we are going to combine both momentum and RMSProp. $\\varepsilon$ is a small term (in practice we can set it to $10^{-8}$) that prevents dividing by some quantity close to zero that would make the numerator explode. Note also that, differently from weight momentum, here we dump out the oscillations by dividing by a term which is proportional to the width of the step (to the derivative of the loss function with respect to $W$ or $b$), therefore, wider steps (steps in a direction in which the function is steeper) will be dump out more than narrow steps (steps in a direction in which the function is flatter). In other words, every parameter is weighted by a different learning rate.\n",
        "\n",
        "A consequence of this is that we can use a larger learning rate $\\alpha$ without diverging in the steepest directions."
      ],
      "id": "monetary-briefs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mediterranean-latin"
      },
      "source": [
        "<a name='Adam'></a>\n",
        "## Adam\n",
        "\n",
        "Adaptive moment estimation (Adam) puts together weight momentum and RMSProp. The parameters are initialized to zero.\n",
        "\n",
        "$$V_{\\mathrm{d}W}=0, \\quad S_{\\mathrm{d}W}=0, \\quad V_{\\mathrm{d}b}=0, \\quad S_{\\mathrm{d}b}=0.$$\n",
        "\n",
        "Then, on iteration $t$:\n",
        "- Compute $\\mathrm{d}W$ and $\\mathrm{d}b$ using the current mini-batch \n",
        "- Compute Momentum exponentially weighted average:  \n",
        "    - $V_{\\mathrm{d}W}=\\beta_1 V_{\\mathrm{d}W} + (1-\\beta_1)\\mathrm{d}W$  \n",
        "    - $V_{\\mathrm{d}b}=\\beta_1 V_{\\mathrm{d}b} + (1-\\beta_1)\\mathrm{d}b$  \n",
        "- Compute the RMSProp update terms:\n",
        "    - $S_{\\mathrm{d}W}=\\beta_2 S_{\\mathrm{d}W} + (1-\\beta_2)\\mathrm{d}W^2$  \n",
        "    - $S_{\\mathrm{d}b}=\\beta_2 S_{\\mathrm{d}b} + (1-\\beta_2)\\mathrm{d}b^2$  \n",
        "- Perform the bias correction:\n",
        "    - $V_{\\mathrm{d}W}^{corr} = \\frac{V_{\\mathrm{d}W}}{1-\\beta_1^t}$  \n",
        "    - $V_{\\mathrm{d}b}^{corr} = \\frac{V_{\\mathrm{d}b}}{1-\\beta_1^t}$  \n",
        "    - $S_{\\mathrm{d}W}^{corr} = \\frac{S_{\\mathrm{d}W}}{1-\\beta_2^t}$  \n",
        "    - $S_{\\mathrm{d}W}^{corr} = \\frac{S_{\\mathrm{d}W}}{1-\\beta_2^t}$  \n",
        "- Update the weights:\n",
        "    - $W := W - \\alpha \\frac{V_{\\mathrm{d}W}^{corr}}{\\sqrt{S_{\\mathrm{d}W}^{corr}}+\\varepsilon}$\n",
        "    - $b := b - \\alpha \\frac{V_{\\mathrm{d}b}^{corr}}{\\sqrt{S_{\\mathrm{d}b}^{corr}}+\\varepsilon}$\n",
        "    \n",
        "This algorithm has some hyperparameters that have to be tuned, and others that are tipically initialized to some common values:\n",
        "- Learning rate $\\alpha$ needs to be tuned (we could try few values and choose the one yielding the best result, or adopt learning rate decay).\n",
        "- $\\beta_1 = 0.9$\n",
        "- $\\beta_2 = 0.999$\n",
        "- $\\varepsilon = 10^{-8}$"
      ],
      "id": "mediterranean-latin"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rising-colony"
      },
      "source": [
        "<a name='Common-practices'></a>\n",
        "# Common practices"
      ],
      "id": "rising-colony"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nervous-syndicate"
      },
      "source": [
        "<a name='Dynamic-learning-rate'></a>\n",
        "## Dynamic learning rate\n",
        "\n",
        "Instead of using a fixed, chosen a priori, learning rate, $\\alpha$ is often replaced by a learning rate that decreases over time, for example:\n",
        "\n",
        "$$\\alpha = \\frac{\\alpha_0}{1 + \\text{decay_rate} \\cdot \\text{epoch_num}}$$\n",
        "\n",
        "where $\\alpha_0$ is the initial learning rate.\n",
        "\n",
        "There exist other learning rate decay methods, for instance:\n",
        "\n",
        "- **Exponential decay**:\n",
        "\n",
        "$$\\alpha = k^{\\text{epoch_num}}\\cdot \\alpha_0$$\n",
        "\n",
        "where $k$ is a constant, e.g. $k=0.95$.\n",
        "\n",
        "- **Based on epoch number:**\n",
        "\n",
        "$$\\alpha = \\frac{k}{\\sqrt{\\text{epoch_num}}} \\cdot \\alpha_0$$\n",
        "\n",
        "where $k$ is a constant.\n",
        "\n",
        "- **Based on batch size:**\n",
        "\n",
        "$$\\alpha = \\frac{k}{\\sqrt{\\text{batch_size}}} \\cdot \\alpha_0$$\n",
        "\n",
        "where $k$ is a constant.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "nervous-syndicate"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valuable-boost"
      },
      "source": [
        "<a name='Tuning-hyperparameters'></a>\n",
        "## Tuning hyperparameters\n",
        "\n",
        "When training neural networks we normally have to deal with many hyperparameters: learning rate, learning rate decay, number of epochs, dropout probability, hyperparameters that are specific to the optimization algorithm in use, number of layers, number of units for each hidden layer, batch size and so on... some hyperparameters are more important than others, for instance, in many applications, the most important one is considered to be the learning rate.\n",
        "\n",
        "In the notebook about _\"Model evaluation and validation\"_ we have talked about grid search as a method to choose a combination of hyperparameters leading to a better model. When the number of hyperparameters is quite small, that would be a good approach, anyway, with deep learning, we typically don't know a-priori which hyperparameters are going to be the most important ones. Consider the following example: we want to tune the learning rate $\\alpha$ and the hyperparameter $\\varepsilon$ that appears in Adam when we perform the update step. If we constructed a grid of 25 points (left figure) using 5 different values for $\\alpha$ respectively combined with 5 different values for $\\varepsilon$, we would disccover that the perforance of our neural network are mainly influenced by $\\alpha$ while $\\epsilon$ doesn't play an important role in that. This means that we trained 25 different neural networks invain, when we could have obtained the same result with just 5 of them.\n",
        "\n",
        "In order to cope with this issue, it is suggested to pick the values for the hyperparameters at random (right figure). In that case, even if different values of $\\varepsilon$ doesn't make significant improvements, we are trying a larger set of values for $\\alpha$.\n",
        "\n",
        "<img src=\"https://github.com/pietroventurini/machine-learning-notes/blob/main/images/neural_networks/random_search.png?raw=1\" style=\"width:40em; display: block; margin-left: auto; margin-right: auto;\" />\n",
        "\n",
        "We can iterate the search process by sampling hyperparameters in a coarse to fine scheme: at each iteration, we search in a neighborhood of the values that worked best at the previous iteration. In other words, we zoom in to a smaller region of the hyperparameter space and sample more densely within that region."
      ],
      "id": "valuable-boost"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gorgeous-remove"
      },
      "source": [
        "### Choosing an appropriate scale\n",
        "\n",
        "For some hyperparameters it is reasonable to pick their values uniformily between a lower bound and an upper bound. For instance, when choosing the number of hidden units within a hidden layer, we can reasonably sample values uniformily between, say, 50 and 100. The same holds for the number of hidden layers, which can be manually set, for instance, to 2, 3 or 4.\n",
        "\n",
        "Anyway, if we consider the learning rate $\\alpha$, and we sample a bunch of values between 0.0001 and 1, we will end using 90% of the resources searching for values between 0.1 and 1, and just 10% of the resources for searching between 0.0001 and 0.1. A more suitable way would be to sample those values uniformily on the log scale. On that scale, the points 0.0001, 0.001, 0.01, 0.1 and 1 are equally spaced. In practice, that can be obtained in the following way:\n",
        "\n",
        "```python\n",
        "alpha = 10**np.random.uniform(-4, 0) # ⍺ ∈ [0.0001, 1]\n",
        "```\n",
        "\n",
        "This trick can be used, for instance, to sample values for the hyperparameter $\\beta$ when computing a weighted average. It is a bad idea to sample it from a linear scale since the size of the window is highly sensitive on $\\beta$ when $\\beta$ is close to 1. Say that we want $\\beta$ to be between 0.9 and 0.999, that is equivalent to requiring that $1-\\beta=0.1$ and $1-\\beta=0.001$. Therefore we can sample $\\beta$ with `beta = 1 - 10**np.random.uniform(-2, -1)`."
      ],
      "id": "gorgeous-remove"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binding-capacity"
      },
      "source": [
        "### Tuning hyperparameters in practice\n",
        "\n",
        "In practice we can either train a single model and tune it by changing its hyperparameters over time (that is a feasible approach with limited computational capacity), or we can train many models in parallel, finally choosing the best one at the end."
      ],
      "id": "binding-capacity"
    }
  ]
}