{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "```\n",
    "TO-DO: \n",
    "1 - Complete and integrate the notes about linear regression (Data Driven System Modelling)\n",
    "```\n",
    "\n",
    "# Linear regression\n",
    "Linear regression is a supervised learning problem that consists in finding the curve that best fits some data points.\n",
    "\n",
    "We start considering the simple case of $m$ points in the 2D space, and we look for a linear regressor that fits the data well. That regressor has the form:\n",
    "$$\\hat{y} = \\theta_1x + \\theta_2$$\n",
    "Our goal is to find $\\left[\\theta_1, \\theta_2\\right]^T$ that minimize the mean squared error:\n",
    "$$\\frac{1}{2m}\\sum_{i=1}^{m}{(y-\\hat{y})^2}$$\n",
    "To minimize that quantity, we can use the gradient descent method:\n",
    "$$w_i \\rightarrow w_i - \\alpha\\frac{\\partial}{\\partial w_i}\\text{Error}$$\n",
    "\n",
    "At this point, it seems that we've seen two ways of doing linear regression.\n",
    "- **Stochastic**: By applying the squared (or absolute) trick at every point in our data one by one, and repeating this process many times.\n",
    "- **Batch**: By applying the squared (or absolute) trick at every point in our data all at the same time, and repeating this process many times.\n",
    "\n",
    "If our data is huge, both are a bit slow, computationally. The best way to do linear regression, is to split the data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update the weights. This is still called mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life expectancy for a BMI of 21.07931 is equal to 60.315647163993056\n"
     ]
    }
   ],
   "source": [
    "bmi_life_data = pd.read_csv(\"datasets/bmi_and_life_expectancy.csv\")\n",
    "\n",
    "# Setup and fit the model with data\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "# Predict life expectancy for a BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict([[21.07931]])\n",
    "print('Life expectancy for a BMI of 21.07931 is equal to {}'.format(laos_life_exp[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher dimensions\n",
    "The prediction in a $n$ dimensional space, with variables $x_1,x_2,...,x_{n-1}$ is a $n-1$ dimensional hyperplane.\n",
    "\n",
    "$$\\hat{y}=\\theta_1x_1+\\theta_2x_2+\\dots+\\theta_{n-1}x_{n-1}+\\theta_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "X = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Fit the model and assign it to the model variable\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "# Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we simply compute the derivatives of the error function and set to zero, obtaining a system of linear equations in the variables $\\theta_i$? If the number of variables is $n$, we would have to solve a linear systems of $n$ equations and $n$ unknowns. That can be very expensive when $n$ is high. For this reason, gradient descent is a good alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Consider two model, one simpler, with just two variables, and one more complex:\n",
    "\n",
    "$$3x_1+4x_2+5=0$$\n",
    "\n",
    "$$2x_1^3-2x_1^2x_2-4x_2^3+3x_1^3+6x_1x_2+4x_2^2+5=0$$\n",
    "\n",
    "### L1 Regularization\n",
    "L1 regularization consists in taking the absolute values of the coefficients, and adding them to the error of our model.\n",
    "\n",
    "$$\\text{Error} = |3| + |4| = 7$$\n",
    "\n",
    "$$\\text{Error} = |2| + |-2| + |-4| + |3| + |6| + |4| = 21$$\n",
    "\n",
    "- L1 regularization is computationally inefficient unless data is sparse.\n",
    "- Gives us features selection\n",
    "\n",
    "### L2 Regularization\n",
    "L2 regularization consists in taking the squared values of the coefficients, and adding them to the error of our model.\n",
    "\n",
    "$$\\text{Error} = 3^2 + 4^2 = 25$$\n",
    "\n",
    "$$\\text{Error} = 2^2 + (-2)^2 + (-4)^2 + 3^2 + 6^2 + 4^2 = 85$$\n",
    "\n",
    "There exists applications that requires a small error in the model, so we it is ok if it is a complex model and so *punishment* on the complexity should be small. In other cases, simplicity is required, so we can accept errors in our model and so *punishment* on the complexity should be large. That *punishment* is regulated by the $\\lambda$ parameter.\n",
    "\n",
    "- L2 regularization is computationally efficient and better for unsparse data (uniformily distributed between columns)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
