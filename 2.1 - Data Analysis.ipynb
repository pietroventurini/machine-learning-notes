{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "2.1 - Data Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pietroventurini/machine-learning-notes/blob/main/2.1%20-%20Data%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgus3f58SXoH"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYPPiFfdSXoK"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. **[Introduction](#Introduction)** (⚠️ I have to translate it to english)\n",
        "    1. Types of data  \n",
        "    2. Data structures  \n",
        "    3. Data quality  \n",
        "    4. Data similarity measures\n",
        "2. [**Data preprocessing**](#Data-preprocessing)  \n",
        "    1. [Feature scaling](#Feature-scaling)  \n",
        "    2. [Encoding categorical features](#Encoding-categorical-features)  \n",
        "    3. [Curse of dimensionality](#Curse-of-dimensionality)  \n",
        "    4. [Dealing with outliers](#Dealing-with-outliers)  \n",
        "    5. [Ranking](#Ranking)  \n",
        "    6. [Non-linear transformations](#Non-linear-transformations)\n",
        "3. [**Feature engineering**](#Feature-engineering)  \n",
        "    1. [Numeric features](#Numeric-features)  \n",
        "    2. [Categorical features](#Categorical-features)  \n",
        "    3. [Mean encoding](#Mean-encoding)\n",
        "    4. [Missing values](#Missing-values)  \n",
        "    5. [Feature extraction from text and images](#Feature-extraction-from-text-and-images)  \n",
        "    6. [Statistics and distance based features](#Statistics-and-distance-based-features)  \n",
        "    7. [Interaction features](#Interaction-features) \n",
        "4. [**Data leakages**](#Data-leakages)  \n",
        "    1. [Data leaks in time series](#Data-leaks-in-time-series)  \n",
        "    2. [Unexpected information](#Unexpected-information)  \n",
        "    3. [Leaderboard probing](#Leaderboard-probing)  \n",
        "    4. [Example: Expedia Kaggle competition](#Example:-Expedia-Kaggle-competition)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQAtY3vzSXoK"
      },
      "source": [
        "<a name='Introduction'></a>\n",
        "# Introduction \n",
        "\n",
        "Most of the topics covered in this notebook are inspired by the Coursera course [\"How to win a data science competition\"](https://www.coursera.org/learn/competitive-data-science).\n",
        "\n",
        "Appunti: finire di tradurre in inglese ⚠️.\n",
        "\n",
        "Attributi possono essere qualitativi/quantitativi. Per attributi quantitativi va tenuta in considerazione l'unità di misura (si pensi ad una variabile altezza presente in due copie di un dataset: una in Europa, misurata in *cm* e l'altra in America ma convertita in *feet*. Questo fattore va tenuto in conto, ad esempio nel momento in cui decida di addestrare il mio algoritmo su un dataset Europeo, e successivamente testi il mio algoritmo con dati aventi unità di misura differente.\n",
        "\n",
        "## Proprietà fondamentali\n",
        "Attributi possono essere distinti in:\n",
        "- **Nominali:** ID, colore, zip codes...\n",
        "- **Ordinali:** il loro valore è una categoria ordinata e la distanza tra le categorie non è nota, per esempio altezza (basso, medio, alto), ranking (da 1 a 10).\n",
        "- **Interval:** attributi su cui ha senso calcolare delle differenze (es: temperatura, data...)\n",
        "- **Ratio:** attributi su cui ha senso fare dei rapporti dei loro valori (es: temperatura in Kelvin perché non include lo zero, lunghezza, tempo...)\n",
        "\n",
        "Questa classificazione è sostanzialmente basata sul tipo di operazioni che posso fare sugli attributi:\n",
        "- Uguaglianza: $=, \\neq$\n",
        "- Confronto ordinale: $<, \\le, >, \\ge$\n",
        "- Addizione: $+,-$\n",
        "- Moltiplicazione: $\\times, \\div$ \n",
        "\n",
        "Le **operazioni** effettuabili sui miei dati determineranno le tecniche di machine learning e data mining utilizzabili. Alcuni esempi di operazioni effettuabili:\n",
        "- Nominali: moda, entropia, test di correlazione $\\chi^2$...\n",
        "- Ordinali: mediana, calcolo di percentili, correlazione dei ranghi, test dei segni...\n",
        "- Interval: media, deviazione standard, coefficiente di Pearson, t test, F test...\n",
        "- Ratio: media geometrica, media armonica, variazione percentuale...\n",
        "\n",
        "Le **trasformazioni** effettuabili sui miei dati:\n",
        "- Nominali: permutazioni dei valori (se gli ID dei miei dipendenti venissero riassegnati farebbe qualche differenza?)\n",
        "- Ordinali: qualsiasi cambio dei valori che mantenga l'ordinamento (quindi $value_{new} = f(value_{old}$ tale che $f$ sia monotona)\n",
        "- Interval: qualsiasi operazione che mantenga la proporzione tra i vari intervalli (una funzione affine ad esempio)\n",
        "- Ratio: qualsiasi operazione che mantenga il rapporto dei vari valori (come una moltiplicazione per una costante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq-UHYp1SXoL"
      },
      "source": [
        "## Attributi discreti e continui\n",
        "Discreti possono assumere un insieme finito di valori, Continui possono assumere un insieme infinito di valori (tenere comunque a mente che la rappresentazione e memorizzazione su un calcolatore comporterà una discretizzazione di questi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9cQ-upvSXoL"
      },
      "source": [
        "## Tipologie di strutture dati\n",
        "Il dati possono essere memorizzati secondo diverse tipologie di struture che determineranno gli algoritmi di apprendimento applicabili\n",
        "- Un insieme di **record**: \n",
        "    - dati matriciali: ciascun record è costituito da un insieme fisso di attributi\n",
        "    - testuali: ad esempio gli attributi possono essere delle parole e i record possono corrispondere a documenti. In ciascuna cella avrò il numero di occorrenze di quella parola per quel particolare documento.\n",
        "    - transizioni di stato\n",
        "- Un **grafo**: WWW, strutture molecolari...\n",
        "- Dati **ordinati**: dati spaziali, temporali, sequenze..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyevqmm7SXoM"
      },
      "source": [
        "## Qualità dei dati\n",
        "Le misure sono tipicamente affette da rumore, quindi questo fattore è da tenere presente. Prima di fare una qualunque analisi del nostro sistema è opportuno porsi delle domande: è presente del rumore nel nostro dataset? In quale misura? \n",
        "\n",
        "Vi sono degli *outliers* nel nostro dataset? Eventualmente potrei utilizzare la mediana piuttosto che la media, che è meno influenzata dalla presenza di outliers. In tecniche di apprendimento non supervisionato, come il clustering, la presenza di outliers può influenzare notevolmente l'accuratezza del risultato prodotto dall'algoritmo. Potrei quindi decidere di filtrare eventuali outliers.\n",
        "\n",
        "Va tenuto conto anche di valori mancanti (è necessario capire se la loro mancanza abbia un certo significato, in modo da poter associare un valore di default evaentualmente) o duplicati. Spesso gli algoritmi di machine learning escludono i record con valori mancanti, ma questo potrebbe causare notevoli problemi qualora il numero di dati mancanti sia alto. Il motivo dell'assenza di alcuni valori può essere dovuto a:\n",
        "- informazioni non raccolte: ad esempio in un questionario una persona potrebbe non voler specificare il sesso.\n",
        "- informazioni non applicabili: un salario ad un bambino\n",
        "\n",
        "In questi casi potrei cercare di dedurre il valore inferendolo da altri record simili a quello con il valore mancante oppure a partire da altri attributi dello stesso record (dall'altezza e dall'indice di massa corporea potrei inferire il peso). Posso cercare di eliminare record con troppi valori mancanti, fare una stima, ignorare tale attributo, rimpiazzarli con valori quantitativi con una probabilità associata.\n",
        "\n",
        "Eventuali duplicati (o dati molto simili) vanno gestiti attraverso un processo di **data cleaning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPXJ5G3SXoM"
      },
      "source": [
        "## Similarità e Dissimilarità dei dati\n",
        "La **similarità** definisce quanto due campioni siano simili; è spesso misurata nel range $[0,1]$. La **dissimilarità** indica quanto due campioni non si assomiglino. Più questa è bassa, più i campioni si assomigliano (se sono identici, la dissimilarità sarà 0). Il limite superiore può variare, ad esempio può essere utilizzata la distanza euclidea tra due punti come misura di dissimilarità.\n",
        "\n",
        "A seconda del tipo di attributi possiamo individuare:\n",
        "- **Nominali:** possiamo verificare se questi siano uguali o meno.  \n",
        "    - Dissimilarità: \n",
        "$$d=\\begin{cases}0 & \\text{if } p=q \\\\ 1 & \\text{if } p\\neq q \\end{cases}$$\n",
        "    - Similarità:\n",
        "$$s=\\begin{cases}1 & \\text{if } p=q \\\\ 0 & \\text{if } p\\neq q \\end{cases}$$\n",
        "- **Ordinali:** avendo un ordinamento tra questi $n$ elementi, possiamo utilizzare i loro indici ($|p-q|$ rappresenta la distanza in termini di numero di elementi tra $p$ e $q$. Se questi coincidono, allora $|p-q|=0$, se questi sono agli estremi opposti, allora $|p-q|=n-1$\n",
        "    - Dissimilarità: $d = \\frac{|p-q|}{n-1}$\n",
        "    - Similarità:    $s = 1 - \\frac{|p-q|}{n-1}$\n",
        "- **Interval o Ratio:** \n",
        "    - Dissimilarità: $d = |p-q|$ (distanza euclidea, ma possono anche essercene altre)\n",
        "    - Similarità:    $s = -d$, $s = \\frac{1}{1+d}$ or $s = 1 - \\frac{d - min_d}{max_d - min_d}$\n",
        "    \n",
        "Come distanza per attributi di tipo interval, possiamo utilizzare la **distanza euclidea** $d = \\sqrt{\\sum_{i=1}^{n}{\\left(p_i - q_i \\right)^2}}$, dove $n$ rappresenta il numero di features (dimensioni). In questo caso la standardizzazione risulta necessaria in modo che ciascuna feature contribuisca in ugual modo nel calcolo della distanza. È quindi possibile costruire una matrice di similarità o di dissimilarità andando a misurare le distanze tra tutti i dati, secondo i valori di loro attributi di tipo interval.\n",
        "\n",
        "Una generalizzazione di distanza Euclidea e della distanza di Manhattan è la **distanza di Minkowski**\n",
        "\n",
        "$$d = \\left(\\sum_{i=1}^{n}{|p_i - q_i |^r}\\right)^{\\frac{1}{r}}$$\n",
        "\n",
        "dove $r$ è un parametro:\n",
        "- $r=1$: norma $l_1$\n",
        "- $r=2$: distanza Euclidea\n",
        "- $r\\rightarrow\\infty$: norma $l_\\infty$\n",
        "\n",
        "Una distanza deve rispettare le proprietà di definizione positiva, simmetria e disuguaglianza triangolare.\n",
        "\n",
        "Anche la similarità deve rispettare alcune proprietà: \n",
        "- $s(p,q)=1 \\Leftrightarrow p=q$ (massima similarità)\n",
        "- $s(p,q) = s(q,p) \\; \\forall q,p$ (simmetria)\n",
        "di conseguenza la similarità non è una distanza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0-w94l0SXoM"
      },
      "source": [
        "### Similarità tra vettori binari\n",
        "Per quanto riguarda la **similarità tra vettori binari** (dove $p$ e $q$ hanno solo attributi binari) possiamo definire il **Simple Matching Coefficient** e il **coefficiente di Jaccard**:\n",
        "\n",
        "- $M_{00}$: numero di attributi dove $p=0$ e $q=0$\n",
        "- $M_{01}$: numero di attributi dove $p=0$ e $q=1$\n",
        "- $M_{10}$: numero di attributi dove $p=1$ e $q=0$\n",
        "- $M_{11}$: numero di attributi dove $p=1$ e $q=1$\n",
        "\n",
        "$$SMC = \\frac{\\text{numero di matches}}{\\text{numero di attributi}} = \\frac{M_{11} + M_{00}}{M_{01} +M_{10} +M_{11} +M_{00}}$$\n",
        "\n",
        "$$J = \\frac{\\text{numero di } M_{11} \\text{ matches}}{\\text{numero di attributi non entrambi zero}} = \\frac{M_{11}}{M_{01} +M_{10} +M_{11}}$$\n",
        "\n",
        "#### Esempio\n",
        "$p = 1000000000$\n",
        "\n",
        "$q = 0000001001$\n",
        "- $M_{00}$: 7\n",
        "- $M_{01}$: 2\n",
        "- $M_{10}$: 1\n",
        "- $M_{11}$: 0\n",
        "\n",
        "$SMC = \\frac{7+0}{7+2+1+0} = 0.7$\n",
        "\n",
        "$J = \\frac{0}{2+1+0} = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH2NSZ3CSXoN"
      },
      "source": [
        "### Similarità del coseno\n",
        "Dati due vettori $d_1,d_2$ si ha:\n",
        "\n",
        "$$\\cos(d_1,d_2) = \\frac{(d_1 \\bullet d_2)}{||d_1|| \\cdot ||d_2||}$$\n",
        "\n",
        "dove $\\bullet$ indica il prodotto scalare e $||d||$ indica la lunghezza del vettore (norma).\n",
        "\n",
        "#### Esempio\n",
        "$d_1 = 3205000200$\n",
        "\n",
        "$d_2 = 1000000102$\n",
        "\n",
        "$\\cos(d_1,d_2) = \\frac{5}{6.481+2.245} = 0.3150$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUIkQoNmSXoN"
      },
      "source": [
        "### Misure di correlazione\n",
        "Per calcolare la correlazione, prima si standardizzano $p$ e $q$, poi si calcola il prodotto scalare:\n",
        "\n",
        "$$p'_k=\\frac{\\left(p_k-\\mu_p\\right)}{\\sigma_p} \\quad q'_k=\\frac{\\left(q_k-\\mu_q\\right)}{\\sigma_q}$$\n",
        "\n",
        "$$correlation(p,q) = p'\\bullet q'$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNff9JHXSXoO"
      },
      "source": [
        "<a name='Data-preprocessing'></a>\n",
        "\n",
        "# Data preprocessing\n",
        "\n",
        "Prima di applicare algoritmi di machine learning potrei effettuare operazioni di preprocessing dei dati al fine di facilitare la successiva gestione di tali dati.\n",
        "- **Aggregazione:** potrei cercare di aggregare tra loro più attributi o più record per ridurre il numero di dati, per effettuare cambi di scala (aggregare paesi in regioni, regioni in nazioni...) o per avere dati più stabili, con meno variazioni.\n",
        "- **Campionamento (sampling):** quando ho un dataset molto grande posso fare un campionamento del dataset e applicare inizialmente le tecniche di machine learning su un dataset ridotto (analogamente per la valutazione del modello ottenuto). Tecniche di **stratified cross validation** (che mantengono le proprietà statistiche del dataset originale) sono consigliate a questo scopo. Il campionamento può essere casuale, senza reinserimento, con reinserimento, *stratified* (divido i dati in partizioni ed estraggo campioni casuali da ciascuna partizione)...\n",
        "[Quanto deve essere grande il sample?](https://en.wikipedia.org/wiki/Sample_size_determination) In genere più è grande, più è alta la precisione nella stima di parametri sconosciuti. Questo fenomeno è descritto da varie leggi matematiche, tra cui la legge dei grandi numeri e il teorema del limite centrale.\n",
        "\n",
        "- **Dimensionality reduction:** in modo da poter usare tecniche che lavorino solo sugli attributi principali\n",
        "- **Feature subset selection:** cercare di identificare le feature (variabili) più rilevanti e quali invece contengano rumore\n",
        "- **Feature creation:** ho una conoscenza sul dominio che potrei sfruttare per inserire nuovi attributi nel mio dataset (ad esempio da altezza e peso ricavare l'indice di massa corporea)\n",
        "- **Attribute transformation:** un peso di una persona in milligrammi potrebbe essere trasformato in kilogrammi. Possono essere applicati trasformazioni lineari o non-lineari per favorire l'apprendimento (esempio: trasformazione logaritmica per passare da un modello esponenziale ad uno lineare).\n",
        "- **Discretization and Binarization:** se so che gli attributi numerici sono afflitti da rumore, potrei decidere di discretizzare questi valori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50iZPbVcSXoO"
      },
      "source": [
        "## Feature scaling\n",
        "\n",
        "There are algorithms that are sensitive to feature scaling, meaning that varying the scale of a feature may change the behavior of the algorithm itself:\n",
        "- k-nearest neighbors: if we want all features to contribute equally, we need to scale them to a common range.\n",
        "- k-means\n",
        "- logistic regression, SVMs, Neural networks, etc.: gradient descent can be much quicker if features are properly scaled.\n",
        "- linear discriminant analysis, principal component analysis: since we want to find orthogonal directions of maximizing the variance, we want features on the same scale, in order not to emphasize certain variables more.\n",
        "\n",
        "On the contrary, tree-based algorithms such as the CART decision tree are scale-invariant, in the sense that they aim at finding the optimal threshold to split a certain feature, regardless of the scale.\n",
        "\n",
        "There exists many methods to perform feature scaling, such as:\n",
        "\n",
        "- Rescaling (min-max normalization)\n",
        "- Mean normalization\n",
        "- Standardization\n",
        "- Scaling to unit length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQp_r2ASXoO"
      },
      "source": [
        "### Min-max normalization\n",
        "\n",
        "It consists in rescaling the range of possible values to $[0,1]$ or $[-1,1]$.\n",
        "\n",
        "\\begin{equation}\n",
        "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEqXqiAcSXoO"
      },
      "source": [
        "### Mean normalization\n",
        "\n",
        "\\begin{equation}\n",
        "x' = \\frac{x - \\mu_x}{\\max(x) - \\min(x)}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mu_x$ being the sample mean of $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAjiigeYSXoP"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "Mean normalization makes the normalized feature to have zero mean and unit variance.\n",
        "\n",
        "\\begin{equation}\n",
        "x' = \\frac{x - \\mu_x}{\\sigma_x}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mu_x$ is the sample mean and $\\sigma_x$ is the sample standard deviation of $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcst7W3jSXoP"
      },
      "source": [
        "### Scaling to unit length\n",
        "\n",
        "It consists in scaling a feature vector such that the resulting one has length one.\n",
        "\n",
        "\\begin{equation}\n",
        "x' = \\frac{x}{\\Vert x \\Vert}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRW7FRL2SXoP"
      },
      "source": [
        "#### Example\n",
        "\n",
        "In the following example, we will compare how different scalers (respectively the Standard scaler and the Min-max scaler) affect the performance of logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sjGZSMmSXoQ",
        "outputId": "e5a1ad3f-23ca-4a0d-8b78-70b59c868b07"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = load_wine(as_frame=True)\n",
        "X = data.data[['alcohol','malic_acid']]\n",
        "y = data.target\n",
        "X.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13.000618</td>\n",
              "      <td>2.336348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.811827</td>\n",
              "      <td>1.117146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.362500</td>\n",
              "      <td>1.602500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.050000</td>\n",
              "      <td>1.865000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>13.677500</td>\n",
              "      <td>3.082500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.830000</td>\n",
              "      <td>5.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          alcohol  malic_acid\n",
              "count  178.000000  178.000000\n",
              "mean    13.000618    2.336348\n",
              "std      0.811827    1.117146\n",
              "min     11.030000    0.740000\n",
              "25%     12.362500    1.602500\n",
              "50%     13.050000    1.865000\n",
              "75%     13.677500    3.082500\n",
              "max     14.830000    5.800000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZchqylpSXoQ",
        "outputId": "82740c8d-5c1f-4ddb-8d08-441c3df7ecc8"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1234)\n",
        "\n",
        "# fit logistic regression without scaling\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# compute root MSE on test set\n",
        "rmse = [model.score(X_test, y_test)]\n",
        "\n",
        "# build pipelines using standard scaler and min-max scaler\n",
        "pipelines = [make_pipeline(MinMaxScaler(), LogisticRegression()),\n",
        "             make_pipeline(StandardScaler(), LogisticRegression())]\n",
        "\n",
        "for pipe in pipelines:\n",
        "    pipe.fit(X_train, y_train)\n",
        "    pred = pipe.predict(X_test)\n",
        "    rmse.append(np.sqrt(mean_squared_error(y_test,pred)))\n",
        "    \n",
        "df_lr = pd.DataFrame({'RMSE':rmse}, index=['Original','Min-Max','Normalized'])\n",
        "df_lr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Original</th>\n",
              "      <td>0.796296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Min-Max</th>\n",
              "      <td>0.745356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Normalized</th>\n",
              "      <td>0.680414</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                RMSE\n",
              "Original    0.796296\n",
              "Min-Max     0.745356\n",
              "Normalized  0.680414"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDTV6PEsSXoR"
      },
      "source": [
        "As we can see, by normalizing the two features, we obtained a smaller root mean squared error on the test set, compared to the score we obtained on the unscaled test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2bRXkowSXoR"
      },
      "source": [
        "## Encoding categorical features\n",
        "\n",
        "### Ordinal encoding\n",
        "\n",
        "Categorical features can be efficiently coded as integers, by replacing the categories by an integer. For instance, consider these 3 categorical features that a person could have:\n",
        "```python\n",
        "Sex = ['male', 'female']\n",
        "Occupation = ['student', 'worker', 'unemployed']\n",
        "From = ['Europe', 'Asia', 'North-America', 'South-America', 'Africa', 'Australia', 'Antarctica']\n",
        "```\n",
        "\n",
        "We can convert each of these categories into an integer value by using [`sklearn.preprocessing.OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRTbO0aJSXoR",
        "outputId": "4dbfbb8d-9ca9-4c72-e10a-fc654063affa"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "enc = OrdinalEncoder(categories=[['male', 'female'], \n",
        "                                 ['student', 'worker', 'unemployed'], \n",
        "                                 ['Europe', 'Asia', 'North-America', 'South-America', 'Africa', 'Australia', 'Antarctica']])\n",
        "X = [['male', 'unemployed', 'North-America'], ['female', 'student', 'Africa']]\n",
        "enc.fit(X)\n",
        "enc.transform(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 2., 2.],\n",
              "       [1., 0., 4.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hrVB9vtSXoS"
      },
      "source": [
        "In general, such representation can't be used directly with estimators because ordinal encoding creates a fictional ordinal relationship in the data. For this reason, in order to improve the performance of the learning algorithm we will use, **one-hot encoding** is often applied to categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-CPbfJoSXoS"
      },
      "source": [
        "### Frequency encoding\n",
        "\n",
        "Frequency encoding consists into associating to each value a categorical feature may take, the corresponding frequency in the dataset. For instance, assume that the titanic dataset has a categorical feature called `Embarked`:\n",
        "\n",
        "```python\n",
        "encoding = titanic.groupby('Embarked').size()\n",
        "encoding = encoding / len(titanic)\n",
        "titanic['Embarked_enc'] = titanic.Embarked.map(encoding)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrVdti1RSXoS"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "One-hot encoding consists in creating, for each unique value in the original categorical column, a new column representing a binary feature that can take only two values: 1, if in the original column, that entry takes value equal to the one that this column represents, 0 otherwise.\n",
        "\n",
        "For instance, consider an instance having `Occupation = worker`. Therefore, by one-hot encoding the categorical columns, the `Occupation` feature will give birth to 3 new columns, one for each value that `Occupation` may take.\n",
        "\n",
        "| student | worker | unemployed |\n",
        "|---------|--------|------------|\n",
        "| 0       | 1      | 0          |\n",
        "\n",
        "One-hot encoding can be performed using [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) or [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQj5eeG1SXoS",
        "outputId": "99f52817-9881-4452-d4ed-83723f7f0c8f"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(categories=[['male', 'female'], \n",
        "                                ['student', 'worker', 'unemployed'], \n",
        "                                ['Europe', 'Asia', 'North-America', 'South-America', 'Africa', 'Australia', 'Antarctica']])\n",
        "X = [['male', 'unemployed', 'North-America'], ['female', 'student', 'Africa']]\n",
        "enc.fit(X)\n",
        "enc.transform(X).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0XbubGpSXoT"
      },
      "source": [
        "Note that not every model requires to one-hot encode categorical features. For instance, random forests, can work directly with categorical, without requiring those features to be transformed at all. Note also that, categorical features with many values would end up in many new binary features, resulting into a very sparse matrix. In that case, we can store just the non-zero elements using **sparse matrices**, reducing the memory occupation by a lot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mm_OVHFSXoT"
      },
      "source": [
        "## Curse of dimensionality\n",
        "The curse of dimensionality is a phenomenon that arises when working in very-high dimensional spaces. As the number of dimensions increases, the space grows exponentially and data soon becomes sparse (meaning that much volume is left unoccupied by dataset's instances). Suppose, for example, that every variable (i.e. every dimension) can take only one of two values. With $n$ variables, we would have $2^n$ possible combinations of their values. When variables are more than observations we face the risk of overfitting. For example, clustering algorithms fails to operate on very large spaces, since data points (i.e. vectors of the space) tend to be equally spaced, frustrating any attempt of identifying clusters based on how close points are together.\n",
        "\n",
        "A trivial example of clustering is: assume we have 8 candies (8 records). Each candies is characterized by two boolean attributes that describe its color: <span style=\"color:red\">Reddish</span> and <span style=\"color:blue\">Bluish</span>. Our dataset consists of 4 reddish candies and 4 bluish ones.\n",
        "\n",
        "|    | Reddish   | Bluish   |\n",
        "|----|-----------|----------|\n",
        "| c1 | 1         | 0        |\n",
        "| c2 | 1         | 0        |\n",
        "| c3 | 1         | 0        |\n",
        "| c4 | 1         | 0        |\n",
        "| c5 | 0         | 1        |\n",
        "| c6 | 0         | 1        |\n",
        "| c7 | 0         | 1        |\n",
        "| c8 | 0         | 1        |\n",
        "\n",
        "It won't be difficult for a clustering algorithm to indentify two clusters. But let's think now of having 8 boolean variables that describe the candy's color more in detail.\n",
        "\n",
        "|    | Red    | Orange  | Yellow | Fuchsia|  Blue  | Light blue | Green | Purple |\n",
        "|:----:|:--------:|:---------:|:--------:|:--------:|:--------:|:---------:|:-------:|:-------:|\n",
        "| c1 | 1      | 0       | 0      | 0      | 0      | 0       | 0     | 0     |\n",
        "| c2 | 0      | 1       | 0      | 0      | 0      | 0       | 0     | 0     |\n",
        "| c3 | 0      | 0       | 1      | 0      | 0      | 0       | 0     | 0     |\n",
        "| c4 | 0      | 0       | 0      | 1      | 0      | 0       | 0     | 0     |\n",
        "| c5 | 0      | 0       | 0      | 0      | 1      | 0       | 0     | 0     |\n",
        "| c6 | 0      | 0       | 0      | 0      | 0      | 1       | 0     | 0     |\n",
        "| c7 | 0      | 0       | 0      | 0      | 0      | 0       | 1     | 0     |\n",
        "| c8 | 0      | 0       | 0      | 0      | 0      | 0       | 0     | 1     |\n",
        "\n",
        "The clustering algorithm is not able anymore to detect similarities (correlation between variables) between subsets of candies.\n",
        "\n",
        "It is possible to remedy by adopting techinques of ***dimensionality reduction*** like:\n",
        "- **Principal Component Analysis (PCA)**:\n",
        "- **Singular Value Decomposition:**\n",
        "- **Feature Subset Selection:** we try to remove redundant features (e.g. the price of a product and the corresponding tax) or insignificant features (e.g. the eyes color of a client in a clustering problem about types of clients of a supermarket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEyevT6dSXoT"
      },
      "source": [
        "## Dealing with outliers\n",
        "\n",
        "To protect a linear model from being affected by outliers we can *clip* features' values between within a specific range, choosing the lower bound and the upper bound as some percentiles of that features (e.g. first, and 99th percentile). This procedure is called ***winsorization*** and can be done in Python using [`scipy.stats.mstats.winsorize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.winsorize.html). An example is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DudAi304SXoT",
        "outputId": "594b66bb-7c37-4317-ef63-815d3e94f573"
      },
      "source": [
        "# generate data within a range\n",
        "x = np.random.uniform(low=0, high=100, size=(200,))\n",
        "\n",
        "# add outlier\n",
        "x = np.append(x, -60)\n",
        "\n",
        "# plot histogram\n",
        "pd.Series(x).hist(bins=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee-m-zt2SXoU",
        "outputId": "d0fa7856-ef79-410e-bbc5-a350c0eb4719"
      },
      "source": [
        "# compute 99% interval, clip and plot histogram\n",
        "upper_bound, lower_bound = np.percentile(x, [1,99])\n",
        "x = np.clip(x, upper_bound, lower_bound)\n",
        "\n",
        "pd.Series(x).hist(bins=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCYRBoCkSXoU"
      },
      "source": [
        "## Ranking\n",
        "\n",
        "Another useful preprocessing operation is *ranking*. [Ranking](https://en.wikipedia.org/wiki/Ranking) consists into performing a weak ordering of the values a feature takes. By doing this we move outliers closer to other values, defining a mapping between values and indexes. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0IcSg3DSXoV",
        "outputId": "cc8d5389-55cf-46b5-ece5-eeefcaaac5d7"
      },
      "source": [
        "from scipy.stats import rankdata\n",
        "\n",
        "rankdata([10, 3, 1e-4, 50, 3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4. , 2.5, 1. , 5. , 2.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ntLnOtSXoV"
      },
      "source": [
        "Note that, in order to keep the same rank mapping also with the test set, we need to store the mapping computed with the training set. Alternatively, we can first concatenate the training with the test set, then compute the ranking of the ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smKvn5zCSXoV"
      },
      "source": [
        "## Non-linear transformations\n",
        "\n",
        "Non-linear transformations of the values can help non-tree-based algorithms, such as neural nets, because they bring the values closer to their mean and make the values near zero a little more distinguishable. Examples are:\n",
        "- Log transform\n",
        "\n",
        "$$x' = \\log(1+x)$$\n",
        "\n",
        "- Rising to a power $< 1$\n",
        "\n",
        "$$x' = \\sqrt{x + 2/3}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hour9qgSXoV"
      },
      "source": [
        "<a name='Feature-engineering'></a>\n",
        "\n",
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAaTkx1xSXoV"
      },
      "source": [
        "## Numeric features\n",
        "\n",
        "Numeric features can be extended by generating new ones, either with knowledge about the domain of the problem, or through *exploratory data analysis* (EDA). For example, if we had `height` and `width` features, we could generate a new `distance` variable by using Pitagora's theorem. Similarly, `squared_area` and `total_price` can be combined into an interaction features called `price_for_m2`, computing the price for square meter.\n",
        "\n",
        "Another thing we may try, is to extract the fractional part of a decimal number. Variables such as `price` can benefit from this because factors such as people perception of prices could then be taken into account by the model.\n",
        "\n",
        "| price | price_decimal_part |\n",
        "|:-----:|:------------------:|\n",
        "|  0.99 |         .99        |\n",
        "|  6.49 |         .49        |\n",
        "|  1.00 |         .00        |\n",
        "|  9.98 |         .98        |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAu50WrgSXoW"
      },
      "source": [
        "### Date and time features\n",
        "\n",
        "Date and time are quite interesting features, because not only they are linear in nature, but they also have several different tiers, such as *year*, *day*, *week*... Features generated by datetime can be divided into two categories:\n",
        "1. **Time moments in a period**: day number in the week, month, season, year, seconds, minutes, hours. This is useful to capture **repetitive patterns** in data. Another attribute of this type can be a boolean value indicating whether that day is a holiday.\n",
        "2. **Time passed since a particular event**: this can be either \n",
        "    - **row-independent**: such as time past from a general moment for all data (e.g. from year 2000).\n",
        "    - **row-dependent**: for instance, the number of days left until Christmas, or the days past since the last promotional campaign.\n",
        "    \n",
        "Generating features of this kind may return useful in tasks such as **churn prediction**, which consists in estimating the likelihood that a customer will churn (i.e. will stop paying for a service). Combining a feature like `last_purchase_date` with other relevant dates, into a new variable `date_diff`, can improve by a lot the model's performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5l9oJ2_SXoW"
      },
      "source": [
        "### Coordinates features\n",
        "\n",
        "When we deal with features involving coordinates (such as the position of a house), we can augment our dataset in many ways:\n",
        "- Computing the distance from a certain relevant place, like an hospital, a school or a popular meeting place.\n",
        "- Splitting the map into squares, and for every flat in each square, computing the distance from the most expensive flat in that area.\n",
        "- Organizing data points into clusters, then use the centroid of each cluster as an important point from which to compute the distance.\n",
        "- Computing aggregating statistics for objects' surrounding area (e.g. number of flats within a certain range, which can be interpreted as area's popularity).\n",
        "- Transform coordinates by rotating them around a point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kadcy0ndSXoW"
      },
      "source": [
        "## Categorical features\n",
        "\n",
        "When dealing with categorical features, we can derive new ones by considering the interaction between the variables, so that the model can find optimal coefficient for interaction features and improve. This can be useful with non-tree-based models.\n",
        "This can be done by concatenating the values of two or more features, and then one-hot encoding them.\n",
        "\n",
        "Assume to have this data, where the interaction feature `sex_fav_color` between `sex` and `fav_color` has already been built.\n",
        "\n",
        "| sex    | fav_color | sex_fav_color    |\n",
        "|--------|-----------|------------------|\n",
        "| male   | green     | male_green       |\n",
        "| female | black     | female_black     |\n",
        "| female | turquoise | female_turquoise |\n",
        "| female | black     | female_black     |\n",
        "\n",
        "By one-hot encoding the `sex_fav_color` variable, we obtain:\n",
        "\n",
        "| female_black | female_turquoise | male_green |\n",
        "|--------------|------------------|------------|\n",
        "| 0            | 0                | 1          |\n",
        "| 1            | 0                | 0          |\n",
        "| 0            | 1                | 0          |\n",
        "| 1            | 0                | 0          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP4o5zSNSXoW"
      },
      "source": [
        "## Mean encoding\n",
        "\n",
        "Mean encoding (or likelihood/target encoding) is a powerful technique that allows to create new features from the target variable. The simplest way to achieve mean encoding is to encode each value of a categorical variable with the corresponding target mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hwn5fKwSXoW",
        "outputId": "adfeaaea-0f2d-40ab-ed41-c388c92a6930"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "dataset = [('Milan', 0),('Milan', 1),('Milan', 1),('Milan', 0),('Milan', 0),('Rome', 1),\n",
        "           ('Rome', 1),('Rome', 1),('Rome', 0),('Venice', 0),('Venice', 0),('Rome', 1)]\n",
        "df = pd.DataFrame(dataset, columns=['feature','target'])\n",
        "\n",
        "# perform label encoding of feature\n",
        "enc = LabelEncoder()\n",
        "enc.fit(df.feature)\n",
        "df['feature_label'] = enc.transform(df['feature'])\n",
        "\n",
        "# computing feature mean\n",
        "means = df.groupby('feature').target.mean()\n",
        "df['feature_mean'] = df['feature'].map(means)\n",
        "\n",
        "# reorder columns and show dataframe\n",
        "df = df[['feature', 'feature_label', 'feature_mean', 'target']]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>feature_label</th>\n",
              "      <th>feature_mean</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Milan</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Milan</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Milan</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Milan</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milan</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rome</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Rome</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Rome</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Rome</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Venice</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Venice</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Rome</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature  feature_label  feature_mean  target\n",
              "0    Milan              0           0.4       0\n",
              "1    Milan              0           0.4       1\n",
              "2    Milan              0           0.4       1\n",
              "3    Milan              0           0.4       0\n",
              "4    Milan              0           0.4       0\n",
              "5     Rome              1           0.8       1\n",
              "6     Rome              1           0.8       1\n",
              "7     Rome              1           0.8       1\n",
              "8     Rome              1           0.8       0\n",
              "9   Venice              2           0.0       0\n",
              "10  Venice              2           0.0       0\n",
              "11    Rome              1           0.8       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7-trsNASXoX"
      },
      "source": [
        "Now, by plotting the histograms of a certain label encoded and mean encoded feature, we can observe that, with label encoding, the plot doesn't convey any information about the target value, since the label associated to each feature's value is not correlated with the target variable. On the contrary, by looking at the histogram of the target mean encoded feature, the classes look way more separable.\n",
        "\n",
        "<img src=\"https://github.com/pietroventurini/machine-learning-notes/blob/main/images/data_analysis/mean_vs_label_encoding.png?raw=1\" alt=\"comparison of label encoding and mean encoding histograms\" style=\"display: block; margin-left: auto; margin-right: auto; width:50em\" />\n",
        "\n",
        "In decision trees, with mean encoding we can reach a better loss than we can with label encoding, still keeping the tree short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EitX2Zu_SXoX"
      },
      "source": [
        "### Remarks\n",
        "\n",
        "The presence of categorical variables with a lot of values is usually an indicator that mean encoding may help the model.\n",
        "\n",
        "The target variable can be used as we have seen before (by computing the mean of the target), but also in these ways for each value of the categorical variable:\n",
        "- Weight of evidence\n",
        "$$\\ln\\left(\\frac{\\text{# positive instances}}{\\text{# negative instances}}\\right)\\cdot 100$$\n",
        "- Counting the occurrences of each class\n",
        "- Difference between the number of positive instances and the number of negative ones.\n",
        "\n",
        "One main drawback of mean encoding, is that we must **pay extreme attention during validation**: if we compute mean encodings after performing the train/validation split, independently on the training set and on the validation set, then we will probably overfit the training set and obtain a low score on the validation set. This is because it may happen that the same feature value pops up in both the train set and validation set but with different target values (resulting in completely different target means in the train and in the validations set).\n",
        "\n",
        "So, it is important to **compute mean encodings before the train validation split**. Moreover, we can't use mean encoding as it is, but we need to cope with overfitting by applying regularaziation on the training set.\n",
        "\n",
        "We are now going to cover some types of regularization:\n",
        "- Cross validation loop inside the training data,\n",
        "- Smoothing\n",
        "- Sorting and calculating expanding mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KEwKbvhSXoX"
      },
      "source": [
        "#### Cross validation regularization\n",
        "\n",
        "As we would do with k-fold cross validation, we split the dataset into $k$ folds, and for each fold we compute its mean encoding using data outside of that fold. In practice, 4 or 5 folds would do the job. \n",
        "For instance, consider the following script, in which `df` contains the training data and `train_new` is assumed to have been previously initialized:\n",
        "\n",
        "```python\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123) \n",
        "\n",
        "for tr_idx, val_idx in skf.split(X=np.zeros(n_samples), y=df['target'].values): # n_samples = len(df.index)\n",
        "    X_tr, X_val = df.iloc[tr_idx], df.iloc[val_idx]\n",
        "    for col in cols: # cols contains the columns we want to encode\n",
        "        means = X_tr.groupby(col)['target'].mean() \n",
        "        X_val[col+'_mean_target'] = X_val[col].map(means)\n",
        "    train_new.iloc[val_idx] = X_val\n",
        "\n",
        "global_mean = df['target'].mean()\n",
        "train_new.fillna(global_mean, inplace=True) # fill NaNs with global mean\n",
        "df = pd.concat([df, train_new], axis=1)\n",
        "```\n",
        "\n",
        "\n",
        "Cross validation helps us to avoid obtaining a perfect feature which is in 1:1 match with the target value, being able to predict perfectly every instance of the training set, but being useless on unseen data. If we use a Leave-One-Out scheme, we may fall into that trap. Indeed, consider this dataset:\n",
        "\n",
        "| feature | feature_mean | target |\n",
        "|---------|--------------|--------|\n",
        "| Rome    | 0.50         | 0      |\n",
        "| Rome    | 0.25         | 1      |\n",
        "| Rome    | 0.25         | 1      |\n",
        "| Rome    | 0.50         | 0      |\n",
        "| Rome    | 0.50         | 0      |\n",
        "\n",
        "in this case `feature_mean` has been obtained with a LOO scheme, but it is a target variable leakage, indeed we can perfectly predict the target by just looking at the `feature_mean` variable since they are in a 1:1 match.\n",
        "\n",
        "A simple way to perform LOO encoding of a column named _feature_ is the following:\n",
        "\n",
        "```python\n",
        "target_sum = df['feature'].map(df.groupby('feature')['target'].sum())\n",
        "target_count = df['feature'].map(df.groupby('feature')['target'].count())\n",
        "df['feature_mean_LOO'] = (target_sum - df['target']) / (target_count - 1)\n",
        "\n",
        "global_mean = df['target'].mean()\n",
        "df['feature_mean_LOO'].fillna(global_mean, inplace=True) # fill NaNs with global mean\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKoVckk4SXoX"
      },
      "source": [
        "#### Smoothing regularization\n",
        "\n",
        "Smoothing is based on the following idea: if a category is big (it has a lot of data points) then we can trust the estimated encoding, while for a category that is rare, it is the opposite. The following formula uses this idea:\n",
        "\n",
        "$$\\frac{mean(\\text{target_value})\\cdot \\text{n_rows_target} + \\text{global_mean}\\cdot \\alpha}{\\text{n_rows_target}+\\alpha}$$\n",
        "\n",
        "The hyperparameter $\\alpha$ controls the amount of regularization:\n",
        "- $\\alpha=0:$ no regularization \n",
        "- $\\alpha \\rightarrow \\infty:$ encoding approaches global mean\n",
        "\n",
        "In some sense, $\\alpha$ is equal to the category size we can trust.\n",
        "\n",
        "We can use other formulas: basically, everything that penalizes small sized categories falls under smoothing. Note that smoothing can't work on its own, but we can combine it with, for example, CV loop regularization.\n",
        "\n",
        "Using Pandas it can be done in this way:\n",
        "\n",
        "```python\n",
        "alpha = 100\n",
        "target_mean = df['feature'].map(df.groupby('feature')['target'].mean())\n",
        "target_count = df['feature'].map(df.groupby('feature')['target'].count())\n",
        "df['feature_mean_smooth'] = (target_mean*target_count + global_mean*alpha) / (target_count*alpha)\n",
        "\n",
        "global_mean = df['target'].mean()\n",
        "df['feature_mean_smooth'].fillna(global_mean, inplace=True) # fill NaNs with global mean\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWSprJI1SXoX"
      },
      "source": [
        "#### Expanding mean regularization\n",
        "\n",
        "We fix some sorting order for our data and we use the rows from $0$ to $i-1$ to calculate the encoding for the $i$-th row.\n",
        "\n",
        "For example we can do:\n",
        "\n",
        "```python\n",
        "cumsum = df.groupby('feature')['target'].cumsum() - df['target']\n",
        "cumcnt = df.groupby('feature').cumcount()\n",
        "df['feature_mean_target'] = cumsum / cumcnt\n",
        "\n",
        "global_mean = df['target'].mean()\n",
        "df['feature_mean_target'].fillna(global_mean, inplace=True) # fill NaNs with global mean\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mU9e0OrSXoY"
      },
      "source": [
        "### Generalizations\n",
        "\n",
        "How do we apply mean encodings in **regression**? Unlike binary classification, where the target mean is the only meaningful statistics we can extract from the target variable, in regression tasks we can extract many meaningful statistics like median, percentiles, standard deviation of target variable. We could also create bins: assume that the target variable is distributed between 1 and 100; then we can create 10 bins, the first one counting the number of times the target value falls between 1 and 10, the second one between 11 and 20, and so on...\n",
        "\n",
        "How do we do that in domains with **many-to-many relationships**? Consider the following dataset, in which each user uses certain apps, and each app is used by more users.\n",
        "\n",
        "| User_id |       Apps       | Target |   \n",
        "|:-------:|:----------------:|:------:|\n",
        "|    1    | app1, app2, app3 |    0   |\n",
        "|    2    |    app1, app4    |    1   |\n",
        "|    3    |       app2       |    1   |\n",
        "\n",
        "\n",
        "We can take a cross product and compute a _long representation_:\n",
        "\n",
        "| User_id |      App_id      | Target |\n",
        "|:-------:|:----------------:|:------:|\n",
        "|    1    |       app1       |    0   |\n",
        "|    1    |       app2       |    0   |\n",
        "|    1    |       app3       |    0   |\n",
        "|    2    |       app1       |    1   |\n",
        "|    2    |       app4       |    1   |\n",
        "|    3    |       app2       |    1   |\n",
        "\n",
        "Finally, we perform mean encoding as we did initially with the categorical feature and the binary target variable.\n",
        "\n",
        "How can we encode **interaction features** and **numerical** ones? In order to discover the most relevant feature interactions, we can fit a tree model without any encodings, then we look for numerical features with many splits (meaning that they have some complicated dependency with the target) and we mean encode them. The split points can be used to bin the feature. Regarding interaction features, we first need to detect which features interact the most. Two features interact in a tree model, if they are in two neighboring nodes (parent and child). With that in mind, we can iterate among every tree of the model and count how many times each feature interaction appears. The most frequent interactions are probably worth of mean encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obzNyqSoSXoY"
      },
      "source": [
        "## Missing values\n",
        "\n",
        "We can consider as missing values not only `NaN` values, but also empty strings, special values like `-1`, outliers like `-999` and many more. The presence of missing values can provide insights on why they occurred. The first step is to identify them, and that can be done by inspecting the histograms of the variables.\n",
        "\n",
        "Once we identified them, we must decide what to do:\n",
        "- Replace them with a value outside of the feature's values range (it can help tree-based models, but may worsen the performance of linar models or neural networks).\n",
        "- Replace them with the mean/median of the values of the feature involved.\n",
        "- Add new boolean feature `is_missing` according the the presence/absence of the value.\n",
        "- Reconstruct their value (e.g. with time series, sometimes it can make sense to interpolate nearby points in order to estimate a reasonable value for the missing one.\n",
        "\n",
        "Be careful when generating new features using missing values: for example, if we chose to replace a missing value of a numeric feature with `-999`, then we can't encode a different categorical feature using the average values from the numeric feature, because we would end up in a situation like this, doing more harm than good:\n",
        "\n",
        "| categorical_feature | numeric_feature | numeric_filled | categorical_encoded |\n",
        "|:-------------------:|:---------------:|:--------------:|:-------------------:|\n",
        "|          A          |        1        |        1       |         1.5         |\n",
        "|          A          |        4        |        4       |         1.5         |\n",
        "|          A          |        2        |        2       |         1.5         |\n",
        "|          A          |        -1       |       -1       |         1.5         |\n",
        "|          B          |        9        |        9       |         -495        |\n",
        "|          B          |       NaN       |      -999      |         -495        |\n",
        "\n",
        "In conclusion, there are algorithms, like XGBoost that are able to handle NaNs by themselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0dNDlqeSXoY"
      },
      "source": [
        "## Feature extraction from text and images\n",
        "\n",
        "Sometimes some of our dataset's attributes may be text attributes or images. In that case, we need to extract meaningful features from those attributes.\n",
        "\n",
        "### Text to vectors\n",
        "\n",
        "#### Bag of words\n",
        "\n",
        "Suppose we have a text document. Bag of words consists of creating a new feature for each work encountered in the document, and then counting the number of occurrences of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAmtElZFSXoY",
        "outputId": "9bc79581-ac03-4e77-87a5-cb6df259691d"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Hello Anna, how are you?\", \n",
        "    \"Are you hungry or are you thirsty?\", \n",
        "    \"Hello?! HELLO!?! I said you hello!\",\n",
        "    ]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "df = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anna</th>\n",
              "      <th>are</th>\n",
              "      <th>hello</th>\n",
              "      <th>how</th>\n",
              "      <th>hungry</th>\n",
              "      <th>or</th>\n",
              "      <th>said</th>\n",
              "      <th>thirsty</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   anna  are  hello  how  hungry  or  said  thirsty  you\n",
              "0     1    1      1    1       0   0     0        0    1\n",
              "1     0    2      0    0       1   1     0        1    2\n",
              "2     0    0      3    0       0   0     1        0    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "288wOL1hSXoY"
      },
      "source": [
        "We can also post-process the calculated matrix, this is because of the dependence on scaling of linear models. We want to make samples comparable, and, at the same time, to boost more important features, decreasing the scale of useless ones. For this purpose we can compute the ***terms frequencies*** in every entry, in order to replace occurrences with frequencies. Doing that will make the values along each row to sum up to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WunB0FhSSXoZ",
        "outputId": "69df00b3-d7f7-4c34-85bd-fc5385c53ecb"
      },
      "source": [
        "tf = (1 / df.sum(axis=1)).to_numpy() [:,None]\n",
        "df_tf = df * tf\n",
        "df_tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anna</th>\n",
              "      <th>are</th>\n",
              "      <th>hello</th>\n",
              "      <th>how</th>\n",
              "      <th>hungry</th>\n",
              "      <th>or</th>\n",
              "      <th>said</th>\n",
              "      <th>thirsty</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   anna       are  hello  how    hungry        or  said   thirsty       you\n",
              "0   0.2  0.200000    0.2  0.2  0.000000  0.000000   0.0  0.000000  0.200000\n",
              "1   0.0  0.285714    0.0  0.0  0.142857  0.142857   0.0  0.142857  0.285714\n",
              "2   0.0  0.000000    0.6  0.0  0.000000  0.000000   0.2  0.000000  0.200000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThNI3gYSXoZ"
      },
      "source": [
        "Another approach consists of applying the ***[Tf-idf](https://en.wikipedia.org/wiki/Tf–idf) (term frequency–inverse document frequency)***, which consists in normalizing data column-wise by the logarithmically scaled inverse fraction of the documents that contain the word corresponding to a certain feature. We can use [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) or compute them manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLEXud7iSXoZ",
        "outputId": "7f20bb7d-1a5e-47b9-86a3-cd28b518d113"
      },
      "source": [
        "idf = np.log(df.shape[0] / (df > 0).sum(axis=0))\n",
        "df_idf = df * idf\n",
        "df_idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anna</th>\n",
              "      <th>are</th>\n",
              "      <th>hello</th>\n",
              "      <th>how</th>\n",
              "      <th>hungry</th>\n",
              "      <th>or</th>\n",
              "      <th>said</th>\n",
              "      <th>thirsty</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.810930</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.216395</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       anna       are     hello       how    hungry        or      said  \\\n",
              "0  1.098612  0.405465  0.405465  1.098612  0.000000  0.000000  0.000000   \n",
              "1  0.000000  0.810930  0.000000  0.000000  1.098612  1.098612  0.000000   \n",
              "2  0.000000  0.000000  1.216395  0.000000  0.000000  0.000000  1.098612   \n",
              "\n",
              "    thirsty  you  \n",
              "0  0.000000  0.0  \n",
              "1  1.098612  0.0  \n",
              "2  0.000000  0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsJP-go4SXoZ"
      },
      "source": [
        "Note what happened in the last column (corresponding to \"you\"): since all three documents contained the word \"you\", tf-idf scaled down that feature to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69zyf-QUSXoa"
      },
      "source": [
        "#### N-grams\n",
        "\n",
        "N-grams works in a similar way to bag of words, except for the fact that now, we don't only add new features corresponding to single words, but also features corresponding to sequence of *n* words. The same concept can be applied to sequences of characters. In scikit learn we can tune the attributes `ngram_range` and `analyzer` of [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORR_ZNnNSXoa",
        "outputId": "7445bf53-5a5f-4865-bca9-59917c5190f8"
      },
      "source": [
        "corpus = [\n",
        "    \"Hello Anna, how are you?\", \n",
        "    \"Are you hungry or are you thirsty?\", \n",
        "    \"Hello?! HELLO!?! I said you hello!\",\n",
        "    ]\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='word')\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "df = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anna</th>\n",
              "      <th>anna how</th>\n",
              "      <th>are</th>\n",
              "      <th>are you</th>\n",
              "      <th>hello</th>\n",
              "      <th>hello anna</th>\n",
              "      <th>hello hello</th>\n",
              "      <th>hello said</th>\n",
              "      <th>how</th>\n",
              "      <th>how are</th>\n",
              "      <th>...</th>\n",
              "      <th>hungry or</th>\n",
              "      <th>or</th>\n",
              "      <th>or are</th>\n",
              "      <th>said</th>\n",
              "      <th>said you</th>\n",
              "      <th>thirsty</th>\n",
              "      <th>you</th>\n",
              "      <th>you hello</th>\n",
              "      <th>you hungry</th>\n",
              "      <th>you thirsty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   anna  anna how  are  are you  hello  hello anna  hello hello  hello said  \\\n",
              "0     1         1    1        1      1           1            0           0   \n",
              "1     0         0    2        2      0           0            0           0   \n",
              "2     0         0    0        0      3           0            1           1   \n",
              "\n",
              "   how  how are  ...  hungry or  or  or are  said  said you  thirsty  you  \\\n",
              "0    1        1  ...          0   0       0     0         0        0    1   \n",
              "1    0        0  ...          1   1       1     0         0        1    2   \n",
              "2    0        0  ...          0   0       0     1         1        0    1   \n",
              "\n",
              "   you hello  you hungry  you thirsty  \n",
              "0          0           0            0  \n",
              "1          0           1            1  \n",
              "2          1           0            0  \n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6vJlQvySXoa"
      },
      "source": [
        "#### Text preprocessing\n",
        "\n",
        "In order to help bag of words, we should perform pre-processing of our text via:\n",
        "- Lowercase: converting the text to lowercase, in order not to treat words like \"hello\", \"Hello\" and \"HEllO\" differently.\n",
        "- Stemming\n",
        "- Lemmatization\n",
        "- Stopwords\n",
        "\n",
        "[Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) have a similar goal which is to group similar words together.\n",
        "\n",
        "##### Stemming\n",
        "\n",
        "Stemming is a heuristic process used to chop off the suffixes of different words sharing the same root, for example *\"organize\"*, *\"organizes\"*, *\"organizing\"*, *\"organization\"*, they are all sharing a common root. Performing stemming will chop those words to *\"organiz\"*.\n",
        "\n",
        "##### Lemmatization\n",
        "On the other hand, lemmatization does a similar job but more carefully, taking into account also the morphological analysis of words and aiming at finding the meaning shared by all the words. For example, if we had *\"democracy\"*, *\"democratic\"* and *\"democratization\"* it would identify the word *\"democracy\"* as the shared concept between the three.\n",
        "\n",
        "##### Stopwords\n",
        "\n",
        "Stopwords are words that do not contain important information. They are either insignificant like articles and prepositions, or they are very common words that do not happen to help us with our task. There exists many libraries that allow to identify stopwords, such as [NLTK](https://www.nltk.org). Scikit-learn has some basic functionality for that, tunable via the parameter `max_df` of `sklearn.feature_extraction.text.CountVectorizer`, that is the maximum frequency, above which the word can be removed.\n",
        "\n",
        "In conclusion, we can build a *pipeline* to process text-data which operates in this way:\n",
        "1. Pre-processing: lowercase, stemming, lemmatization, stopwords.\n",
        "2. Build Bag of words or N-grams\n",
        "3. Post-process: TFiDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_54TvcRwSXoa"
      },
      "source": [
        "#### Word2vec (Embeddings)\n",
        "\n",
        "Word to vector is used to get a vector representation of words and texts, in a more concise way than with bag of words and n-grams. It converts words into vector of a space that usually has several hundreds of dimensions. Words that are usually used in the same context, are close together in this vector representation. Operations on those vectors (such as sums or subtractions) are usually interpretable.\n",
        "\n",
        "There exist many implementations of this concept, like Word2vec, Glove, FastText... \n",
        "\n",
        "If we needed to derive vectors for sentences instead of words, we could use models like [Doc2vec](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn_14uXrSXoa"
      },
      "source": [
        "#### To summarize\n",
        "        \n",
        "| **Method**       | **Pros**                                                                                         | **Cons**                                          |\n",
        "|--------------|----------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
        "| **Bag of words** | the meaning of each value in the vector is known                                             | works with very large vectors                 |\n",
        "| **Word2vec**     | works with relatively small vectors <br> words with similar meaning often have similar embeddings | vectors can be interpreted only in some cases |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFy7EfVFSXoa"
      },
      "source": [
        "## Statistics and distance based features\n",
        "\n",
        "Consider a CTR (click through rate) task, in which the dateset looks like the following one:\n",
        "\n",
        "<img src=\"https://github.com/pietroventurini/machine-learning-notes/blob/main/images/data_analysis/example_ctr.png?raw=1\" alt=\"example CTR dataset\" style=\"display:block; margin-left:auto; margin-right:auto; width:25em\"/>\n",
        "\n",
        "The most straightful way to approach this problem is to label encode the categorical variable `ad_position` and fit a classifier. No matter how good that classifier will turn out to be, it will still treat every data point independently. Notice in the dataset that many rows correspond to the same user interacting with the same page. Likely, the ad with the lowest price on a page will catch most the user attention. Therefore, let's create the features that are more relevant to such implication, for example, we can add the lowest and the highest price of an ad for every user-page pair, together with the position of the minimum price ad.\n",
        "\n",
        "In pandas we can do that in this way:\n",
        "\n",
        "```python\n",
        "gb = df.groupby(['user_id','page_id'], as_index=False).agg({'ad_price':{'max_price':np.max, 'min_price':np.min}})\n",
        "gb.columns = ['user_id', 'page_id', 'min_price', 'max_price']\n",
        "df = pd.merge(df, gb, how='left', on=['user_id','page_id'])\n",
        "```\n",
        "\n",
        "obtaining the following result:\n",
        "\n",
        "<img src=\"https://github.com/pietroventurini/machine-learning-notes/blob/main/images/data_analysis/example_ctr_features.png?raw=1\" alt=\"example CTR dataset\" style=\"display:block; margin-left:auto; margin-right:auto; width:30em\"/>\n",
        "\n",
        "We may also add features as:\n",
        "- how many pages a user visited during a session\n",
        "- how many times a user visided a certain page\n",
        "- standard deviation of prices\n",
        "- many more...\n",
        "\n",
        "When there are no features to perform a group-by on, we can adopt neighborhood-based methods in order to generate features like:\n",
        "- number of houses in the _x_ meters neighborhood of each house\n",
        "- average price per square meter in the _x_ meters neighborhood\n",
        "- number of schools/supermarket in the _x_ meters neighborhood\n",
        "- distance to the closest subway station\n",
        "- many more...\n",
        "\n",
        "Neighborhooding does not limit itself to variables based on geographical coordinates, but it can be applied also in more abstract (or even anonymized) feature spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEotYA-ISXob"
      },
      "source": [
        "## Interaction features\n",
        "\n",
        "### Categorical features\n",
        "Consider two categorical features $f_1$ and $f_2$:\n",
        "\n",
        "| f1 | f2 |\n",
        "|:--:|:--:|\n",
        "|  A |  X |\n",
        "|  B |  Y |\n",
        "|  B |  Z |\n",
        "|  A |  Z |\n",
        "\n",
        "Interactions between $f_1$ and $f_2$ can be computed in two ways:\n",
        "- Concatenate the values of the two columns, then one-hot encode the new column:\n",
        "\n",
        "| A\\|X | B\\|Y | B\\|Z | A\\|Z |\n",
        "|:----:|:----:|:----:|:----:|\n",
        "|   1  |   0  |   0  |   0  |\n",
        "|   0  |   1  |   0  |   0  |\n",
        "|   0  |   0  |   1  |   0  |\n",
        "|   0  |   0  |   0  |   1  |\n",
        "\n",
        "- One-hot encoding both variables first, then compute the interaction between each of the one-hot encodings:\n",
        "\n",
        "| A\\|X | B\\|Y | B\\|Z | A\\|Z |\n",
        "|:----:|:----:|:----:|:----:|\n",
        "|   1  |   0  |   0  |   0  |\n",
        "|   0  |   1  |   0  |   0  |\n",
        "|   0  |   0  |   1  |   0  |\n",
        "|   0  |   0  |   0  |   1  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCpws57zSXob"
      },
      "source": [
        "### Numerical features\n",
        "\n",
        "Interactions between two numerical features $f_1$ and $f_2$ can be computed by multiplying the values of the two columns.\n",
        "\n",
        "|  f1 |  f2  | f1f2  |\n",
        "|:---:|:----:|:-----:|\n",
        "| 1.2 |  0.0 |  0.0  |\n",
        "| 3.4 |  0.1 |  0.34 |\n",
        "| 5.6 |  1.0 |  5.6  |\n",
        "| 7.8 | -1.0 |  -7.8 |\n",
        "\n",
        "We are not limited to multiplication, but we can also sue sum, difference and division.\n",
        "\n",
        "Note that the number of interaction features that may rise from $N$ variables is equal to $N^2$ (but can grow even larger). In order to reduce that number, we can either use **dimensionality reduction** techniques, or perform **feature selection**. Sometimes, even with just a subset of the possible feature interactions we can achieve the same score as with every possible combination. So, the general pipeline of interaction generation, is to compute feature interaction terms using sums, subtractions, multiplications and division, fit a tree based model, get feature importances and select just few of the most important interaction terms to include into our model.\n",
        "\n",
        "A final remark, is that we can construct interactions of higher order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEZhXTUvSXob"
      },
      "source": [
        "<a name='Data-leakages'></a>\n",
        "\n",
        "# Data leakages\n",
        "\n",
        "A data leakage, or leak, can be defined informally as unexpected information in the data that allows us to make unrealistic good predictions. We can think of it as of directly or indirectly adding ground thruth into the test data. Data leaks are completely unusable in the real world but, sometimes, they happen to be found and exploited in competitions such as Kaggle competitions. Please, note that exploiting data leakages is completely against the main point of competitions. In this section we'll discover some of the most common types of data leaks with respect to common machine learning competitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2rUNG4iSXob"
      },
      "source": [
        "## Data leaks in time series\n",
        "\n",
        "With time series, it is common sense not to pick features from the future in order to predict something that happened before (for instance, we don't use future prices to predict previous ones  ). This is due to the fact that we almost always deal with [**causal**](https://en.wikipedia.org/wiki/Causality_(physics)) systems, namely systems in which effects cannot occur before their causes. However, useful future informations and incorrect time splits still exists. When we enter a time series competition, it is a good practice to first check train, public and private splits. If even one of them is not based on time, then we have found a data leak. In such cases, unrealistic features like future prices will be the most important features. \n",
        "\n",
        "Note that, even when the split is based on time, features may still contain information about future. For instance, in a _click-through rate_ task, where we are asked to predict the ratio of users clicking on a specific link to the ratio of total users visiting that page, user future history can describe it perfectly. Another example were a competition in which the goal was to predict whether the content of a HTML page was sponsored or not. There was a data leak in archive dates: we could assume that sponsored and non-sponsored HTML files were gotten during different periods of time. Anyway, even if the date of each archive was not directly reported, it was still possible to infer it by looking at elements inside each page: from explicit timestamps to much more subtle things, like news contents.\n",
        "\n",
        "Competitions in which one cannot access rows from the future, or test sets with no features at all other than  IDs, are the only way to prevent leaks of this kind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK8OnYBFSXob"
      },
      "source": [
        "## Unexpected information\n",
        "\n",
        "Sometimes, meta information such as file creation date, image resolution, etc., may be connected to the target variable. For example, consider the task of predicting wheter a picture contains a dog or a cat. If all the cats' photos were taken with a specific camera (different from the one used for the dogs), or before taking the pictures of the dogs, than, those informations can be used to make perfect predictions about unseen data. Because of that, a good practice for the organizer of a competition, is to erase the metadata, resize the pictures and change creation date.\n",
        "\n",
        "Useful information can be exploited also from IDs. It makes no sense to include them into the model, because we typically assume that they are automatically generated. Anyway, in reality, that is not always true: IDs may be the hash of something which is probably not intended for disclosure. Therefore, they may contain traces of information connected to target variable, so, including them into the model can improve the results.\n",
        "\n",
        "Another source of information is the order of the rows. In trivial cases they can be shuffled by target variable, so, by including the row index or the row relative index, we may observe improvements in the score. In other cases, there may be some kind of row duplication, where rows next to each other usually have the same label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NamxIIXqSXoc"
      },
      "source": [
        "## Leaderboard probing\n",
        "\n",
        "Leaderboard probing is a competition-specific technique which can be distinguished between two types:\n",
        "- Extracting all the ground truth from the public part of the leaderboard. [Here](https://www.kaggle.com/olegtrott/the-perfect-score-script) you can find a description of how to efficently do it.\n",
        "- Despite the split of the test set into a public and a private part (in order to protect the private one from information extraction), the private test set is still vulnerable to information extraction. Sometimes we can submit predictions in such a way to obtain information about private data. For instance, consider a binary classification problem, in which the dataset contains multiple rows with the same ID and so, with the same target value. If that dataset have been split into a public and a private part, then, by fixing the same target value for a certain chunk of entries, we can infer the correct target value by observing if our score improves or decreases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwjq2txQSXoc"
      },
      "source": [
        "## Example: Expedia Kaggle competition\n",
        "\n",
        "In the [Expedia Kaggle competition](https://www.kaggle.com/c/expedia-hotel-recommendations) we were required to predict which hotel group users was going to book according to information such as what customers searched for, how they interacted with search results, clicks or books and whether or not the search result was a travel package. The prediction target was _the hotel group_. This competition had a very non-trivial and extremely hard to exploit data leak. This leak allowed people to match specific rows containing user and hotel location in the training data with those in the testing set, resulting in 33% of the test data being able to be predicted 90% of the time.\n",
        "\n",
        "Among the features there was one called `orig_destination_distance` describing the distance between the user's city and the hotel he clicked or booked. Unique `orig_destination_distance` - `user_city` pairs corresponded to unique hotels. So, we could treat user city and destination distance pair as a proxy to our target. When in this set we encountered such pair already present in the train set, we could simply take a label from there as our prediction. However, nearly half of test set consisted from new pairs without a match from train set. Therefore, we needed to explore new strategies to improve our solution. That can be done in two ways:\n",
        "\n",
        "The first way consists in creating current features on corteges similar to `orig_destination_distance` - `user_city` pairs, for instance, how many hotels of which group there are for `user_city` - `hotel_country` - `hotel_city` triplets. Then, we train a ML model on such features.\n",
        "\n",
        "The second method is to find more matches: first we need to find true coordinates for `user_city` and `hotel_city`. From that, in order to guess the `orig_destination_distance` feature, we can try to guess the `orig_destination_distance` in this way: since we are working with geographical coordinates, the distances are geodesic, so, we can use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula). Suppose that we know true coordinates ofthree points and distances from fourth point with unknown coordinates to each of them, if we write down a system of three equations, one for each distance, we can unambiguously solve it and get true coordinates for the fourth point. Now, we need to reverse engineer the true coordinates of three big cities. After that, we can iteratively find the coordinates of more and more cities. Unfortunately, after each iterations, the rounding error keeps increasing and the accuracy decreases. A possible workaround to this issue is not to limit ourselves to a system of three equations, but creating a very large system of equations from all the known distances with true coordinates being the variables. We end up with literally hundreds or thousands of equations and tens of thousands of unknown variables. Luckily, this system is very sparse, allowing us to apply appropriate methods from the SciPy library to efficiently solve such a system. By doing that, we end up with very precise coordinates for both hotel cities and user cities. Using city coordinates and destination distance, we can approximate the true coordinates of an actual hotel. Given a user city, we draw a circumference arount it with radius equal to `orig_destination_distance`, and then we look over the circumference to find the true hotel location. Now, let's fix some hotel city and draw such circumferences from all user cities to that fixed hotel cities, and draw them for every given destination distance. The hotels should be on intersection points and the more circumferences intersect in such point, the higher the probability of a hotel being there. However, the circumference did not intersect perfectly in the same point, so we need to proceed in the following way: for every city we create a grid around its center (e.g. 10km times 10km with step size of 100m), then, using training data, for every cell in the grid we count how many hotels of each type fall within that area by increasing by one the counter corresponding to that cell, every time a circumference went through it. For each of those cells, we create features like the sum of all counters, their average, the maximum and so on.\n",
        "\n",
        "The winner of the competition adopted a similar approach, which he described in [this discussion](https://www.kaggle.com/c/expedia-hotel-recommendations/discussion/21607)."
      ]
    }
  ]
}