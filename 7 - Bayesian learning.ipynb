{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning\n",
    "\n",
    "## Contents\n",
    "1. **Introduction**  \n",
    "2. **The Bayes theorem**  \n",
    "    2.1. Maximum a posteriori (MAP)  \n",
    "    2.2. Relation with concept learning  \n",
    "    2.3. Bayes optimal classifier   \n",
    "3. **Gibbs classifier**  \n",
    "4. **Naive Bayes classifiers**  \n",
    "    4.1. The naive Bayes algorithm  \n",
    "    4.2. Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Bayesian learning is a set of approaches to machine learning that, differently from the learning approaches that we have encountered till now, which aim to find a single hypothesis, associates instead a probability to each hypothesis to being correct. It is based on the concept of conditional probability. With a Bayesian approach, it is possible to combine prior knowledge with observed data. Bayesian learning treats model parameters as random variables. Consequently, parameter estimation amounts to computing posterior distributions for these random variables based on the observed data. Even if they are not applicable in certain contexts, because of the large demand of probabilistic estimations, they are adopted from a theoretical perspective because they provide the so-called *gold standard*, a theoretical landmark for evaluating other learning algorithms.\n",
    "\n",
    "We need few concepts from probability theory:\n",
    "- **Chain (or product) rule**: given two random events $A$ and $B$, their joint probability is\n",
    "\n",
    "$$P(A\\cap B) = P(B|A)\\cdot P(A).$$\n",
    "\n",
    "In the general case, given the events $A_1,A_2,...,A_n$, the chain rule extends to the formula\n",
    "\n",
    "$$P(A_n\\cap ... \\cap A_1) = P(A_n | A_{n-1} \\cap ... \\cap A_1) \\cdot P(A_{n-1} \\cap ... \\cap A_1),$$\n",
    "\n",
    "which, by induction may be turned into\n",
    "\n",
    "$$P(A_n \\cap ... \\cap A_1) = \\prod_{k=1}^n{P\\left(A_k \\big\\vert \\bigcap_{j=1}^{k-1} A_j \\right)} .$$\n",
    "\n",
    "- **Sum rule (or addition law)**: given two random events $A$ and $B$, the probability that $A$ **or** $B$ will happen is:\n",
    "\n",
    "$$P(A\\cup B) = P(A) + P(B) - P(A\\cap B).$$\n",
    "\n",
    "- **Law of total probability:** if events $A_1,...,A_n$ are mutually exclusive with $\\sum_{i=1}^nP(A_1)=1$, then for every event $B$ of the same probability space\n",
    "\n",
    "$$P(B) = \\sum_{i=1}^n {P(B\\cap A_i)} = \\sum_{i=1}^n P(B|A_i)P(A_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bayes theorem\n",
    "The Bayes theorem, formulated by Thomas Bayes (1701-1761), allows to compute a posterior probability for an event, from a prior probability, according to given observations. The theorem states:\n",
    "\n",
    "$$ P(h | D) = \\frac{P(D | h) P(h)}{P(D)}$$\n",
    "\n",
    "where\n",
    "- $P(h)$ = prior probability of hypothesis $h$;\n",
    "- $P(D)$ = prior probability of training data $D$, also called *evidence*;\n",
    "- $P(h|D)$ = probability of $h$ given $D$ (posterior probability);\n",
    "- $P(D|h)$ = probability of $D$ given $h$.\n",
    "\n",
    "Generally, we want the most probable hypothesis given $D$. In the context of a classification problem, the posterior probability can be interpreted as the probability that a particular object belongs to class $i$ given its observed feature values. Note that, in constrast to a frequentist's approach, here we introduced a *prior probability*, that can be interpreted as the *prior belief* or as the *a priori knowledge*.\n",
    "\n",
    "### Example\n",
    "Let's suppose that a patient gets tested positive ($+$) for a certain disease that affects $1$ every $10000$ people, with a test having $99%$ accuracy. Therefore,\n",
    "- Sick: $P(S)=0.0001$\n",
    "- Healthy: $P(H)=0.9999$\n",
    "- Tested Positive: $P(+) = P(S)P(+|S) + P(H)P(+|H)$\n",
    "- $P(+|S) = 0.01$\n",
    "- $P(+|H) = 0.99$\n",
    "\n",
    "$$P(S|+) = \\frac{P(S)\\cdot P(+|S)}{P(S)P(+|S) + P(H)P(+|H)}=0.0098 < 1\\%$$\n",
    "\n",
    "\n",
    "## Maximum a posteriori\n",
    "A simple estimation method, is the *Maximum a posteriori (MAP)*, which is closely related to the method of *maximum likelihood (ML)* estimation (MAP can be seen as a regularization of ML estimation, because it incorporates a prior distribution into its objective function).\n",
    "\n",
    "$$h_{MAP} = \\underset{h\\in H}{\\mathrm{argmax}}P(h|D) = \\underset{h\\in H}{\\mathrm{argmax}} \\frac{P(D|h)\\cdot P(h)}{P(D)} = \\underset{h\\in H}{\\mathrm{argmax}} P(D|h)P(h),$$\n",
    "\n",
    "because $P(D)$ does not depend on $h$.\n",
    "\n",
    "If we **assume a uniform probability distribution** over $H$, that is $P(h_i)=P(h_j)\\; \\forall h_i,h_j \\in H$, we can further simplify, and choose the *maximum likelihood* hypothesis\n",
    "\n",
    "$$h_{ML} = \\underset{h\\in H}{\\mathrm{argmax}} P(D|h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying MAP to the hypothesis learning problem\n",
    "We can adopt a brute force approach, by calculating the posterior probability for each hypothesis $h\\in H$:\n",
    "\n",
    "$$P(h|D) = \\frac{P(D|h)\\cdot P(h)}{P(D)},$$\n",
    "\n",
    "then choosing the hypothesis with the highest posterior probability:\n",
    "\n",
    "$$h_{MAP} = \\underset{h\\in H}{\\mathrm{argmax}} P(h|D).$$\n",
    "\n",
    "\n",
    "### Relation with concept learning\n",
    "\n",
    "Consider an instance space $X$, an hypothesis space $H$ and a training set $D$. Consider also the `FindS` algorithm (which has been introduced in the chapter *\\\"Concept Learning\\\"*), which outputs the most specific hypothesis from the version space $VS_{H,D}$ (the subset of hypotheses that are consistent with the training data in $D$). What is the difference between the hypotheies found by `FindS` and the MAP hypothesis produced by the Bayes rule?\n",
    "\n",
    "Assume that no noise affects training data and in the target function in $H$. Consider a fixed set of instances $\\langle x_1,...,x_m\\rangle$ and let $D$ be the set of binary classifications $D=\\langle c(x_1),...,c(x_m)\\rangle$.\n",
    "\n",
    "$$P(D|h) =  \n",
    "\\begin{cases}\n",
    "1 & \\text{if $h$ is consistent with $D$ (because no noisy data)} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Assume $P(h)$ to be the uniform distribution: $P(h) = \\frac{1}{|H|} \\forall h\\in H$ (note that `FindS` does not make this assumption).\n",
    "\n",
    "Then,\n",
    "\n",
    "$$P(D) = \\sum_{h_i\\in H} P(D|h_i)P(h_i) = \\sum_{h_i\\in VS_{H,D}}1\\cdot \\frac{1}{|H|} + \\sum_{h_i \\not\\in VS_{H,D}}0\\cdot \\frac{1}{|H|} = \\sum_{h_i\\in VS_{H,D}}1\\cdot \\frac{1}{|H|} = \\frac{|VS_{H,D}|}{|H|},$$\n",
    "\n",
    "therefore,\n",
    "\n",
    "$$P(h|D) = \\begin{cases} \n",
    "\\frac{1}{|VS_{H,D}|} & \\text{if $h$ is consistent with $D$ (i.e. $h\\in VS_{H,D}$)} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Definition (Consistent learner):** a learning algorithm that outputs $h$ with zero errors on the training examples.\n",
    "\n",
    "- Every consistent hypothesis has posterior probability $\\frac{1}{|VS_{H,D}|}$.\n",
    "- Every consistent hypothesis is a MAP hypothesis.\n",
    "- If we assume uniform prior probabilities and deterministic, noise free training data, then consistent learners output a MAP hypothesis.\n",
    "- Consistent learners can output MAP hypotheses also with different prior probability distributions.\n",
    "\n",
    "For example, `FindS` doesn't assume a uniform distribution for $H$, but it outputs a MAP hypothesis if we assume a probability distribution $P(h)$ that assigns $P(h_1)\\ge P(h_2)$ if $h_1$ is **more specific** than $h_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "With the following example, we want to highlight how the posterior probabilities evolve with observations incoming.\n",
    "\n",
    "Consider $5$ bags of candies, each containing a high, indefinite number of candies (the flavours are: Cherry = Red, Lime = Green). Each bag represents a single hypothesis. Assume the probabilities of the candies picks don't changes, because every time we pick a candy, we do not remove it from the bag.\n",
    "\n",
    "|       | Total Candies | Red candies | Green candies |\n",
    "|:-----:|:-------------:|:-----------:|:-------------:|\n",
    "| $h_1$ |      10%      |     100%    |       0%      |\n",
    "| $h_1$ |      20%      |     75%     |      25%      |\n",
    "| $h_1$ |      40%      |     50%     |      50%      |\n",
    "| $h_1$ |      20%      |     25%     |      75%      |\n",
    "| $h_1$ |      10%      |      0%     |      100%     |\n",
    "\n",
    "Let $d=\\{G,G,G,G,G,G,G,G,G,G\\}$ represent a drawn from a bag (i.e. 10 green candies).\n",
    "\n",
    "- What kind of bag is it? (what is the MAP hypothesis?)\n",
    "- What colour will the next candy be?\n",
    "\n",
    "The next graph plots the evolution of the posterior probabilities, as we pick candies from a certain bag. Note that when $d=0$, the posterior is equal to the prior probability. Observe that if we keep picking green candies, the hypothesis $h_5$ becomes more and more likely to be the correct one.\n",
    "\n",
    "\n",
    "Posteriors evolution            |  Probability of extracting a green candy\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"images/bayesian_learning/posteriors.png\" alt=\"Posterior probabilities\" style=\"width: 30em;\"/>  |  <img src=\"images/bayesian_learning/next_candies.png\" alt=\"Posterior probabilities\" style=\"width: 30em;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider now a new manufacturer producing bags with an arbitrary choiche of red/green candies. Each bag can be characterized by the parameter $\\theta \\equiv \\frac{\\text{n. of red candies}}{N} \\in [0,1]$. Consequently we have a continous space for hypotheses $h_\\theta$. Consider now an experiment consisting in $N=r+g$ picks of $r$ red candies and $g$ green candies.\n",
    "\n",
    "If we assume that the hypotheses are uniformly distributed (all proportions of red candies are equally likely a priori, then we can find the maximum likelihood hypothesis:\n",
    "\n",
    "$$h_{ML} = \\underset{h_\\theta}{\\mathrm{argmax}}P(d|h_\\theta) = \\underset{h_\\theta}{\\mathrm{argmax}}L(d|h_\\theta) $$\n",
    "\n",
    "where $L(d|h_\\theta) = \\log P(d|h_\\theta)$\n",
    "\n",
    "$$P(d|h_\\theta) = \\prod_{j=1}^N P(d_j | h_\\theta) = \\theta^r\\cdot(1-\\theta)^g$$\n",
    "\n",
    "$$L(d|h_\\theta) = r\\cdot \\log\\theta + g\\cdot \\log(1-\\theta)$$\n",
    "\n",
    "In order to find the maximum, we compute the derivative and set it equal to zero:\n",
    "\n",
    "$$\\frac{\\text{d}L(d|h_\\theta)}{\\text{d}\\theta} = \\frac{r}{\\theta} - \\frac{g}{1-\\theta}$$\n",
    "\n",
    "$$\\quad \\Rightarrow \\quad \\frac{r(1-\\theta)-g\\theta}{\\theta(1-\\theta)} = 0 \\quad \\Leftrightarrow \\quad r(1-\\theta) - g\\theta = 0 \\quad \\Leftrightarrow \\quad \\theta_{ML} = \\frac{r}{r+g} = \\frac{r}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Most probable classification of new instances\n",
    "\n",
    "So far we have sought the most probable hypothesis given the data $D$ (i.e. $h_{MAP}$). Given a new instance $x$, is $h_{MAP}(x)$ the most probable classification? Consider the case in which we have three hypotheses:\n",
    "\n",
    "- $P(h_1 | D) = 0.4$\n",
    "- $P(h_2 | D) = 0.3$\n",
    "- $P(h_3 | D) = 0.3$\n",
    "\n",
    "In this case $h_{MAP}=h_1$. Consider a new instance $x$ and assume that we obtain the following classifications:\n",
    "\n",
    "- $h_1(x) = True$\n",
    "- $h_2(x) = False$\n",
    "- $h_3(x) = False$\n",
    "\n",
    "If we consider just $h_{MAP}$, then the answer would be `True`. Anyway, if we consider all the hypotheses, we would predict `True`with probability $0.4$, and `False` with probability $0.3+0.3=0.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes optimal classifier\n",
    "Consider the classification of a new instance $x$ assuming target values $v_j\\in V$ (in the previous example we had $V=\\{True, False\\}$).\n",
    "\n",
    "$$P(v_j|D) = \\sum_{h_i \\in H}P(v_j|h_i)P(h_i|D)$$\n",
    "\n",
    "where $P(v_j|h_i)$ is the probability that $h_i$ predicts class $v_j$.\n",
    "\n",
    "The Bayes optimal classification is:\n",
    "\n",
    "\n",
    "$$v^* = \\underset{v_j\\in V}{\\mathrm{argmax}} \\sum_{h_i\\in H} P(v_j | h_i)P(h_i | D)$$\n",
    "\n",
    "Sticking with the previous example we now have:\n",
    "\n",
    "- $P(h_1 | D) = 0.4 \\quad P(False | h_1) = 0 \\quad P(True | h_1) = 1 $\n",
    "- $P(h_2 | D) = 0.3 \\quad P(False | h_2) = 1 \\quad P(True | h_2) = 0 $\n",
    "- $P(h_3 | D) = 0.3 \\quad P(False | h_3) = 1 \\quad P(True | h_3) = 0 $\n",
    "\n",
    "therefore,\n",
    "\n",
    "$$P(True | D) = \\sum_{h_i\\in H} P(True | h_i)P(h_i | D) = 0.4$$\n",
    "\n",
    "$$P(False | D) = \\sum_{h_i\\in H} P(False | h_i)P(h_i | D) = 0.6$$\n",
    "\n",
    "and\n",
    "\n",
    "$$v^* = \\underset{v_j\\in V}{\\mathrm{argmax}} \\sum_{h_i\\in H} P(v_j | h_i)P(h_i | D) = False$$\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The Bayes optimal classifier is the **optimal learner:** no other classification method using the same hypothesis space and the same prior knowledge can outperform this method on average (in terms of probability). \n",
    "- It maximizes the probability that the new instance is classified correctly. For example, in learning boolean concepts using version spaces, it takes a weighted vote among all the members of the $VS$, with each candidate hypothesis weighted by its posterior probability $\\left(\\frac{1}{|VS_{H,D}|}\\right)$. \n",
    "- Predictions made can correspond to a hypothesis not contained in $H$. Labeling new instances with $\\underset{v_j\\in V}{\\mathrm{argmax}}P(v_j|D)$ can correspond to none of the hypotheses in $H$, because Bayesian learning linearly combines multiple hypotheses (the hypotheses space is different)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs classifier\n",
    "Bayes optimal classifier provides best result, but can be expensive if we have a large number of hypothesis. To overcome this issue, we can use the **Gibbs algorithm**:\n",
    "\n",
    "1. Choose one hypothesis at random from the version space $VS$, according to the posterior probability distribution $P(h|D)$.\n",
    "2. Use this hypothesis to classify new instances.\n",
    "\n",
    "Surpisingly it works quite well. Assume that the target concepts are drawn at random from $H$ according to priors on $H$, then, on average\n",
    "\n",
    "$$\\mathbb{E}[error_{Gibbs}] \\le 2\\cdot\\mathbb{E}[error_{BayesOptimal}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "Naive Bayes classifiers, along with decision trees, neural networks, nearest neighborhood, are one of the most practical learning methods. They are linear classifiers based on the Bayes' theorem, and they are known for being simple yet very efficient. It is typically used when the training set is moderately large or when attributes that descrive instances are conditionally independent given classifications. Examples of its application are diagnosis, classification\n",
    "of RNA sequences in taxonomic studies and text documents classification such as spam email filtering. The adjective *naive* comes from the assumption that the variables (the features) in the dataset are mutually independent. This is a quite unrealistic assumption because, in practice, the independence assumption is often violated. Anyway, they still perform very well in many cases even under this assumption. Note that strong violations of the independence assumptions and\n",
    "non-linear classification problems can lead to very poor performances of this type of classifiers.\n",
    "\n",
    "**Definition (Conditional independence):** let $X,Y,Z$ be random events. $X$ is conditionally independent of $Y$ given $Z$ if the probability distribution governing $X$ is independent of the value of $Y$ given the value of $Z$. Mathematically speaking:\n",
    "\n",
    "$$P(X=x_i | Y=y_j, Z=z_k) = P(X=x_i | Z=z_k) \\quad \\forall x_i,y_i,z_k,$$\n",
    "\n",
    "that can be written in a more compact notation as:\n",
    "\n",
    "$$P(X|Y,Z) = P(X|Z)$$\n",
    "\n",
    "Naive Bayes uses contitional independency to justify\n",
    "\n",
    "$$P(X,Y|Z) = P(X|Y,Z)P(Y|Z) = P(X|Z)P(Y|Z)$$\n",
    "\n",
    "Check [this explanation](https://math.stackexchange.com/questions/23093/could-someone-explain-conditional-independence) out!\n",
    "\n",
    "\n",
    "Now let's go back to the *naive Bayes classifier*. Assume $f:X\\rightarrow V$ is the target function, where each instance $x$ is described by attributes $\\langle a_1,...,a_n\\rangle$, $V$ is a finite set and $d$ is the set of instances (the dataset).\n",
    "\n",
    "The most probable value $f(x)$ of a new instance $x$ given the training dataset $d$ is:\n",
    "\n",
    "$$\\begin{align}\n",
    "v_{MAP} &= \\underset{v_j\\in V}{\\mathrm{argmax}} P(v_j | a_1, \\dots, a_n, d) \\notag \\\\\n",
    "&= \\underset{v_j\\in V}{\\mathrm{argmax}} \\frac{P(a_1,\\dots,a_n | v_j,d)P(v_j|d)}{P(a_1,\\dots,a_n | d)} \\qquad \\text{(Bayes th.)} \\notag \\\\\n",
    "&= \\underset{v_j\\in V}{\\mathrm{argmax}} P(a_1,\\dots,a_n | v_j,d) P(v_j|d). \\qquad \\text{(Denominator independent from $v_j$)}\\notag\n",
    "\\end{align}$$\n",
    "\n",
    "We need a large number of traning instances in order to estimate these probabilities accurately. We simplify everything by making the **Naive Bayes assumption**:\n",
    "\n",
    "$$P(a_1,\\dots,a_n | v_j, d) = \\prod_{i=1}^n P(a_i | v_j, d)$$\n",
    "\n",
    "which gives an approximation of $v_{MAP}$. Consequently:\n",
    "\n",
    "$$v_{NB} = \\underset{v_j\\in V}{\\mathrm{argmax}} P(v_j | d) \\prod_{i=1}^n P(a_i | v_j, d).$$\n",
    "\n",
    "**Question:** how do we estimate $P(v_j | d)$ and $P(a_i | v_j, d)$?  In case of categorical data we count $v_j$ and $(v_j,a_j)$ frequencies in the training dataset $d$.\n",
    "\n",
    "$$\\hat{P}(a_i | v_j) = \\frac{N_{a_i,v_j}}{N_{v_j}}$$\n",
    "\n",
    "where:\n",
    "- $N_{a_i,v_j}$ is the number of times that feature value $a_i$ appears in samples from class $v_j$.\n",
    "- $N_{v_j}$ is the number of entries of class $v_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The naive Bayes algorithm\n",
    "\n",
    "<img src=\"images/bayesian_learning/naive_bayes_alg.png\" alt=\"Posterior probabilities\" style=\"width: 30em;\"/>\n",
    "\n",
    "then for a new instance $x=\\langle \\alpha_1,\\dots, \\alpha_n\\rangle$ we estimate its value using:\n",
    "\n",
    "$$v_{NB} = \\underset{v_j\\in V}{\\mathrm{argmax}} \\hat{P}(v_j | d) \\prod_{i=1}^n \\hat{P}(\\alpha_i | v_j, d).$$\n",
    "\n",
    "Note that:\n",
    "- The set of $\\hat{P}$'s and the $v_{NB}$ rule corresponds to the learned $h$.\n",
    "- No search in the hypothesis space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "#### Conditional independence assumption is often violated but it works well anyway\n",
    "We don't need estimated posteriors $\\hat{P}(v_j | x)$ to be correct, but we only need that:\n",
    "\n",
    "$$\\underset{v_j\\in V}{\\mathrm{argmax}} \\hat{P}(v_j | d) \\prod_{i=1}^n \\hat{P}(a_i | v_j, d) = \\underset{v_j\\in V}{\\mathrm{argmax}} {P}(v_j | d) \\prod_{i=1}^n {P}(a_i | v_j, d) $$\n",
    "\n",
    "\n",
    "#### Problems with attribute values missing in the traning examples\n",
    "If none of the training instances with target value $v_j$ have attribute value $a_i$ (before we referred with $a_i$ to the *i*-th attribute and with $\\alpha_i$ to its *i*-th value, now, since we are considering a single attribute $a$, we refer to its *i*-th value using $a_i$), then\n",
    "\n",
    "$$\\hat{P}(a_i | v_j, d)=0$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\\hat{P}(v_j | d)= \\prod_{i} \\hat{P}(a_i | v_j, d)=0.$$\n",
    "\n",
    "In order to overcome this problem, instead of estimate the probability of a certain attribute, given the target value, according to $\\frac{n_c}{n}$, we do it in this way:\n",
    "\n",
    "$$\\hat{P}(a_i | v_j) = \\frac{n_c+mp}{n+m}$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of training examples for which $v=v_j$.\n",
    "- $n_c$ is the number of examples for which $v=v_j$ and $a=a_i$.\n",
    "- $p$ is the prior estimate for $\\hat{P}(a_i | v_j)$ (e.g. uniform distribution: $1/|a_i|$).\n",
    "- $m$ is the weight given to prior, and it is called *equivalent sample size*, which is interpretable as augmenting the $n$ examples by $m$ virtual examples distributed as $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Consider the problem of detemining if someone is going to play tennis according to the value of some attributes. Our instance is:\n",
    "\n",
    "$$\\langle \\text{Outlook} = sun, \\text{Temp} = cool, \\text{Humid} = high, \\text{Wind} = strong \\rangle$$\n",
    "\n",
    "and we want to compute\n",
    "\n",
    "$$v_{NB} = \\underset{v_j\\in V}{\\mathrm{argmax}} {P}(v_j | d) \\prod_{i=1}^n {P}(\\alpha_i | v_j, d).$$\n",
    "\n",
    "The first thing to do is to estimate conditional probabilities:\n",
    "\n",
    "$$\\hat{P}(v_j|d) = \\frac{|\\{V=v_j\\}|}{|d|}$$\n",
    "\n",
    "- $\\hat{P}(\\text{PlayTennis}=yes | d) = P(y|d) = 9/14 = 0.64$\n",
    "- $\\hat{P}(\\text{PlayTennis}= no | d) = P(n|d) = 5/14 = 0.36$\n",
    "\n",
    "\n",
    "$$\\hat{P}(\\alpha_i|v_j, d) = \\frac{|\\{A=\\alpha_i, V=v_j\\}|}{|\\{V=v_j\\}|}$$\n",
    "\n",
    "- $\\hat{P}(\\text{Wind}= strong | \\text{PlayTennis}=yes, d) = 3/9 = 0.33$\n",
    "- $\\hat{P}(\\text{Wind}= strong | \\text{PlayTennis}=no, d) = 3/5 = 0.60$\n",
    "\n",
    "Then we compute the naive Bayes solution:\n",
    "\n",
    "$$v_{NB} = \\underset{v_j\\in V}{\\mathrm{argmax}} \\hat{P}(v_j | d) \\prod_{i=1}^n \\hat{P}(\\alpha_i | v_j, d)$$\n",
    "\n",
    "where:\n",
    "- $j\\in\\{1,2\\}$,\n",
    "- $v_1 = yes(y)$\n",
    "- $v_2 = no(y)$\n",
    "\n",
    "We compute both:\n",
    "\n",
    "- $\\hat{P}(y | d) \\hat{P}(sun | y,d) \\hat{P}(cool | y,d) \\hat{P}(high | y,d) \\hat{P}(strong |y,d) = 0.005$\n",
    "- $\\hat{P}(n | d) \\hat{P}(sun | n,d) \\hat{P}(cool | n,d) \\hat{P}(high | n,d) \\hat{P}(strong |n,d) = 0.021$\n",
    "\n",
    "allowing us to conclude that $v_{NB} = no$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Consider a dataset of $500$ documents, where $100$ of them are classified as spam. This is a binary classification problem with $V=\\{ham,spam\\}$, We now want to compute the class-conditional probability for the message `Hello world`, which consists in two words \"hello\" and \"world\". Assume that in our dataset, of the $100$ spam message, the word \"hello\" appears into $20$ of them, while the word \"world\" into $2$ of them. According to the naive Bayes assumption, we assume that, if we know that a message is spam, then the probability for the message to contain \"hello\" is independent from the probability for it to contain \"world\" (this is not a realistic hypothesis, however, naive Bayes classifiers are known to perform still well in those cases). Consequently:\n",
    "\n",
    "$$P(a=\\begin{bmatrix} \\text{hello,} & \\text{world}\\end{bmatrix} | v = \\text{spam}) = P(\\text{hello } | \\text{ spam}) \\cdot P(\\text{world } | \\text{ spam}).$$\n",
    "\n",
    "We now estimate the maximum likelihood according to frequencies:\n",
    "\n",
    "$$\\hat{P}(a=\\begin{bmatrix} \\text{hello,} & \\text{world}\\end{bmatrix} | v = \\text{spam}) = \\frac{20}{100}\\cdot \\frac{2}{100} = 0.004$$\n",
    "\n",
    "Recall that, for the Bayes' theorem:\n",
    "\n",
    "$$\\text{posterior probability} = \\frac{\\text{conditional probability} \\cdot \\text{prior probability}}{\\text{evidence}}$$\n",
    "\n",
    "But if we assume that the prior probability follows a uniform distribution (not realistic, we should instead consult a domain expert or estimate it by the training data, assuming that the training data is i.i.d. and a representative sample of the entire population, in that case it would be $\\hat{P}(\\text{spam})=100/500=0.2$), then the posterior probability is completely determined by the class conditional probability and by the evidence. But since the evidence term is a constant, then the decision rule will entirely depend on the class conditional probability (similar to a frequentist's approach and maximum-likelihood estimate).\n",
    "\n",
    "The decision rule for this particular example will be:\n",
    "\n",
    "```python\n",
    "if P(a | spam) >= P(a | ham):\n",
    "    return spam\n",
    "return ham\n",
    "```\n",
    "\n",
    "The posterior probability is the product between the class conditional probability and the prior probability:\n",
    "\n",
    "$$P(a | v = spam) = P(spam | a) \\cdot P(spam)$$\n",
    "$$P(a | v = ham) = P(ham | a) \\cdot P(ham)$$\n",
    "\n",
    "where the priors are estimated from the training set:\n",
    "- $\\hat{P}(v=spam) = \\frac{\\text{# spam messages}}{\\text{# messages}} = \\frac{100}{500} = 0.2$\n",
    "- $\\hat{P}(v=ham) = \\frac{\\text{# ham messages}}{\\text{# messages}} = \\frac{400}{500} = 0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
