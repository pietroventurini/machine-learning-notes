{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Introduction  \n",
    "2. The maximal margin classifier  \n",
    "3. The support vector classifier  \n",
    "    3.1 Bias-variance tradeoff  \n",
    "    3.2 Robustness  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The support vector machine is an approach to classification problems that has been developed in the 1990s as a generalization of a simpler classifier called the ***maximal margin classifier***. In the first part of this notebook, we will assume that the training data points are linearly separable and belong to only two classes (binary classification).\n",
    "\n",
    "Differently from other classification algorithms, for example logistic regression, SVMs, in addition to classifying data points (each of which is represented by a $p$-dimensional vector), aims to find the post possible boundary, that is the largest separation between the classes by means of a $(p-1)$-dimensional *hyperplane*.\n",
    "\n",
    "**Definition:** a *hyperplane* of a $p$-dimensional space $V$ is a flat affine subspace of dimension $p-1$, or equivalently, of codimension $1$ in $V$.\n",
    "\n",
    "**Example:** if $V$ is the vector space $\\mathbb{R}^3$, then a hyperplane is a flat two dimensional subspace, that is, a plane.\n",
    "\n",
    "A $p$ dimensional hyperplane has the form \n",
    "\n",
    "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p = 0.$$\n",
    "\n",
    "In the sense that, if a point $X = [X_1,\\dots,X_p]^\\top$ satisfy this equation, then $X$ lies on the hyperplane. If that equation is not satisfied, for example $\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p > 0$ then $X$ will lie on one side of the hyperplane.\n",
    "\n",
    "The best hyperplane which we are seeking is the one so that the distance from it to the nearest data point on each side is maximized. If such hyperplane exists, it is called the **maximum-margin hyperplane** and the linear classifier it identifies is called the *maximum-margin classifier*.\n",
    "\n",
    "\n",
    "~The cost function to minimize contains two contributions: one due to the *classification error* and the other due to the *margin error*.~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maximal margin classifier\n",
    "Consider a binary classification problem where our dataset consists into $n$ samples, each of which consists in $p$ features. Therefore $X \\in \\mathbb{R}^{n\\times p}$. The training samples are:\n",
    "\n",
    "$$x_1 = \\begin{bmatrix}x_{11} \\\\ \\vdots \\\\ x_{1p}\\end{bmatrix}, \\dots, x_n = \\begin{bmatrix}x_{n1} \\\\ \\vdots \\\\ x_{np}\\end{bmatrix},$$\n",
    "\n",
    "and they fall into two classes $y = \\{-1,1\\}$. We want to predict the output label of a new sample $x^* = \\begin{bmatrix}x_1^* & \\dots & x_p^*\\end{bmatrix}^\\top$. Similarly as what we have already seen in previous chapters, we seek a *separating hyperplane*, which has the property that, for all $n$,\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) > 0$$\n",
    "\n",
    "which is a compact instead of writing:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} > 0 & \\text{if } y_i = 1 \\\\\n",
    "\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} < 0 & \\text{if } y_i = -1\n",
    "\\end{cases}$$\n",
    "\n",
    "If a separating hyperplane exists, then we can build a simple classifier according to the side in which the observation we want to classify falls, that is the sign of $f(x^*) = \\beta_0 + \\beta_1x_1^* + \\dots + \\beta_px_p^*$. The *magnitude* of $f(x^*)$ can be used as a confidence measure about the class prediction. But which of the inifinitely many hyperplanes should be chosen? A natural choice is the *maximal margin hyperplane*, which is the fartest hyperplane from the training observations. For a fixed separating hyperplane, we can compute the perpendicular distance of each data point from it. The smallest distance is known the ***margin***. The maximal margin hyperplane is the separating hyperplane that has the farthest minimum distance from the observations.\n",
    "\n",
    "**Observation:** the maximal margin classifier with coefficients $\\beta_0, \\dots, \\beta_p$ can lead to overfitting when $p$ is large.\n",
    "\n",
    "In the following image we can observe the maximal margin hyperplane that has been found in a two dimensional space, with the corresponding margin.\n",
    "\n",
    "<img src='images/svm/support-vectors.png' alt='maximal margin hyperplane' style='width: 300px;'/>\n",
    "\n",
    "There are 3 points which are equidistant from the hyperplane. Those are called ***support vectors***. The chosen separating hyperplane depends directly on those three points but not on the other observations: moving around even one support vector may affect the position of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the maximal margin classifier\n",
    "The maximal margin hyperplane based on a set of $n$ observations is the solution to the optimization problem:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\max_{\\beta_0,\\beta_1,\\dots,\\beta_p, M}{M} \\notag\\\\\n",
    "&\\text{subject to} \\sum_{j=1}^{p}{\\beta_j^2} = 1, \\notag\\\\\n",
    "&y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\ge M \\quad \\forall i=1,\\dots,n\n",
    "\\end{align}$$\n",
    "\n",
    "The last constraint guarantees every observation to be on the right side of the hyperplane, where $M$ represents the margin of the hyperplane, which we want to maximize. The first constraint ensures that the perpendicular distance from the *i*-th observation to the hyperplane is given by $y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip})$. This fact comes to the formula of the [distance from a point to a hyperplane](https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane):\n",
    "\n",
    "$$\\frac{\\lvert\\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip}\\rvert}{\\lVert \\beta \\rVert}.$$\n",
    "\n",
    "In most cases no separating hyperplane exists, therefore the optimization problem we have just presented has no solution. Fortunately, the concept of separating hyperplane can be generalized to one that *almost* separates the classes, using a *soft margin*. This is what a support vector classifier does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The support vector classifier\n",
    "\n",
    "A classifier based on a maximal margin separating hyperplane may not be desirable since it would extrimely sensitive to changes in individual observations, which is a sign of overfitting. Indeed, we may face a situation in which all the positive training samples are all concentrated in a particular area of the, say, $\\mathbb{R}^2$ space except one, which is further away from the others and close to the negative training points. That outlier will considerably affect the hyperplane and its margin, which can be a tiny one, resulting in an unsatisfactory classifier.\n",
    "\n",
    "To cope with this issue, we will now seek for a hyperplane that does not perfectly separate the two classes, aiming at greater robustness to individual observations and better classification of most of the training observations. This is what a *support vector classifier* (also known as *soft margin classifier*) does: it allows some observations to be on the incorrect side of the margin, or even on the wrong side of the hyperplane. The classification still depends on which side of the hyperplane the point lies.\n",
    "\n",
    "It is the solution to the following optimization problem:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\max_{\\beta_0,\\beta_1,\\dots,\\beta_p, \\epsilon_1,\\dots,\\epsilon_n, M}{M} \\notag\\\\\n",
    "&\\text{subject to} \\sum_{j=1}^{p}{\\beta_j^2} = 1, \\notag\\\\\n",
    "&y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\ge M(1-\\epsilon_i) \\quad \\forall i=1,\\dots,n, \\\\\n",
    "&\\epsilon_i \\ge 0, \\quad \\sum_{i=1}^{n}{\\epsilon_i} \\le C\n",
    "\\end{align}$$\n",
    "\n",
    "where $C\\ge 0$ is a tuning parameter and $M$ is the margin's width. $\\epsilon_i$ are called **slack variables** and allow individual observations to be on the wrong side of the margin:\n",
    "- If $\\epsilon_i = 0$ then the $i$-th observation lies on the correct side of the margin.\n",
    "- If $\\epsilon_i > 0$ then the $i$-th observation *has violated the margin* because it is on the wrong side of it.\n",
    "- If $\\epsilon_i > 1$ then it is on the wrong side of the hyperplane.\n",
    "\n",
    "The parameter $C$ bounds the sum of the $\\epsilon_i$ terms, and so it determines how much the violation is tolerated. Note that if $C>0$, then no more than $C$ observations can be on the wrong side of the hyperplane.\n",
    "\n",
    "Only the observation that lie on the margin or that violate it affects the hyperplane. Therefore if we change the position of a point that strictly lies on the correct side of the margin (and we leave it on that side), then it won't affect the hyperplane, hence the classifier. The observations that are on the margin or on its wrong side are called ***support vectors*** and their position affects the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance tradeoff\n",
    "\n",
    "The parameter $C$ controls the *bias-variance tradeoff*. **If $C$ is small**, there will be fewer support vectors, thus we seek narrow margins that are rarely violated. The resulting classifier will highly fit the data, possibily resulting in a classifier with **low bias but high variance**. **If $C$ is large**, then the margin is wider and more observations are allowed to violate it. Consequently there will be many support vectors meaning that many observations are involved in determining the hyperplane. The resulting classifier will potentially have **low variance but high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness of the SVC\n",
    "\n",
    "Unlike other classification methods, such as [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis), that depends on all the observations, the support vector classifier depends only on a subset of points, thus is robust to the observations that are far away from the hyperplane, similarly to logistic regression, which has low sensitivity to points far away from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
