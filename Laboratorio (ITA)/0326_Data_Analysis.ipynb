{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis (26/03 - 02/04)\n",
    "Attributi possono essere qualitativi/quantitativi. Per attributi quantitavi va tenuta in considerazione l'unit√† di misura (si pensi ad una variabile altezza presente in due copie di un dataset: una in Europa, misurata in *cm* e l'altra in America ma convertita in *feet*. Questo fattore va tenuto in conto, ad esempio nel momento in cui decida di addestrare il mio algoritmo su un dataset Europeo, e successivamente testi il mio algoritmo con dati aventi unit√† di misura differente.\n",
    "## Propriet√† fondamentali\n",
    "Attributi possono essere distinti in:\n",
    "- **Nominali:** ID, colore, zip codes...\n",
    "- **Ordinali:** il loro valore √® una categoria ordinata e la distanza tra le categorie non √® nota, per esempio altezza (basso, medio, alto), ranking (da 1 a 10).\n",
    "- **Interval:** attributi su cui ha senso calcolare delle differenze (es: temperatura, data...)\n",
    "- **Ratio:** attributi su cui ha senso fare dei rapporti dei loro valori (es: temperatura in Kelvin perch√© non include lo zero, lunghezza, tempo...)\n",
    "\n",
    "Questa classificazione √® sostanzialmente basata sul tipo di operazioni che posso fare sugli attributi:\n",
    "- Uguaglianza: $=, \\neq$\n",
    "- Confronto ordinale: $<, \\le, >, \\ge$\n",
    "- Addizione: $+,-$\n",
    "- Moltiplicazione: $\\times, \\div$ \n",
    "\n",
    "Le **operazioni** effettuabili sui miei dati determineranno le tecniche di machine learning e data mining utilizzabili. Alcuni esempi di operazioni effettuabili:\n",
    "- Nominali: moda, entropia, test di correlazione $\\chi^2$...\n",
    "- Ordinali: mediana, calcolo di percentili, correlazione dei ranghi, test dei segni...\n",
    "- Interval: media, deviazione standard, coefficiente di Pearson, t test, F test...\n",
    "- Ratio: media geometrica, media armonica, variazione percentuale...\n",
    "\n",
    "Le **trasformazioni** effettuabili sui miei dati:\n",
    "- Nominali: permutazioni dei valori (se gli ID dei miei dipendenti venissero riassegnati farebbe qualche differenza?)\n",
    "- Ordinali: qualsiasi cambio dei valori che mantenga l'ordinamento (quindi $value_{new} = f(value_{old}$ tale che $f$ sia monotona)\n",
    "- Interval: qualsiasi operazione che mantenga la proporzione tra i vari intervalli (una funzione affine ad esempio)\n",
    "- Ratio: qualsiasi operazione che mantenga il rapporto dei vari valori (come una moltiplicazione per una costante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributi discreti e continui\n",
    "Discreti possono assumere un insieme finito di valori, Continui possono assumere un insieme infinito di valori (tenere comunque a mente che la rappresentazione e memorizzazione su un calcolatore comporter√† una discretizzazione di questi).\n",
    "\n",
    "## Tipologie di strutture dati\n",
    "Il dati possono essere memorizzati secondo diverse tipologie di struture che determineranno gli algoritmi di apprendimento applicabili\n",
    "- Un insieme di **record**: \n",
    "    - dati matriciali: ciascun record √® costituito da un insieme fisso di attributi\n",
    "    - testuali: ad esempio gli attributi possono essere delle parole e i record possono corrispondere a documenti. In ciascuna cella avr√≤ il numero di occorrenze di quella parola per quel particolare documento.\n",
    "    - transizioni di stato\n",
    "- Un **grafo**: WWW, strutture molecolari...\n",
    "- Dati **ordinati**: dati spaziali, temporali, sequenze...\n",
    "\n",
    "## Qualit√† dei dati\n",
    "Le misure sono tipicamente affette da rumore, quindi questo fattore √® da tenere presente. Prima di fare una qualunque analisi del nostro sistema √® opportuno porsi delle domande: √® presente del rumore nel nostro dataset? In quale misura? \n",
    "\n",
    "Vi sono degli *outliers* nel nostro dataset? Eventualmente potrei utilizzare la mediana piuttosto che la media, che √® meno influenzata dalla presenza di outliers. In tecniche di apprendimento non supervisionato, come il clustering, la presenza di outliers pu√≤ influenzare notevolmente l'accuratezza del risultato prodotto dall'algoritmo. Potrei quindi decidere di filtrare eventuali outliers.\n",
    "\n",
    "Va tenuto conto anche di valori mancanti (√® necessario capire se la loro mancanza abbia un certo significato, in modo da poter associare un valore di default evaentualmente) o duplicati. Spesso gli algoritmi di machine learning escludono i record con valori mancanti, ma questo potrebbe causare notevoli problemi qualora il numero di dati mancanti sia alto. Il motivo dell'assenza di alcuni valori pu√≤ essere dovuto a:\n",
    "- informazioni non raccolte: ad esempio in un questionario una persona potrebbe non voler specificare il sesso.\n",
    "- informazioni non applicabili: un salario ad un bambino\n",
    "\n",
    "In questi casi potrei cercare di dedurre il valore inferendolo da altri record simili a quello con il valore mancante oppure a partire da altri attributi dello stesso record (dall'altezza e dall'indice di massa corporea potrei inferire il peso). Posso cercare di eliminare record con troppi valori mancanti, fare una stima, ignorare tale attributo, rimpiazzarli con valori quantitativi con una probabilit√† associata.\n",
    "\n",
    "Eventuali duplicati (o dati molto simili) vanno gestiti attraverso un processo di **data cleaning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "Prima di applicare algoritmi di machine learning potrei effettuare operazioni di preprocessing dei dati al fine di facilitare la successiva gestione di tali dati.\n",
    "- **Aggregazione:** potrei cercare di aggregare tra loro pi√π attributi o pi√π record per ridurre il numero di dati, per effettuare cambi di scala (aggregare paesi in regioni, regioni in nazioni...) o per avere dati pi√π stabili, con meno variazioni.\n",
    "- **Campionamento (sampling):** quando ho un dataset molto grande posso fare un campionamento del dataset e applicare inizialmente le tecniche di machine learning su un dataset ridotto (analogamente per la valutazione del modello ottenuto). Tecniche di **stratified cross validation** (che mantengono le propriet√† statistiche del dataset originale) sono consigliate a questo scopo. Il campionamento pu√≤ essere casuale, senza reinserimento, con reinserimento, *stratified* (divido i dati in partizioni ed estraggo campioni casuali da ciascuna partizione)...\n",
    "[Quanto deve essere grande il sample?](https://en.wikipedia.org/wiki/Sample_size_determination) In genere pi√π √® grande, pi√π √® alta la precisione nella stima di parametri sconosciuti. Questo fenomeno √® descritto da varie leggi matematiche, tra cui la legge dei grandi numeri e il teorema del limite centrale.\n",
    "\n",
    "- **Dimensionality reduction:** in modo da poter usare tecniche che lavorino solo sugli attributi principali\n",
    "- **Feature subset selection:** cercare di identificare le feature (variabili) pi√π rilevanti e quali invece contengano rumore\n",
    "- **Feature creation:** ho una conoscenza sul dominio che potrei sfruttare per inserire nuovi attributi nel mio dataset (ad esempio da altezza e peso ricavare l'indice di massa corporea)\n",
    "- **Attribute transformation:** un peso di una persona in milligrammi potrebbe essere trasformato in kilogrammi. Possono essere applicati trasformazioni lineari o non-lineari per favorire l'apprendimento (esempio: trasformazione logaritmica per passare da un modello esponenziale ad uno lineare).\n",
    "- **Discretization and Binarization:** se so che gli attributi numerici sono afflitti da rumore, potrei decidere di discretizzare questi valori.\n",
    "\n",
    "## Curse of dimensionality\n",
    "Si tratta di un fenomeno che sorge lavorando con dati in spazi con un alto numero di dimensioni. Al crescere del numero di dimensioni, lo spazio cresce esponenzialmente e i dati diventano presto sparsi (tanto volume non occupato dalle istanze del data set). Supponiamo che ciascuna variabile (ciascuna dimensione) possa assumere 2 valori. Con $n$ variabili si avrebbero $2^n$ possibili combinazioni dei loro valori. Con pi√π variabili che osservazioni si incorre nel rischio di overfitting. Nelle tecniche di clustering, con un elevato numero di dimensioni accade che i punti (dati) tendono a risultare equidistanti tra loro, vanificando i tentativi di formare dei cluster sulla base della vicinanza tra questi punti. \n",
    "\n",
    "Un esempio banale ma esplicativo di clustering: 8 caramelle (8 record). Si hanno due variabili binarie (assumono valore vero o falso) che ne descrivono il colore:  \"rossastra\" e \"bluastra\". 4 caramelle con colore tendende al rosso e 4 con colore tendente al blu.\n",
    "\n",
    "|    | Rossastra | Bluastra |\n",
    "|----|-----------|----------|\n",
    "| c1 | 1         | 0        |\n",
    "| c2 | 1         | 0        |\n",
    "| c3 | 1         | 0        |\n",
    "| c4 | 1         | 0        |\n",
    "| c5 | 0         | 1        |\n",
    "| c6 | 0         | 1        |\n",
    "| c7 | 0         | 1        |\n",
    "| c8 | 0         | 1        |\n",
    "\n",
    "Non sar√† difficile per l'algoritmo di clustering individuare due cluster. Pensiamo per√≤ ora di avere 8 variabili binarie che ne descrivono il colore pi√π specificamente.\n",
    "\n",
    "|    | Rossa | Arancio | Gialla | Fucsia |  Blu   | Azzurro | Verde | Viola |\n",
    "|----|--------|---------|--------|--------|--------|---------|-------|-------|\n",
    "| c1 | 1      | 0       | 0      | 0      | 0      | 0       | 0     | 0     |\n",
    "| c2 | 0      | 1       | 0      | 0      | 0      | 0       | 0     | 0     |\n",
    "| c3 | 0      | 0       | 1      | 0      | 0      | 0       | 0     | 0     |\n",
    "| c4 | 0      | 0       | 0      | 1      | 0      | 0       | 0     | 0     |\n",
    "| c5 | 0      | 0       | 0      | 0      | 1      | 0       | 0     | 0     |\n",
    "| c6 | 0      | 0       | 0      | 0      | 0      | 1       | 0     | 0     |\n",
    "| c7 | 0      | 0       | 0      | 0      | 0      | 0       | 1     | 0     |\n",
    "| c8 | 0      | 0       | 0      | 0      | 0      | 0       | 0     | 1     |\n",
    "\n",
    "L'algoritmo di clustering non √® pi√π in grado di individuare somiglianze (correlazioni tra variabili) tra alcune di queste caramelle.\n",
    "\n",
    "√à possibile ovviare a ci√≤ utilizzando tecniche di ***dimensionality reduction*** come:\n",
    "- **Principal Component Analysis (PCA)**:\n",
    "- **Singular Value Decomposition:**\n",
    "- **Feature Subset Selection:** si cerca di eliminare features ridondanti (esempio: prezzo di un prodotto e relativa imposta) o features poco significative (esempio: colore occhi in un problema di clustering dei clienti di un supermercato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarit√† e Dissimilarit√† dei dati\n",
    "La **similarit√†** definisce quanto due campioni siano simili; √® spesso misurata nel range $[0,1]$. La **dissimilarit√†** indica quanto due campioni non si assomiglino. Pi√π questa √® bassa, pi√π i campioni si assomigliano (se sono identici, la dissimilarit√† sar√† 0). Il limite superiore pu√≤ variare, ad esempio pu√≤ essere utilizzata la distanza euclidea tra due punti come misura di dissimilarit√†.\n",
    "\n",
    "A seconda del tipo di attributi possiamo individuare:\n",
    "- **Nominali:** possiamo verificare se questi siano uguali o meno.  \n",
    "    - Dissimilarit√†: $d=\\begin{cases}0 & \\text{if } p=q \\\\ 1 & \\text{if } p\\neq q \\end{cases}$\n",
    "    - Similarit√†:    $s=\\begin{cases}1 & \\text{if } p=q \\\\ 0 & \\text{if } p\\neq q \\end{cases}$\n",
    "- **Ordinali:** avendo un ordinamento tra questi $n$ elementi, possiamo utilizzare i loro indici ($|p-q|$ rappresenta la distanza in termini di numero di elementi tra $p$ e $q$. Se questi coincidono, allora $|p-q|=0$, se questi sono agli estremi opposti, allora $|p-q|=n-1$\n",
    "    - Dissimilarit√†: $d = \\frac{|p-q|}{n-1}$\n",
    "    - Similarit√†:    $s = 1 - \\frac{|p-q|}{n-1}$\n",
    "- **Interval o Ratio:** \n",
    "    - Dissimilarit√†: $d = |p-q|$ (distanza euclidea, ma possono anche essercene altre)\n",
    "    - Similarit√†:    $s = -d$, $s = \\frac{1}{1+d}$ or $s = 1 - \\frac{d - min_d}{max_d - min_d}$\n",
    "    \n",
    "Come distanza per attributi di tipo interval, possiamo utilizzare la **distanza euclidea** $d = \\sqrt{\\sum_{i=1}^{n}{\\left(p_i - q_i \\right)^2}}$, dove $n$ rappresenta il numero di features (dimensioni). In questo caso la standardizzazione risulta necessaria in modo che ciascuna feature contribuisca in ugual modo nel calcolo della distanza. √à quindi possibile costruire una matrice di similarit√† o di dissimilarit√† andando a misurare le distanze tra tutti i dati, secondo i valori di loro attributi di tipo interval.\n",
    "\n",
    "Una generalizzazione di distanza Euclidea e della distanza di Manhattan √® la **distanza di Minkowski**\n",
    "\n",
    "$$d = \\left(\\sum_{i=1}^{n}{|p_i - q_i |^r}\\right)^{\\frac{1}{r}}$$\n",
    "\n",
    "dove $r$ √® un parametro:\n",
    "- $r=1$: norma $l_1$\n",
    "- $r=2$: distanza Euclidea\n",
    "- $r\\rightarrow\\infty$: norma $l_\\infty$\n",
    "\n",
    "Una distanza deve rispettare le propriet√† di definizione positiva, simmetria e disuguaglianza triangolare.\n",
    "\n",
    "Anche la similarit√† deve rispettare alcune propriet√†: \n",
    "- $s(p,q)=1 \\Leftrightarrow p=q$ (massima similarit√†)\n",
    "- $s(p,q) = s(q,p) \\; \\forall q,p$ (simmetria)\n",
    "di conseguenza la similarit√† non √® una distanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarit√† tra vettori binari\n",
    "Per quanto riguarda la **similarit√† tra vettori binari** (dove $p$ e $q$ hanno solo attributi binari) possiamo definire il **Simple Matching Coefficient** e il **coefficiente di Jaccard**:\n",
    "\n",
    "- $M_{00}$: numero di attributi dove $p=0$ e $q=0$\n",
    "- $M_{01}$: numero di attributi dove $p=0$ e $q=1$\n",
    "- $M_{10}$: numero di attributi dove $p=1$ e $q=0$\n",
    "- $M_{11}$: numero di attributi dove $p=1$ e $q=1$\n",
    "\n",
    "$$SMC = \\frac{\\text{numero di matches}}{\\text{numero di attributi}} = \\frac{M_{11} + M_{00}}{M_{01} +M_{10} +M_{11} +M_{00}}$$\n",
    "\n",
    "$$J = \\frac{\\text{numero di } M_{11} \\text{ matches}}{\\text{numero di attributi non entrambi zero}} = \\frac{M_{11}}{M_{01} +M_{10} +M_{11}}$$\n",
    "\n",
    "#### Esempio\n",
    "$p = 1000000000$\n",
    "\n",
    "$q = 0000001001$\n",
    "- $M_{00}$: 7\n",
    "- $M_{01}$: 2\n",
    "- $M_{10}$: 1\n",
    "- $M_{11}$: 0\n",
    "\n",
    "$SMC = \\frac{7+0}{7+2+1+0} = 0.7$\n",
    "\n",
    "$J = \\frac{0}{2+1+0} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarit√† del coseno\n",
    "Dati due vettori $d_1,d_2$ si ha:\n",
    "\n",
    "$$\\cos(d_1,d_2) = \\frac{(d_1 \\bullet d_2)}{||d_1|| \\cdot ||d_2||}$$\n",
    "\n",
    "dove $\\bullet$ indica il prodotto scalare e $||d||$ indica la lunghezza del vettore (norma).\n",
    "\n",
    "#### Esempio\n",
    "$d_1 = 3205000200$\n",
    "\n",
    "$d_2 = 1000000102$\n",
    "\n",
    "$\\cos(d_1,d_2) = \\frac{5}{6.481+2.245} = 0.3150$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misure di correlazione\n",
    "Per calcolare la correlazione, prima si standardizzano $p$ e $q$, poi si calcola il prodotto scalare:\n",
    "\n",
    "$$p'_k=\\frac{\\left(p_k-mean(p)\\right)}{std(p)} \\quad q'_k=\\frac{\\left(q_k-mean(q)\\right)}{std(q)}$$\n",
    "\n",
    "$$correlation(p,q) = p'\\bullet q'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipologie di errori\n",
    "Distinguiamo\n",
    "- Pitfalls: errori non scontati\n",
    "- Pratfalls: errori macroscopici, grossolani\n",
    "\n",
    "√à buona pratica essere sempre scettici sui dati a disposizione.\n",
    "\n",
    "#### Esempio\n",
    "Carico in weka il dataset `weather-nominal.arff` e uso gli algoritmi:\n",
    "- `OneR`: albero decisionale ad un livello\n",
    "- `J48`: implementazione degli alberi decisionali (C4.5)\n",
    "\n",
    "`OneR` ottiene un'accuratezza pari al 43% (utilizzando 10-fold cross validation) mentre `J48` ottiene un'accuratezza pari al 50%. \n",
    "\n",
    "Cambiamo ora il valore della variabile `outlook` assegnando `unknown` alle prime 4 istanze la cui label (variable `play`) corrispondente √® `no`. Questo pu√≤ essere fatto andando nella sezione `preprocess`. L'accuratezza di `OneR` sale al 93%, mentre quella di `J48` rimane al 50% (non usa la variabile `utlook`per effettuare gli split).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduzione a WEKA\n",
    "Utilizzeremo WEKA, un software all'avanguardia üôÉ e al passo con i tempi üôÉüôÉ la cui ultima versione rilasciata risale al 2013 üôÉüôÉüôÉ.\n",
    "\n",
    "Dopo averlo installato e aperto, utilizziamo il modulo `Explorer`e clicchiamo `Open > Cartella di installazione/data` dove troveremo alcuni dataset di esempio in formato `.arff` (sostanzialmente dei `.csv` con un'intestazione che descrive se gli attributi siano nominali, ordinali...). Scegliamo il dataset `iris` (descritto [QUI](https://archive.ics.uci.edu/ml/datasets/Iris)). \n",
    "\n",
    "Nella sezione `Visualize` possiamo vedere combinazioni di attributi e come ciascun attributo sia distribuito.\n",
    "\n",
    "- Classe (setosa, versicolor, virginica): attributo nominale\n",
    "- Petal Length: attributo interval\n",
    "- Petal witdh: attributo interval\n",
    "\n",
    "Nella sezione *Classify* possiamo segliere un algoritmo di classificazione. Scegliamo `rules/ZeroR`: classificazione in base all'attributo maggiormente frequente (in questo caso, sono uniformemente distribuiti, 50 record per tipo di fiore). Selezionando `Start` vengono visualizzati i risulati della classificazione, tra cui la **confusion matrix** e misure di accuratezza (Precision, Recall, ROC area...). \n",
    "\n",
    "Sempre nella sezione *Classify* √® possibile selezionare un algoritmo di classificazione diverso, ad esempio l'albero decisionale `trees/J48` che corrisponde all'implementazione dell'algoritmo **C4.5**. Nella sezione *Test Options* possiamo scegliere quale dataset utilizzare per effettuare la validazione del modello (scegliendo *Cross-validation* si opera una **stratified** cross validation).\n",
    "\n",
    "Nelle opzioni (*more options*) √® possibile indicare il `Random seed` che determina come verranno mescolati i records in modo da poter ripetere un esperimento con lo stesso riordinamento (non variando il seed).\n",
    "\n",
    "Cliccando sul campo in cui √® visualizzato il nome dell'algoritmo utilizzato √® possibile scegliere la sua configurazione (ad esempio scegliere il `minNumSplit`). Se dovessi aumentare eccessivamente la complessit√† dell'albero (ad esempio non faccio pruning, oppure impostando `minNumSplit=1`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
