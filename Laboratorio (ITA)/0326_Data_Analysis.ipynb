{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. **[Introduction](#Introduction)**  \n",
    "    1.1. Type of data  \n",
    "    1.2. Data structures  \n",
    "    1.3. Data quality  \n",
    "    1.4. Data similarity measures\n",
    "2. [**Data preprocessing**](#Data-preprocessing)  \n",
    "    2.1. Feature scaling  \n",
    "    2.2. Encoding categorical features  \n",
    "    2.3. Curse of dimensionality  \n",
    "    2.3. Dealing with outliers  \n",
    "    2.4. Ranking  \n",
    "    2.5. Non-linear transformations\n",
    "3. [**Feature engineering**](#Feature-engineering)  \n",
    "    3.1. Numeric features  \n",
    "    3.2. Categorical features  \n",
    "    3.3. Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "(26/03 - 02/04)\n",
    "\n",
    "Attributi possono essere qualitativi/quantitativi. Per attributi quantitativi va tenuta in considerazione l'unit√† di misura (si pensi ad una variabile altezza presente in due copie di un dataset: una in Europa, misurata in *cm* e l'altra in America ma convertita in *feet*. Questo fattore va tenuto in conto, ad esempio nel momento in cui decida di addestrare il mio algoritmo su un dataset Europeo, e successivamente testi il mio algoritmo con dati aventi unit√† di misura differente.\n",
    "\n",
    "## Propriet√† fondamentali\n",
    "Attributi possono essere distinti in:\n",
    "- **Nominali:** ID, colore, zip codes...\n",
    "- **Ordinali:** il loro valore √® una categoria ordinata e la distanza tra le categorie non √® nota, per esempio altezza (basso, medio, alto), ranking (da 1 a 10).\n",
    "- **Interval:** attributi su cui ha senso calcolare delle differenze (es: temperatura, data...)\n",
    "- **Ratio:** attributi su cui ha senso fare dei rapporti dei loro valori (es: temperatura in Kelvin perch√© non include lo zero, lunghezza, tempo...)\n",
    "\n",
    "Questa classificazione √® sostanzialmente basata sul tipo di operazioni che posso fare sugli attributi:\n",
    "- Uguaglianza: $=, \\neq$\n",
    "- Confronto ordinale: $<, \\le, >, \\ge$\n",
    "- Addizione: $+,-$\n",
    "- Moltiplicazione: $\\times, \\div$ \n",
    "\n",
    "Le **operazioni** effettuabili sui miei dati determineranno le tecniche di machine learning e data mining utilizzabili. Alcuni esempi di operazioni effettuabili:\n",
    "- Nominali: moda, entropia, test di correlazione $\\chi^2$...\n",
    "- Ordinali: mediana, calcolo di percentili, correlazione dei ranghi, test dei segni...\n",
    "- Interval: media, deviazione standard, coefficiente di Pearson, t test, F test...\n",
    "- Ratio: media geometrica, media armonica, variazione percentuale...\n",
    "\n",
    "Le **trasformazioni** effettuabili sui miei dati:\n",
    "- Nominali: permutazioni dei valori (se gli ID dei miei dipendenti venissero riassegnati farebbe qualche differenza?)\n",
    "- Ordinali: qualsiasi cambio dei valori che mantenga l'ordinamento (quindi $value_{new} = f(value_{old}$ tale che $f$ sia monotona)\n",
    "- Interval: qualsiasi operazione che mantenga la proporzione tra i vari intervalli (una funzione affine ad esempio)\n",
    "- Ratio: qualsiasi operazione che mantenga il rapporto dei vari valori (come una moltiplicazione per una costante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributi discreti e continui\n",
    "Discreti possono assumere un insieme finito di valori, Continui possono assumere un insieme infinito di valori (tenere comunque a mente che la rappresentazione e memorizzazione su un calcolatore comporter√† una discretizzazione di questi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipologie di strutture dati\n",
    "Il dati possono essere memorizzati secondo diverse tipologie di struture che determineranno gli algoritmi di apprendimento applicabili\n",
    "- Un insieme di **record**: \n",
    "    - dati matriciali: ciascun record √® costituito da un insieme fisso di attributi\n",
    "    - testuali: ad esempio gli attributi possono essere delle parole e i record possono corrispondere a documenti. In ciascuna cella avr√≤ il numero di occorrenze di quella parola per quel particolare documento.\n",
    "    - transizioni di stato\n",
    "- Un **grafo**: WWW, strutture molecolari...\n",
    "- Dati **ordinati**: dati spaziali, temporali, sequenze..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualit√† dei dati\n",
    "Le misure sono tipicamente affette da rumore, quindi questo fattore √® da tenere presente. Prima di fare una qualunque analisi del nostro sistema √® opportuno porsi delle domande: √® presente del rumore nel nostro dataset? In quale misura? \n",
    "\n",
    "Vi sono degli *outliers* nel nostro dataset? Eventualmente potrei utilizzare la mediana piuttosto che la media, che √® meno influenzata dalla presenza di outliers. In tecniche di apprendimento non supervisionato, come il clustering, la presenza di outliers pu√≤ influenzare notevolmente l'accuratezza del risultato prodotto dall'algoritmo. Potrei quindi decidere di filtrare eventuali outliers.\n",
    "\n",
    "Va tenuto conto anche di valori mancanti (√® necessario capire se la loro mancanza abbia un certo significato, in modo da poter associare un valore di default evaentualmente) o duplicati. Spesso gli algoritmi di machine learning escludono i record con valori mancanti, ma questo potrebbe causare notevoli problemi qualora il numero di dati mancanti sia alto. Il motivo dell'assenza di alcuni valori pu√≤ essere dovuto a:\n",
    "- informazioni non raccolte: ad esempio in un questionario una persona potrebbe non voler specificare il sesso.\n",
    "- informazioni non applicabili: un salario ad un bambino\n",
    "\n",
    "In questi casi potrei cercare di dedurre il valore inferendolo da altri record simili a quello con il valore mancante oppure a partire da altri attributi dello stesso record (dall'altezza e dall'indice di massa corporea potrei inferire il peso). Posso cercare di eliminare record con troppi valori mancanti, fare una stima, ignorare tale attributo, rimpiazzarli con valori quantitativi con una probabilit√† associata.\n",
    "\n",
    "Eventuali duplicati (o dati molto simili) vanno gestiti attraverso un processo di **data cleaning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarit√† e Dissimilarit√† dei dati\n",
    "La **similarit√†** definisce quanto due campioni siano simili; √® spesso misurata nel range $[0,1]$. La **dissimilarit√†** indica quanto due campioni non si assomiglino. Pi√π questa √® bassa, pi√π i campioni si assomigliano (se sono identici, la dissimilarit√† sar√† 0). Il limite superiore pu√≤ variare, ad esempio pu√≤ essere utilizzata la distanza euclidea tra due punti come misura di dissimilarit√†.\n",
    "\n",
    "A seconda del tipo di attributi possiamo individuare:\n",
    "- **Nominali:** possiamo verificare se questi siano uguali o meno.  \n",
    "    - Dissimilarit√†: $d=\\begin{cases}0 & \\text{if } p=q \\\\ 1 & \\text{if } p\\neq q \\end{cases}$\n",
    "    - Similarit√†:    $s=\\begin{cases}1 & \\text{if } p=q \\\\ 0 & \\text{if } p\\neq q \\end{cases}$\n",
    "- **Ordinali:** avendo un ordinamento tra questi $n$ elementi, possiamo utilizzare i loro indici ($|p-q|$ rappresenta la distanza in termini di numero di elementi tra $p$ e $q$. Se questi coincidono, allora $|p-q|=0$, se questi sono agli estremi opposti, allora $|p-q|=n-1$\n",
    "    - Dissimilarit√†: $d = \\frac{|p-q|}{n-1}$\n",
    "    - Similarit√†:    $s = 1 - \\frac{|p-q|}{n-1}$\n",
    "- **Interval o Ratio:** \n",
    "    - Dissimilarit√†: $d = |p-q|$ (distanza euclidea, ma possono anche essercene altre)\n",
    "    - Similarit√†:    $s = -d$, $s = \\frac{1}{1+d}$ or $s = 1 - \\frac{d - min_d}{max_d - min_d}$\n",
    "    \n",
    "Come distanza per attributi di tipo interval, possiamo utilizzare la **distanza euclidea** $d = \\sqrt{\\sum_{i=1}^{n}{\\left(p_i - q_i \\right)^2}}$, dove $n$ rappresenta il numero di features (dimensioni). In questo caso la standardizzazione risulta necessaria in modo che ciascuna feature contribuisca in ugual modo nel calcolo della distanza. √à quindi possibile costruire una matrice di similarit√† o di dissimilarit√† andando a misurare le distanze tra tutti i dati, secondo i valori di loro attributi di tipo interval.\n",
    "\n",
    "Una generalizzazione di distanza Euclidea e della distanza di Manhattan √® la **distanza di Minkowski**\n",
    "\n",
    "$$d = \\left(\\sum_{i=1}^{n}{|p_i - q_i |^r}\\right)^{\\frac{1}{r}}$$\n",
    "\n",
    "dove $r$ √® un parametro:\n",
    "- $r=1$: norma $l_1$\n",
    "- $r=2$: distanza Euclidea\n",
    "- $r\\rightarrow\\infty$: norma $l_\\infty$\n",
    "\n",
    "Una distanza deve rispettare le propriet√† di definizione positiva, simmetria e disuguaglianza triangolare.\n",
    "\n",
    "Anche la similarit√† deve rispettare alcune propriet√†: \n",
    "- $s(p,q)=1 \\Leftrightarrow p=q$ (massima similarit√†)\n",
    "- $s(p,q) = s(q,p) \\; \\forall q,p$ (simmetria)\n",
    "di conseguenza la similarit√† non √® una distanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarit√† tra vettori binari\n",
    "Per quanto riguarda la **similarit√† tra vettori binari** (dove $p$ e $q$ hanno solo attributi binari) possiamo definire il **Simple Matching Coefficient** e il **coefficiente di Jaccard**:\n",
    "\n",
    "- $M_{00}$: numero di attributi dove $p=0$ e $q=0$\n",
    "- $M_{01}$: numero di attributi dove $p=0$ e $q=1$\n",
    "- $M_{10}$: numero di attributi dove $p=1$ e $q=0$\n",
    "- $M_{11}$: numero di attributi dove $p=1$ e $q=1$\n",
    "\n",
    "$$SMC = \\frac{\\text{numero di matches}}{\\text{numero di attributi}} = \\frac{M_{11} + M_{00}}{M_{01} +M_{10} +M_{11} +M_{00}}$$\n",
    "\n",
    "$$J = \\frac{\\text{numero di } M_{11} \\text{ matches}}{\\text{numero di attributi non entrambi zero}} = \\frac{M_{11}}{M_{01} +M_{10} +M_{11}}$$\n",
    "\n",
    "#### Esempio\n",
    "$p = 1000000000$\n",
    "\n",
    "$q = 0000001001$\n",
    "- $M_{00}$: 7\n",
    "- $M_{01}$: 2\n",
    "- $M_{10}$: 1\n",
    "- $M_{11}$: 0\n",
    "\n",
    "$SMC = \\frac{7+0}{7+2+1+0} = 0.7$\n",
    "\n",
    "$J = \\frac{0}{2+1+0} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarit√† del coseno\n",
    "Dati due vettori $d_1,d_2$ si ha:\n",
    "\n",
    "$$\\cos(d_1,d_2) = \\frac{(d_1 \\bullet d_2)}{||d_1|| \\cdot ||d_2||}$$\n",
    "\n",
    "dove $\\bullet$ indica il prodotto scalare e $||d||$ indica la lunghezza del vettore (norma).\n",
    "\n",
    "#### Esempio\n",
    "$d_1 = 3205000200$\n",
    "\n",
    "$d_2 = 1000000102$\n",
    "\n",
    "$\\cos(d_1,d_2) = \\frac{5}{6.481+2.245} = 0.3150$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misure di correlazione\n",
    "Per calcolare la correlazione, prima si standardizzano $p$ e $q$, poi si calcola il prodotto scalare:\n",
    "\n",
    "$$p'_k=\\frac{\\left(p_k-\\mu_p\\right)}{\\sigma_p} \\quad q'_k=\\frac{\\left(q_k-\\mu_q\\right)}{\\sigma_q}$$\n",
    "\n",
    "$$correlation(p,q) = p'\\bullet q'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipologie di errori\n",
    "Distinguiamo\n",
    "- Pitfalls: errori non scontati\n",
    "- Pratfalls: errori macroscopici, grossolani\n",
    "\n",
    "√à buona pratica essere sempre scettici sui dati a disposizione.\n",
    "\n",
    "#### Esempio\n",
    "Carico in weka il dataset `weather-nominal.arff` e uso gli algoritmi:\n",
    "- `OneR`: albero decisionale ad un livello\n",
    "- `J48`: implementazione degli alberi decisionali (C4.5)\n",
    "\n",
    "`OneR` ottiene un'accuratezza pari al 43% (utilizzando 10-fold cross validation) mentre `J48` ottiene un'accuratezza pari al 50%. \n",
    "\n",
    "Cambiamo ora il valore della variabile `outlook` assegnando `unknown` alle prime 4 istanze la cui label (variable `play`) corrispondente √® `no`. Questo pu√≤ essere fatto andando nella sezione `preprocess`. L'accuratezza di `OneR` sale al 93%, mentre quella di `J48` rimane al 50% (non usa la variabile `utlook`per effettuare gli split).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduzione a WEKA\n",
    "Utilizzeremo WEKA, un software all'avanguardia üôÉ e al passo con i tempi üôÉüôÉ la cui ultima versione rilasciata risale al 2013 üôÉüôÉüôÉ.\n",
    "\n",
    "Dopo averlo installato e aperto, utilizziamo il modulo `Explorer`e clicchiamo `Open > Cartella di installazione/data` dove troveremo alcuni dataset di esempio in formato `.arff` (sostanzialmente dei `.csv` con un'intestazione che descrive se gli attributi siano nominali, ordinali...). Scegliamo il dataset `iris` (descritto [QUI](https://archive.ics.uci.edu/ml/datasets/Iris)). \n",
    "\n",
    "Nella sezione `Visualize` possiamo vedere combinazioni di attributi e come ciascun attributo sia distribuito.\n",
    "\n",
    "- Classe (setosa, versicolor, virginica): attributo nominale\n",
    "- Petal Length: attributo interval\n",
    "- Petal witdh: attributo interval\n",
    "\n",
    "Nella sezione *Classify* possiamo segliere un algoritmo di classificazione. Scegliamo `rules/ZeroR`: classificazione in base all'attributo maggiormente frequente (in questo caso, sono uniformemente distribuiti, 50 record per tipo di fiore). Selezionando `Start` vengono visualizzati i risulati della classificazione, tra cui la **confusion matrix** e misure di accuratezza (Precision, Recall, ROC area...). \n",
    "\n",
    "Sempre nella sezione *Classify* √® possibile selezionare un algoritmo di classificazione diverso, ad esempio l'albero decisionale `trees/J48` che corrisponde all'implementazione dell'algoritmo **C4.5**. Nella sezione *Test Options* possiamo scegliere quale dataset utilizzare per effettuare la validazione del modello (scegliendo *Cross-validation* si opera una **stratified** cross validation).\n",
    "\n",
    "Nelle opzioni (*more options*) √® possibile indicare il `Random seed` che determina come verranno mescolati i records in modo da poter ripetere un esperimento con lo stesso riordinamento (non variando il seed).\n",
    "\n",
    "Cliccando sul campo in cui √® visualizzato il nome dell'algoritmo utilizzato √® possibile scegliere la sua configurazione (ad esempio scegliere il `minNumSplit`). Se dovessi aumentare eccessivamente la complessit√† dell'albero (ad esempio non faccio pruning, oppure impostando `minNumSplit=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Prima di applicare algoritmi di machine learning potrei effettuare operazioni di preprocessing dei dati al fine di facilitare la successiva gestione di tali dati.\n",
    "- **Aggregazione:** potrei cercare di aggregare tra loro pi√π attributi o pi√π record per ridurre il numero di dati, per effettuare cambi di scala (aggregare paesi in regioni, regioni in nazioni...) o per avere dati pi√π stabili, con meno variazioni.\n",
    "- **Campionamento (sampling):** quando ho un dataset molto grande posso fare un campionamento del dataset e applicare inizialmente le tecniche di machine learning su un dataset ridotto (analogamente per la valutazione del modello ottenuto). Tecniche di **stratified cross validation** (che mantengono le propriet√† statistiche del dataset originale) sono consigliate a questo scopo. Il campionamento pu√≤ essere casuale, senza reinserimento, con reinserimento, *stratified* (divido i dati in partizioni ed estraggo campioni casuali da ciascuna partizione)...\n",
    "[Quanto deve essere grande il sample?](https://en.wikipedia.org/wiki/Sample_size_determination) In genere pi√π √® grande, pi√π √® alta la precisione nella stima di parametri sconosciuti. Questo fenomeno √® descritto da varie leggi matematiche, tra cui la legge dei grandi numeri e il teorema del limite centrale.\n",
    "\n",
    "- **Dimensionality reduction:** in modo da poter usare tecniche che lavorino solo sugli attributi principali\n",
    "- **Feature subset selection:** cercare di identificare le feature (variabili) pi√π rilevanti e quali invece contengano rumore\n",
    "- **Feature creation:** ho una conoscenza sul dominio che potrei sfruttare per inserire nuovi attributi nel mio dataset (ad esempio da altezza e peso ricavare l'indice di massa corporea)\n",
    "- **Attribute transformation:** un peso di una persona in milligrammi potrebbe essere trasformato in kilogrammi. Possono essere applicati trasformazioni lineari o non-lineari per favorire l'apprendimento (esempio: trasformazione logaritmica per passare da un modello esponenziale ad uno lineare).\n",
    "- **Discretization and Binarization:** se so che gli attributi numerici sono afflitti da rumore, potrei decidere di discretizzare questi valori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "There are algorithms that are sensitive to feature scaling, meaning that varying the scale of a feature may change the behavior of the algorithm itself:\n",
    "- k-nearest neighbors: if we want all features to contribute equally, we need to scale them to a common range.\n",
    "- k-means\n",
    "- logistic regression, SVMs, Neural networks, etc.: gradient descent can be much quicker if features are properly scaled.\n",
    "- linear discriminant analysis, principal component analysis: since we want to find orthogonal directions of maximizing the variance, we want features on the same scale, in order not to emphasize certain variables more.\n",
    "\n",
    "On the contrary, tree-based algorithms such as the CART decision tree are scale-invariant, in the sense that they aim at finding the optimal threshold to split a certain feature, regardless of the scale.\n",
    "\n",
    "There exists many methods to perform feature scaling, such as:\n",
    "\n",
    "- Rescaling (min-max normalization)\n",
    "- Mean normalization\n",
    "- Standardization\n",
    "- Scaling to unit length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization\n",
    "\n",
    "It consists in rescaling the range of possible values to $[0,1]$ or $[-1,1]$.\n",
    "\n",
    "\\begin{equation}\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean normalization\n",
    "\n",
    "\\begin{equation}\n",
    "x' = \\frac{x - \\mu_x}{\\max(x) - \\min(x)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_x$ being the sample mean of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Mean normalization makes the normalized feature to have zero mean and unit variance.\n",
    "\n",
    "\\begin{equation}\n",
    "x' = \\frac{x - \\mu_x}{\\sigma_x}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_x$ is the sample mean and $\\sigma_x$ is the sample standard deviation of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to unit length\n",
    "\n",
    "It consists in scaling a feature vector such that the resulting one has length one.\n",
    "\n",
    "\\begin{equation}\n",
    "x' = \\frac{x}{\\Vert x \\Vert}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "In the following example, we will compare how different scalers (respectively the Standard scaler and the Min-max scaler) affect the performance of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid\n",
       "count  178.000000  178.000000\n",
       "mean    13.000618    2.336348\n",
       "std      0.811827    1.117146\n",
       "min     11.030000    0.740000\n",
       "25%     12.362500    1.602500\n",
       "50%     13.050000    1.865000\n",
       "75%     13.677500    3.082500\n",
       "max     14.830000    5.800000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_wine(as_frame=True)\n",
    "X = data.data[['alcohol','malic_acid']]\n",
    "y = data.target\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min-Max</th>\n",
       "      <td>0.745356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.680414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                RMSE\n",
       "Original    0.796296\n",
       "Min-Max     0.745356\n",
       "Normalized  0.680414"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1234)\n",
    "\n",
    "# fit logistic regression without scaling\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# compute root MSE on test set\n",
    "rmse = [model.score(X_test, y_test)]\n",
    "\n",
    "# build pipelines using standard scaler and min-max scaler\n",
    "pipelines = [make_pipeline(MinMaxScaler(), LogisticRegression()),\n",
    "             make_pipeline(StandardScaler(), LogisticRegression())]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test,pred)))\n",
    "    \n",
    "df_lr = pd.DataFrame({'RMSE':rmse}, index=['Original','Min-Max','Normalized'])\n",
    "df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by normalizing the two features, we obtained a smaller root mean squared error on the test set, compared to the score we obtained on the unscaled test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "\n",
    "Categorical features can be efficiently coded as integers, by replacing the categories by an integer. For instance, consider these 3 categorical features that a person could have:\n",
    "```python\n",
    "Sex = ['male', 'female']\n",
    "Occupation = ['student', 'worker', 'unemployed']\n",
    "From = ['Europe', 'Asia', 'North-America', 'South-America', 'Africa', 'Australia', 'Antarctica']\n",
    "```\n",
    "\n",
    "We can convert each of these categories into an integer value by using [`sklearn.preprocessing.OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 2.],\n",
       "       [1., 0., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder(categories=[['male', 'female'], \n",
    "                                 ['student', 'worker', 'unemployed'], \n",
    "                                 ['Europe', 'Asia', 'North-America', 'South-America', 'Africa', 'Australia', 'Antarctica']])\n",
    "X = [['male', 'unemployed', 'North-America'], ['female', 'student', 'Africa']]\n",
    "enc.fit(X)\n",
    "enc.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, such representation can't be used directly with estimators because ordinal encoding creates a fictional ordinal relationship in the data. For this reason, in order to improve the performance of the learning algorithm we will use, **one-hot encoding** is often applied to categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency encoding\n",
    "\n",
    "Frequency encoding consists into associating to each value a categorical feature may take, the corresponding frequency in the dataset. For instance, assume that the titanic dataset has a categorical feature called `Embarked`:\n",
    "\n",
    "```python\n",
    "encoding = titanic.groupby('Embarked').size()\n",
    "encoding = encoding / len(titanic)\n",
    "titanic['Embarked_enc'] = titanic.Embarked.map(encoding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding consists in creating, for each unique value in the original categorical column, a new column representing a binary feature that can take only two values: 1, if in the original column, that entry takes value equal to the one that this column represents, 0 otherwise.\n",
    "\n",
    "For instance, consider an instance having `Occupation = worker`. Therefore, by one-hot encoding the categorical columns, the `Occupation` feature will give birth to 3 new columns, one for each value that `Occupation` may take.\n",
    "\n",
    "| student | worker | unemployed |\n",
    "|---------|--------|------------|\n",
    "| 0       | 1      | 0          |\n",
    "\n",
    "One-hot encoding can be performed using [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) or [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories=[['male', 'female'], \n",
    "                                ['student', 'worker', 'unemployed'], \n",
    "                                ['Europe', 'Asia', 'North-America', 'South-America', 'Africa', 'Australia', 'Antarctica']])\n",
    "X = [['male', 'unemployed', 'North-America'], ['female', 'student', 'Africa']]\n",
    "enc.fit(X)\n",
    "enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not every model requires to one-hot encode categorical features. For instance, random forests, can work directly with categorical, without requiring those features to be transformed at all. Note also that, categorical features with many values would end up in many new binary features, resulting into a very sparse matrix. In that case, we can store just the non-zero elements using **sparse matrices**, reducing the memory occupation by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality\n",
    "Si tratta di un fenomeno che sorge lavorando con dati in spazi con un alto numero di dimensioni. Al crescere del numero di dimensioni, lo spazio cresce esponenzialmente e i dati diventano presto sparsi (tanto volume non occupato dalle istanze del data set). Supponiamo che ciascuna variabile (ciascuna dimensione) possa assumere 2 valori. Con $n$ variabili si avrebbero $2^n$ possibili combinazioni dei loro valori. Con pi√π variabili che osservazioni si incorre nel rischio di overfitting. Nelle tecniche di clustering, con un elevato numero di dimensioni accade che i punti (dati) tendono a risultare equidistanti tra loro, vanificando i tentativi di formare dei cluster sulla base della vicinanza tra questi punti. \n",
    "\n",
    "Un esempio banale ma esplicativo di clustering: 8 caramelle (8 record). Si hanno due variabili binarie (assumono valore vero o falso) che ne descrivono il colore:  \"rossastra\" e \"bluastra\". 4 caramelle con colore tendende al rosso e 4 con colore tendente al blu.\n",
    "\n",
    "|    | Rossastra | Bluastra |\n",
    "|----|-----------|----------|\n",
    "| c1 | 1         | 0        |\n",
    "| c2 | 1         | 0        |\n",
    "| c3 | 1         | 0        |\n",
    "| c4 | 1         | 0        |\n",
    "| c5 | 0         | 1        |\n",
    "| c6 | 0         | 1        |\n",
    "| c7 | 0         | 1        |\n",
    "| c8 | 0         | 1        |\n",
    "\n",
    "Non sar√† difficile per l'algoritmo di clustering individuare due cluster. Pensiamo per√≤ ora di avere 8 variabili binarie che ne descrivono il colore pi√π specificamente.\n",
    "\n",
    "|    | Rossa | Arancio | Gialla | Fucsia |  Blu   | Azzurro | Verde | Viola |\n",
    "|----|--------|---------|--------|--------|--------|---------|-------|-------|\n",
    "| c1 | 1      | 0       | 0      | 0      | 0      | 0       | 0     | 0     |\n",
    "| c2 | 0      | 1       | 0      | 0      | 0      | 0       | 0     | 0     |\n",
    "| c3 | 0      | 0       | 1      | 0      | 0      | 0       | 0     | 0     |\n",
    "| c4 | 0      | 0       | 0      | 1      | 0      | 0       | 0     | 0     |\n",
    "| c5 | 0      | 0       | 0      | 0      | 1      | 0       | 0     | 0     |\n",
    "| c6 | 0      | 0       | 0      | 0      | 0      | 1       | 0     | 0     |\n",
    "| c7 | 0      | 0       | 0      | 0      | 0      | 0       | 1     | 0     |\n",
    "| c8 | 0      | 0       | 0      | 0      | 0      | 0       | 0     | 1     |\n",
    "\n",
    "L'algoritmo di clustering non √® pi√π in grado di individuare somiglianze (correlazioni tra variabili) tra alcune di queste caramelle.\n",
    "\n",
    "√à possibile ovviare a ci√≤ utilizzando tecniche di ***dimensionality reduction*** come:\n",
    "- **Principal Component Analysis (PCA)**:\n",
    "- **Singular Value Decomposition:**\n",
    "- **Feature Subset Selection:** si cerca di eliminare features ridondanti (esempio: prezzo di un prodotto e relativa imposta) o features poco significative (esempio: colore occhi in un problema di clustering dei clienti di un supermercato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with outliers\n",
    "\n",
    "To protect a linear model from being affected by outliers we can *clip* features' values between within a specific range, choosing the lower bound and the upper bound as some percentiles of that features (e.g. first, and 99th percentile). This procedure is called ***winsorization*** and can be done in Python using [`scipy.stats.mstats.winsorize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.winsorize.html). An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate data within a range\n",
    "x = np.random.uniform(low=0, high=100, size=(200,))\n",
    "\n",
    "# add outlier\n",
    "x = np.append(x, -60)\n",
    "\n",
    "# plot histogram\n",
    "pd.Series(x).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute 99% interval, clip and plot histogram\n",
    "upper_bound, lower_bound = np.percentile(x, [1,99])\n",
    "x = np.clip(x, upper_bound, lower_bound)\n",
    "\n",
    "pd.Series(x).hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking\n",
    "\n",
    "Another useful preprocessing operation is *ranking*. [Ranking](https://en.wikipedia.org/wiki/Ranking) consists into performing a weak ordering of the values a feature takes. By doing this we move outliers closer to other values, defining a mapping between values and indexes. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4. , 2.5, 1. , 5. , 2.5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "rankdata([10, 3, 1e-4, 50, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in order to keep the same rank mapping also with the test set, we need to store the mapping computed with the training set. Alternatively, we can first concatenate the training with the test set, then compute the ranking of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear transformations\n",
    "\n",
    "Non-linear transformations of the values can help non-tree-based algorithms, such as neural nets, because they bring the values closer to their mean and make the values near zero a little more distinguishable. Examples are:\n",
    "- Log transform\n",
    "\n",
    "$$x' = \\log(1+x)$$\n",
    "\n",
    "- Rising to a power $< 1$\n",
    "\n",
    "$$x' = \\sqrt{x + 2/3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†Numeric features\n",
    "\n",
    "Numeric features can be extended by generating new ones, either with knowledge about the domain of the problem, or through *exploratory data analysis* (EDA). For example, if we had `height` and `width` features, we could generate a new `distance` variable by using Pitagora's theorem. Similarly, `squared_area` and `total_price` can be combined into an interaction features called `price_for_m2`, computing the price for square meter.\n",
    "\n",
    "Another thing we may try, is to extract the fractional part of a decimal number. Variables such as `price` can benefit from this because factors such as people perception of prices could then be taken into account by the model.\n",
    "\n",
    "| price | price_decimal_part |\n",
    "|:-----:|:------------------:|\n",
    "|  0.99 |         .99        |\n",
    "|  6.49 |         .49        |\n",
    "|  1.00 |         .00        |\n",
    "|  9.98 |         .98        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and time features\n",
    "\n",
    "Date and time are quite interesting features, because not only they are linear in nature, but they also have several different tiers, such as *year*, *day*, *week*... Features generated by datetime can be divided into two categories:\n",
    "1. **Time moments in a period**: day number in the week, month, season, year, seconds, minutes, hours. This is useful to capture **repetitive patterns** in data. Another attribute of this type can be a boolean value indicating whether that day is a holiday.\n",
    "2. **Time passed since a particular event**: this can be either \n",
    "    - **row-independent**: such as time past from a general moment for all data (e.g. from year 2000).\n",
    "    - **row-dependent**: for instance, the number of days left until Christmas, or the days past since the last promotional campaign.\n",
    "    \n",
    "Generating features of this kind may return useful in tasks such as **churn prediction**, which consists in estimating the likelihood that a customer will churn (i.e. will stop paying for a service). Combining a feature like `last_purchase_date` with other relevant dates, into a new variable `date_diff`, can improve by a lot the model's performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates features\n",
    "\n",
    "When we deal with features involving coordinates (such as the position of a house), we can augment our dataset in many ways:\n",
    "- Computing the distance from a certain relevant place, like an hospital, a school or a popular meeting place.\n",
    "- Splitting the map into squares, and for every flat in each square, computing the distance from the most expensive flat in that area.\n",
    "- Organizing data points into clusters, then use the centroid of each cluster as an important point from which to compute the distance.\n",
    "- Computing aggregating statistics for objects' surrounding area (e.g. number of flats within a certain range, which can be interpreted as area's popularity).\n",
    "- Transform coordinates by rotating them around a point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features\n",
    "\n",
    "When dealing with categorical features, we can derive new ones by considering the interaction between the variables, so that the model can find optimal coefficient for interaction features and improve. This can be useful with non-tree-based models.\n",
    "This can be done by concatenating the values of two or more features, and then one-hot encoding them.\n",
    "\n",
    "Assume to have this data, where the interaction feature `sex_fav_color` between `sex` and `fav_color` has already been built.\n",
    "\n",
    "| sex    | fav_color | sex_fav_color    |\n",
    "|--------|-----------|------------------|\n",
    "| male   | green     | male_green       |\n",
    "| female | black     | female_black     |\n",
    "| female | turquoise | female_turquoise |\n",
    "| female | black     | female_black     |\n",
    "\n",
    "By one-hot encoding the `sex_fav_color` variable, we obtain:\n",
    "\n",
    "| female_black | female_turquoise | male_green |\n",
    "|--------------|------------------|------------|\n",
    "| 0            | 0                | 1          |\n",
    "| 1            | 0                | 0          |\n",
    "| 0            | 1                | 0          |\n",
    "| 1            | 0                | 0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "We can consider as missing values not only `NaN` values, but also empty strings, special values like `-1`, outliers like `-999` and many more. The presence of missing values can provide insights on why they occurred. The first step is to identify them, and that can be done by inspecting the histograms of the variables.\n",
    "\n",
    "Once we identified them, we must decide what to do:\n",
    "- Replace them with a value outside of the feature's values range (it can help tree-based models, but may worsen the performance of linar models or neural networks).\n",
    "- Replace them with the mean/median of the values of the feature involved.\n",
    "- Add new boolean feature `is_missing` according the the presence/absence of the value.\n",
    "- Reconstruct their value (e.g. with time series, sometimes it can make sense to interpolate nearby points in order to estimate a reasonable value for the missing one.\n",
    "\n",
    "Be careful when generating new features using missing values: for example, if we chose to replace a missing value of a numeric feature with `-999`, then we can't encode a different categorical feature using the average values from the numeric feature, because we would end up in a situation like this, doing more harm than good:\n",
    "\n",
    "| categorical_feature | numeric_feature | numeric_filled | categorical_encoded |\n",
    "|:-------------------:|:---------------:|:--------------:|:-------------------:|\n",
    "|          A          |        1        |        1       |         1.5         |\n",
    "|          A          |        4        |        4       |         1.5         |\n",
    "|          A          |        2        |        2       |         1.5         |\n",
    "|          A          |        -1       |       -1       |         1.5         |\n",
    "|          B          |        9        |        9       |         -495        |\n",
    "|          B          |       NaN       |      -999      |         -495        |\n",
    "\n",
    "In conclusion, there are algorithms, like XGBoost that are able to handle NaNs by themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from text and images\n",
    "\n",
    "Sometimes some of our dataset's attributes may be text attributes or images. In that case, we need to extract meaningful features from those attributes.\n",
    "\n",
    "### Text to vectors\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "Suppose we have a text document. Bag of words consists of creating a new feature for each work encountered in the document, and then counting the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anna</th>\n",
       "      <th>are</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>hungry</th>\n",
       "      <th>or</th>\n",
       "      <th>said</th>\n",
       "      <th>thirsty</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anna  are  hello  how  hungry  or  said  thirsty  you\n",
       "0     1    1      1    1       0   0     0        0    1\n",
       "1     0    2      0    0       1   1     0        1    2\n",
       "2     0    0      3    0       0   0     1        0    1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Hello Anna, how are you?\", \n",
    "    \"Are you hungry or are you thirsty?\", \n",
    "    \"Hello?! HELLO!?! I said you hello!\",\n",
    "    ]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also post-process the calculated matrix, this is because of the dependence on scaling of linear models. We want to make samples comparable, and, at the same time, to boost more important features, decreasing the scale of useless ones. For this purpose we can compute the ***terms frequencies*** in every entry, in order to replace occurrences with frequencies. Doing that will make the values along each row to sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anna</th>\n",
       "      <th>are</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>hungry</th>\n",
       "      <th>or</th>\n",
       "      <th>said</th>\n",
       "      <th>thirsty</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anna       are  hello  how    hungry        or  said   thirsty       you\n",
       "0   0.2  0.200000    0.2  0.2  0.000000  0.000000   0.0  0.000000  0.200000\n",
       "1   0.0  0.285714    0.0  0.0  0.142857  0.142857   0.0  0.142857  0.285714\n",
       "2   0.0  0.000000    0.6  0.0  0.000000  0.000000   0.2  0.000000  0.200000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = (1 / df.sum(axis=1)).to_numpy() [:,None]\n",
    "df_tf = df * tf\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach consists of applying the ***[Tf-idf](https://en.wikipedia.org/wiki/Tf‚Äìidf) (term frequency‚Äìinverse document frequency)***, which consists in normalizing data column-wise by the logarithmically scaled inverse fraction of the documents that contain the word corresponding to a certain feature. We can use [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) or compute them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anna</th>\n",
       "      <th>are</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>hungry</th>\n",
       "      <th>or</th>\n",
       "      <th>said</th>\n",
       "      <th>thirsty</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.216395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       anna       are     hello       how    hungry        or      said  \\\n",
       "0  1.098612  0.405465  0.405465  1.098612  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.810930  0.000000  0.000000  1.098612  1.098612  0.000000   \n",
       "2  0.000000  0.000000  1.216395  0.000000  0.000000  0.000000  1.098612   \n",
       "\n",
       "    thirsty  you  \n",
       "0  0.000000  0.0  \n",
       "1  1.098612  0.0  \n",
       "2  0.000000  0.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(df.shape[0] / (df > 0).sum(axis=0))\n",
    "df_idf = df * idf\n",
    "df_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what happened in the last column (corresponding to \"you\"): since all three documents contained the word \"you\", tf-idf scaled down that feature to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "N-grams works in a similar way to bag of words, except for the fact that now, we don't only add new features corresponding to single words, but also features corresponding to sequence of *n* words. The same concept can be applied to sequences of characters. In scikit learn we can tune the attributes `ngram_range` and `analyzer` of [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anna</th>\n",
       "      <th>anna how</th>\n",
       "      <th>are</th>\n",
       "      <th>are you</th>\n",
       "      <th>hello</th>\n",
       "      <th>hello anna</th>\n",
       "      <th>hello hello</th>\n",
       "      <th>hello said</th>\n",
       "      <th>how</th>\n",
       "      <th>how are</th>\n",
       "      <th>...</th>\n",
       "      <th>hungry or</th>\n",
       "      <th>or</th>\n",
       "      <th>or are</th>\n",
       "      <th>said</th>\n",
       "      <th>said you</th>\n",
       "      <th>thirsty</th>\n",
       "      <th>you</th>\n",
       "      <th>you hello</th>\n",
       "      <th>you hungry</th>\n",
       "      <th>you thirsty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   anna  anna how  are  are you  hello  hello anna  hello hello  hello said  \\\n",
       "0     1         1    1        1      1           1            0           0   \n",
       "1     0         0    2        2      0           0            0           0   \n",
       "2     0         0    0        0      3           0            1           1   \n",
       "\n",
       "   how  how are  ...  hungry or  or  or are  said  said you  thirsty  you  \\\n",
       "0    1        1  ...          0   0       0     0         0        0    1   \n",
       "1    0        0  ...          1   1       1     0         0        1    2   \n",
       "2    0        0  ...          0   0       0     1         1        0    1   \n",
       "\n",
       "   you hello  you hungry  you thirsty  \n",
       "0          0           0            0  \n",
       "1          0           1            1  \n",
       "2          1           0            0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Hello Anna, how are you?\", \n",
    "    \"Are you hungry or are you thirsty?\", \n",
    "    \"Hello?! HELLO!?! I said you hello!\",\n",
    "    ]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='word')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "In order to help bag of words, we should perform pre-processing of our text via:\n",
    "- lowercase: converting the text to lowercase, in order not to treat words like \"hello\", \"Hello\" and \"HEllO\" differently.\n",
    "- stemming\n",
    "- lemmatization\n",
    "- stopwords\n",
    "\n",
    "[Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)n have a similar goal which is to group similar words together words.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Stemming is a heuristic process used to chop off the suffixes of different word, but sharing the same root, for example *\"organize\"*, *\"organizes\"*, *\"organizing\"*, *\"organization\"*, all sharing a common root. Performing stemming would convert group those words under *\"organiz\"*.\n",
    "\n",
    "#### Lemmatization\n",
    "On the other hand, lemmatization do a similar job but more carefully, taking into account also morphological analysis of words and aiming at finding the meaning shared by all the words. For example, if we had *\"democracy\"*, *\"democratic\"* and *\"democratization\"* it would identify the word *\"democracy\"* as the shared concept between the three.\n",
    "\n",
    "#### Stopwords\n",
    "\n",
    "Stopwords are words that do not contain important information. They are either insignificant like articles and prepositions, or they are very common words that do not happen to help us with our task. There exists many libraries that allow to identify stopwords, such as NLTK. Scikit-learn has some basic functionality for that, tunable via the parameter `max_df` of `sklearn.feature_extraction.text.CountVectorizer`, that is the maximum frequency, above which the word can be removed.\n",
    "\n",
    "\n",
    "In conclusion, we can build a pipeline to process text-data which operates in this way:\n",
    "1. Pre-processing: lowercase, stemming, lemmatization, stopwords.\n",
    "2. Build Bag of words or N-grams\n",
    "3. Post-process: TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
