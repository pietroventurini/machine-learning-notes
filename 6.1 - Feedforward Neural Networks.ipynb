{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "## Contents\n",
    "1. Introduction  \n",
    "    1.1. The sigmoid unit  \n",
    "    1.2. The Rectified Linear Unit (ReLU)  \n",
    "2. Multi-layer neural networks  \n",
    "    2.1. Learning a XOR function  \n",
    "    2.2. Visual interpretation  \n",
    "    2.3. Generalization and vectorized notation  \n",
    "3. Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A **feedforward neural network**, also called **multilayer perceptron** (MLP), is a type of **artificial neural network** (ANN) wherein connections between nodes do not form a cycle (differently from its descendant: the *recurrent neural networks*). Its goal is to approximate some function $f^*$ by defining a mapping $y=f(x;w)$ and learning the value of the parameters $w$ that result in the best approximation. It is called feedforward because information flows from the input layer $x$, through the intermediate layers, to the output $y$, without any *feedback* connections where the outputs are fed back into the network. Before talking about networks, we have to introduce the units that will make up the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid unit\n",
    "Let's first introduce a \"building block\" for our neural network: the **sigmoid unit**. It is simply an artificial neuron in which the activation function is a sigmoid function (which has already been introduced in the chapter on Logistic Regression).\n",
    "\n",
    "<img src=\"images/neural_networks/sigmoid_unit.jpg\" style=\"width:60%\"/>\n",
    "\n",
    "Differently from the perceptron, which doesn't distinguish between points that lie close to the boundary and points that lie far, the sigmoid neuron outputs a value between 0 and 1. So, if we think of a classification model composed by only one sigmoid unit, we can consider the output value as the probability of the input point to be classified as positive.\n",
    "\n",
    "The sigmoid function is monotonic and continuously differentiable, so we can use gradient descent methods to \"train\" a single sigmoid unit and an algorithm called **backpropagation** to train a network of units.\n",
    "\n",
    "The cost function is, again:\n",
    "\n",
    "$$J(w)=\\frac{1}{2}\\sum_{i}{\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right)^2}$$\n",
    "\n",
    "$\\phi(z)$ is the sigmoid function and its derivative is $\\frac{\\partial}{\\partial z}{\\phi(z)} = \\phi(z)\\left(1-\\phi(z)\\right)$.\n",
    "\n",
    "The derivative of $J$ is then:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2}\\sum_{i}{\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right) \\right)^2} \\\\\n",
    "    &= \\frac{1}{2} \\sum_{i} 2\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\frac{\\partial}{\\partial w_j}\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\\\\n",
    "    &= \\sum_{i} \\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\left(- \\frac{\\partial}{\\partial w_j} \\phi\\left(z^{(i)}\\right) \\right) \\\\\n",
    "    &= \\sum_{i} \\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\phi(z^{(i)})\\left(1-\\phi(z^{(i)})\\right) \\left(- \\frac{\\partial}{\\partial w_j} z^{(i)}\\right) \\\\\n",
    "    &= -\\sum_{i} \\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\phi(z^{(i)})\\left(1-\\phi(z^{(i)})\\right) x_j^{(i)}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "This is the objective function to be minimized for a single neuron. In a multi-layer network, the inputs to each neuron depends from the outputs produced by the neurons of the previous layers. \n",
    "\n",
    "Although the nonlinear sigmoid neuron function overcomes some of the limitations of the perceptron, like being able to converge even if the points are not linearly separable, in order to find nonlinear boundaries between the data we need to compose many sigmoid units together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rectified Linear Unit\n",
    "There are plenty of [activation functions](https://en.wikipedia.org/wiki/Activation_function) that can be used. One that's typically used is the **rectifier**, used to build ReLU (Rectified Linear Unit). \n",
    "\n",
    "$$\\phi(z)=\\max\\left\\{0,z\\right\\}$$\n",
    "\n",
    "its derivative is:\n",
    "\n",
    "$$\\phi'(z) = \\begin{cases} 0 & \\text{if } z<0 \\\\ 1 & \\text{if } z>0 \\\\ \\text{undefined} & \\text{if } z=0 \\end{cases}$$\n",
    "\n",
    "When we implement the ReLU derivative in a computer, we can choose to assign either value 1 or 0 to the point $z=0$. The main advantage of the rectifier over the sigmoidal activation function is that it leads to fewer [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problems, since its derivative takes value 1, which is not close to zero for a large number of $z$ points, and rarely takes value 0 because the $z$ points will often be positive in a neural network, allowing the learning phase to speed up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rectifier(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "z = np.arange(-6, 6, 0.1)\n",
    "phi_z = rectifier(z)\n",
    "plt.plot(z, phi_z)\n",
    "#plt.axvline(x=0.0, color='k', alpha=0.5)\n",
    "#plt.axhline(y=0.0, color='k', alpha=0.5)\n",
    "plt.title('Rectifier function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\phi (z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this function to the output of a linear transformation yelds a nonlinear function. However, the function remains close to be linear, in the sense that it is a **piecewise linear function**.\n",
    "\n",
    "Regarding the choice of the activation function, some properties are desirable:\n",
    "- **Nonlinear**: using a nonlinear activation function, then a two-layer neural network can be proven to be a universal function approximator ([Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)). The identity activation function $f(z)=z$ doesn't satisfy this property. When multiple layers use the identity activation function, then the entire network can be reduced to a single-layer model.\n",
    "- **Continuously differentiable**: in order to enable gradient-based optimization methods. For example the binary step function (used by the Perceptron) is not differentiable at 0, so gradient methods can make no progress with it. The ReLU is also non-differentiable at 0, anyway it is differentiable anywhere else and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.\n",
    "- **Monotonic**: when the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.\n",
    "- **Smooth functions with a monotonic derivative**: these have been shown to generalize better in some cases.\n",
    "- **Approximates identity near the origin**: when activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.\n",
    "\n",
    "When building a neural network, avoid using the sigmoid activation function, except for the output layer if we are doing binary classification, since the hyperbolic tangent $\\phi(z) = tanh(z)$ is strictly superior to it. The standard activation function which is typically used is the ReLU or, in some cases, the leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer networks\n",
    "The term *network* comes to the fact that multiple functions are composed together in a model resulting into a directed acyclic graph. For example we may have 3 functions $f^{(1)}, f^{(2)}, f^{(3)}$ composed together in a chain to form $f(\\mathbf{x}) = f^{(3)}\\left( f^{(2)} \\left( f^{(1)}(\\mathbf{x})\\right)\\right)$. The i-th function is called the i-th **layer** and the length of the chain gives the **depth** of the model. The final layer is the **output layer**. The training data provides us noisy examples of $f^*(\\mathbf{x})$ evaluated at different points. Those training examples specify what the output layer should produce for each input $\\mathbf{x}$, anyway, they don't specify what the behavior of the intermediate layers should be. It is the task of the learning algorithm to regulate their behaviour. Because the training data doesn't show the desired output of each of these layers, they are called **hidden layers**. A layer can be thought either as a vector-to-vector function or as many units acting in parallel, each representing a vector-to-scalar function (a single neuron).\n",
    "\n",
    "Linear models, such as linear regression or logistic regression can be fit efficiently, either in closed form or with convex optimization. Anyway, they have the defect to limit the model capacity to linear functions. In order to extend its capacity to nonlinear functions of $\\mathbf{x}$, instead of applying the linear model to $\\mathbf{x}$, we can apply it to a transformed input $\\phi(\\mathbf{x})$ where $\\phi$ is a nonlinear function defining a hidden layer and providing a new representation of $\\mathbf{x}$. In feedforward neural nets we have a model $y=f(x;\\theta, w) = \\phi(x;\\theta)^\\top w$ where the parameters $\\theta$ are used to learn $\\phi$ from a broad class of functions (that class has to be chosen and this is not trivial at all).\n",
    "\n",
    "Similarly to linear models, the optimizer, the cost function and the form of the output units still still have to be chosen. Furthermore, since feedforward networks introduced the concept of hidden layers, we must also choose the activation functions that will be used to compute the weights of the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a XOR function\n",
    "The truth table of the XOR function is:\n",
    "\n",
    "| x1 | x2 | XOR |\n",
    "|:--:|:--:|:---:|\n",
    "|  0 |  0 |  0  |\n",
    "|  0 |  1 |  1  |\n",
    "|  1 |  0 |  1  |\n",
    "|  1 |  1 |  0  |\n",
    "\n",
    "We can treat this problem as a regression problem using the MSE as the loss function (in general, we should not use MSE as a cost function for modelling binary data, but prefer the logit function as we've seen in the chapter on logistic regression).\n",
    "\n",
    "$$J(\\theta) = \\sum_{x\\in \\mathbb{X}}{\\left(f^*(x) - f(x;\\theta) \\right)^2}$$\n",
    "\n",
    "If we choose $f(x;\\theta)$ to be a linear model $f(x;\\theta) = \\mathbf{w}^\\top x + b$ with $\\mathbf{w}\\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$, and solve the normal equations, we would end up with $\\mathbf{w} = \\mathbf{0}$ and $b=\\frac{1}{2}$, that is a model that always predict $y = \\frac{1}{2}$. We have to use a nonlinear model that is able to learn a different feature space in which a linear model can represent the solution.\n",
    "\n",
    "Let's then introduce an intermediate layer containing two units resulting in a vector of hidden units $h$ that are computed by $h = f^{(1)}\\left(x; W,c\\right)$. The output layer will be a linear regression model $y = f^{(2)}\\left(h; w, b\\right) = w^\\top h + b$, but acting on $h$ instead of $x$, a representation of the input in a new feature space. The entire network is represented by $f\\left(\\mathbf{x}; \\mathbf{W},\\mathbf{c},\\mathbf{w},b\\right) = f^{(2)}\\left(f^{(1)}(\\mathbf{x})\\right)$ where $f^{(1)}$ has to be nonlinear, otherwise the network would result in a linear function of the input $x$, being unable to learn a XOR function.\n",
    "\n",
    "As we've seen before, a ReLU unit might come to our aid by combining the input in an affine trasformation (obtaining the net input) that will then be fed to a nonlinear function (the rectifier). Let then $h$ be $h=g\\left(W^\\top x + c\\right)$ where $W$ is a matrix containing the weights of the linear trasformation and $c$ the biases (which is a vector, because the affine transformation is from a vector $x$ to a vector $h$. The activation function is typically appliable element-wise, with $h_i = g\\left(x^\\top W_{:,i} + c_i \\right)$, we'll use the rectifier activation function $g(z)=\\max\\left\\{0,z\\right\\}$. The full network is then:\n",
    "\n",
    "$$f\\left(\\mathbf{x}; \\mathbf{W},\\mathbf{c},\\mathbf{w},b\\right) = \\mathbf{w}^\\top \\max\\left\\{0,\\mathbf{W}^\\top \\mathbf{x} + \\mathbf{c}\\right\\} + b $$\n",
    "\n",
    "and it has the following structure:\n",
    "\n",
    "<img src='images/neural_networks/XOR_net.png' style=\"width:15em\"/>\n",
    "\n",
    "Now we can manually specify the solution to the XOR problem letting:\n",
    "\n",
    "$$W = \\begin{bmatrix}1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\qquad c = \\begin{bmatrix}0\\\\ -1 \\end{bmatrix} \\qquad  w=\\begin{bmatrix}1\\\\ -2 \\end{bmatrix} \\qquad b=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[[0 0]], net_input=[[ 0. -1.]], h=[[0. 0.]], y=0.0\n",
      "x=[[0 1]], net_input=[[1. 0.]], h=[[1. 0.]], y=1.0\n",
      "x=[[1 0]], net_input=[[1. 0.]], h=[[1. 0.]], y=1.0\n",
      "x=[[1 1]], net_input=[[2. 1.]], h=[[2. 1.]], y=0.0\n"
     ]
    }
   ],
   "source": [
    "# inputs and parameters\n",
    "X = np.matrix([(0,0),(0,1),(1,0),(1,1)])\n",
    "W = np.ones((2,2))\n",
    "c = np.array([0,-1])\n",
    "w = np.array([1,-2])\n",
    "b = 0\n",
    "\n",
    "# computations of the network\n",
    "net_input = X @ W + c\n",
    "h = rectifier(net_input)\n",
    "y = h.dot(w).T + b\n",
    "\n",
    "# visualize outputs\n",
    "for x, combination, activation, output in zip (X,net_input, h, y):\n",
    "    print('x={}, net_input={}, h={}, y={}'.format(x, combination, activation, output.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real problems, instead of manually choosing parameters values, they are found by a gradient-based optimization algorithm.\n",
    "\n",
    "In the following plots we can see how the inputs are mapped from the original feature space, in which they weren't linearly separable, to a new feature space by the hidden layer computation, where they can be perfectly separated by a suitable linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAEWCAYAAACtyARlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAWfklEQVR4nO3dfZRcdX3H8fc3D7BRIiEJomEjGw0nPUitIJh6FEXERhGFUHyiIoVQtK2HaJVG9PBQqxVFrXisVRSNTxAl0oBijceHHrRKSTD4QDBtjMHsRiCsRAImGMivf9w7MBk2m93s3Jnf7Lxf58zZnXvv3N9vZuc7n/u7987eSCkhSZLyMqHdHZAkSY9nQEuSlCEDWpKkDBnQkiRlyICWJClDBrQkSRkyoDMVEYdExE0RsS0iPtzu/kgqRMTGiDhxFMu/NyLujYi7quyXxh8DugUi4oCyqM+omzY1In4TEafv4WHnAfcCT0opvX2M7S+NiPeOZR2SRi8iZgNvB45IKT1ljOs6PiL6m9MzdQIDugVSSg9QBO4VEXFwOfmDwOqU0vI9POwwYG3K4D/JRMSkdvdB6lCHAYMppXva3RHruAOllLy16AYsBa4BjgcGgacOs9xO4I/AA8CJFBtT7wR+VT72q8D0usdcC9wF/B64CXhmOf28hnV9vZyegLkNbb63/P14oB9YUq7zi3trv6H/M4FvAFuB3wE/ACaU8zYCFwJrgfuAzwE95byDysdtKed9A+itW+/0cvnN5fwVdfNOBm4r2/wR8Kx2/729jc9b+R5+B/Czst6+UnsPNyx3IrAd2FXW3tJy+p+X79GtwE+B4+seczZwB7AN2AC8qZz+xIZ1PQDMqq/bcrnjgf6Gvi4p+/oQMGm49od4DkuAgbI/64CXlNMvBZaXz30b8BPgz+oeV/us2FbW+sKG9f5N3fNcCxxdTp8FfK38DPg1cH67/95tfa+1uwPddCsD6LcUu67P3suyjYX3VuBmoBfYH/gUcE3d/HOAqeW8jwK37Wld5bS9BfTDwAfK9U3ZW/sN634/8Elgcnk7Dohy3kbgF8BsisD977p2ZwB/CTyhfC7XsnsI31h+IBxUrvdF5fSjgXuA+cBE4Kyynf3b/Tf3Nv5u5XvrljJMppdB8+Y9LHs8uwfmoRQbuCdRbPS+tLx/cDn/FcAzgABeBPyhLrx2W1c5rfFzorG9jRQbrrPLOh62/YZ1zwM2AbPK+33AM8rfL6XY8D+9rMV3UATq5HL+q8vXZwLwWuBBygFJOW8AOLZ8nnMp9jRMAG4FLgb2A55OsZGyoN1/87a919rdgW67Ad8pi+7AvSzXWHh3UG69lvefWhbIpCEeO40igA8cal3ltL0F9B+pGxWMsv33ANfXr79u3kbqPszKD4pf7eE1eDZwX117u4CDhlju34F/bpi2jjLAvXlr5q18D7+h7v4HgU/uYdnGwFwCfLFhmZXAWXt4/Apg8VDrKqc1fk40trcROGdf2i+D8x6KPQGTG+ZdCtxcd38CxeDjuD08j9uAU+raWzzEMvOB3zRMuxD4XLv/5u26eQy6hSLiDRRbod+hGJ2OxmHAf0TE1ojYShGYjwCHRMTEiLgsIn4VEfdTFCUUu5r31ZaU0o6RtD/EYy8H1gPfjogNEfHOhvmb6n6/k2JLm4h4QkR8KiLuLJ/HTcC0iJhIMQL4XUrpviHaOwx4e61vZf9m19YrVaD+jOw/AAeM8HGHAa9ueK++gGIDlIh4eUTcHBG/K+edxNjqGHavt2Hbr5dSWk+x5+xS4J6IWBYR9TW1qW7ZXRSHxWq1/MaIuK2ujSPrnsdsit3fjQ4DZjX07V0M/RnTFQzoFomIJwP/SnHs5U3AayLihaNYxSbg5SmlaXW3npTSAHAGcArFlu6BFBsBUOw+gmK03OgPFLuSaxrPMG18zHDt7/7AlLallN6eUno68ErgHyLiJXWLzK77/WkUx5ShONt1HjA/pfQkoPb6RNn+9IiYNsRz2QS8r6FvT0gpXTPEslI7baIYwda/V5+YUrosIvanOP76IeCQlNI04JsMX8cPMnwdNz5uj+0P1dmU0tUppRdQhGdi94HFo3UcERMoDn9tjojDgE8DbwFmlM/jF3XPYxPFbvxGm4BfN/RtakrppKH61g0M6Nb5OMXx1O+nlH4L/CPw6bIoR+KTwPvKNz8RcXBEnFLOm0pxAsggRbH+S8Nj76Y4nlPvNuCMcvT9MorjXfva/m4i4uSImBsRAdxPMdJ+pG6Rv4+I3oiYTrGF/JW657Ed2FrOu6T2gPI1+0/gExFxUERMrtvA+TTw5oiYH4UnRsQrImLqXp6T1GpfAl4ZEQvK2uspvz7VS3HcdX+KE6QejoiXA39R99i7gRkRcWDdtNuAkyJiekQ8hWLEu6/t7yYi5kXECeVn1A6K2qyv4+dExGnl2eFvpfgMupnihLZUPg8i4myKEXTNZ4B3RMRzynqdW36u3ALcHxFLImJK2b8jI+LYvTynccuAboGIOJViN9IFtWkppc9Q7BK6eISruQK4gWK38TaKQphfzvsCxa7iAYozIm9ueOxVwBHlbqMV5bTFFKPbrcBfURzr2tf2Gx1OsRv/AeDHwCdSSv9VN/9q4NsUJ4BsAGrf0f4oxYks95br/1bDes+kOO79S4pjY28FSCmtptgz8XGKs7vXA3+9l+cjtVxKaRPF3q53UQTYJorPhQkppW3A+RTfkLiPYs/YDXWP/SXFt0A2lLU8i+IbFj+lOKz1bR7b2B11+0Msvj9wGUU93gU8uXxczfUUJ4DdR1Gbp6WUdqaU1gIfpqj9u4E/pTgZtNaHa4H3UXwObKP47JmeUnqE4jPp2RQnnN1LEeb1GyRdpXZmrdQSEbERODel9J1290XSvomISylOAn1Du/synjmCliQpQwa0JEkZche3JEkZcgQtSVKGsvrn6TNnzkx9fX3t7oaUtVtvvfXelNLBe1+yfaxlaWSGq+esArqvr4/Vq1e3uxtS1iLiznb3YW+sZWlkhqtnd3FLkpQhA1qSpAwZ0JIkZSirY9CSxq+dO3fS39/Pjh079r7wONLT00Nvby+TJ09ud1fUYQxoSS3R39/P1KlT6evro7iOyviXUmJwcJD+/n7mzJnT7u6ow7iLW1JL7NixgxkzZnRNOANEBDNmzOi6vQZqDgNaUst0UzjXdONzVnMY0JIkZciAlqR99NBDD/Ha176WuXPnMn/+fDZu3NjuLmkcMaAlaR9dddVVHHTQQaxfv563ve1tLFmypN1d0jhiQEvK0oo1Azz/su8x55038vzLvseKNQNjWt9FF13EFVdc8ej9d7/73XzsYx8b0zqvv/56zjrrLABOP/10vvvd7+IVAtUsfs1KUnZWrBngwut+zvadjwAwsHU7F173cwBOPerQfVrnokWLOO2001i8eDG7du1i2bJl3HLLLY9b7rjjjmPbtm2Pm/6hD32IE088cbdpAwMDzJ49G4BJkyZx4IEHMjg4yMyZM/epj1I9A1pSdi5fue7RcK7ZvvMRLl+5bp8Duq+vjxkzZrBmzRruvvtujjrqKGbMmPG45X7wgx+MeJ1DjZY9a1vNYkBLys7mrdtHNX2kzj33XJYuXcpdd93FOeecM+QyoxlB9/b2smnTJnp7e3n44Yf5/e9/z/Tp08fUR6nGgJaUnVnTpjAwRBjPmjZlTOtduHAhF198MTt37uTqq68ecpnRjKBf9apX8fnPf57nPe95LF++nBNOOMERtJrGgJaUnQsWzNvtGDTAlMkTuWDBvDGtd7/99uPFL34x06ZNY+LEiWPtJosWLeLMM89k7ty5TJ8+nWXLlo15nVKNAS0pO7XjzJevXMfmrduZNW0KFyyYt8/Hn2t27drFzTffzLXXXtuMbtLT09O0dUmNDGhJWTr1qEPHHMj11q5dy8knn8zChQs5/PDDm7ZeqSoGtKSucMQRR7Bhw4Z2d0MaMf9RiSRJGTKgJUnKkAEtSVKGDGhJkjJkQEvSPrrppps4+uijmTRpEsuXL293dzTOGNCStI+e9rSnsXTpUs4444x2d0XjkF+zkpSvz72i+Hn2jWNe1UUXXcTMmTNZvHgxUFxu8pBDDuH888/f53X29fUBMGGCYx01X2UBHRGfBU4G7kkpHVlVO2PSxOJXd1mxZqDp/+UqZx1Rz3tRxeUm1Z1aVf9VjqCXAh8HvlBhG1LLVXGt4g6wlFbWc23j+c4f7n5/DBvTVVxuUt2nlfVfWUCnlG6KiL6q1j8mFRS/ukcV1yrOXdb1PArNvtykuk8r67/tx6Aj4jzgPChOuJByV9W1ijtdU2u5trHc5I3nZl9uUt2nlfXf9oBOKV0JXAlwzDHHpJY0WlHxqztUda3iTteWWh6lZl9uctWqVSxcuJD77ruPr3/961xyySXcfvvtTeipctXK+vfUQ2mULlgwjymTd/9wb8a1ijWEs29s6gZ07XKTixYtasr6jj32WPr7+3nwwQcZHBw0nLtAK+u/7SPotnLkrH1Q1bWKVS0vN6lmaGX9V/k1q2uA44GZEdEPXJJSuqqq9qRWava1inM3HurZy02qWVpV/1Wexf36qtYtqbWaVc8pJSKiGavqGClleTheHcBj0JJaoqenh8HBwa4KrJQSg4OD9PT0tLsr6kDdfQxaUsv09vbS39/Pli1b2t2Vlurp6aG3t7fd3VAHMqAltcTkyZOZM2dOu7shdQx3cUuSlCEDWpKkDBnQkiRlyICWJClDBrQkSRkyoCVJypABLUlShgxoSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMGdCSJGXIgJYkKUMGtCRJGTKgJUnKkAEtSVKGDGhJkjJkQEuSlCEDWpKkDBnQkiRlyICWJClDBrQkSRkyoCVJypABLUlShgxoSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMGdCSJGXIgJYkKUMGtCRJGTKgJUnKkAEtSVKGDGhJkjJkQEuSlCEDWpKkDBnQkiRlyICWJClDBrQkSRkyoCVJypABLUlShgxoSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMDRvQEfGkiHjGENOfVV2XJFXBepY6yx4DOiJeA/wS+FpE3B4Rx9bNXlp1xyQ1j/UsdZ7hRtDvAp6TUno2cDbwxYg4rZwXlfdMUjNZz1KHmTTMvIkppd8CpJRuiYgXA9+IiF4gtaR3kprFepY6zHAj6G31x6vK4j4eOAV4ZsX9ktRc1rPUYYYL6L8FJkTEEbUJKaVtwMuAc6vumKSmsp6lDrPHgE4p/TSl9H/AVyNiSRSmAB8B/q5lPZQ0Ztaz1HlG8j3o+cBs4EfAKmAz8PwqOyWpMtaz1CFGEtA7ge3AFKAH+HVKaVelvZJUFetZ6hAjCehVFAV9LPAC4PURsbzSXkmqivUsdYjhvmZVsyiltLr8/S7glIg4s8I+SaqO9Sx1iL2OoOuKuX7aF6vpjqQqWc9S5/BiGZIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMGdCSJGXIgJYkKUMGtCRJGTKgJUnKkAEtSVKGDGhJkjJkQEuSlCEDWpKkDBnQkiRlyICWJClDBrQkSRkyoCVJypABLUlShgxoSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMGdCSJGXIgJYkKUMGtCRJGTKgJUnKkAEtSVKGDGhJkjJkQEuSlCEDWpKkDBnQkiRlyICWJClDBrQkSRkyoCVJypABLUlShgxoSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMGdCSJGXIgJYkKUMGtCRJGTKgJUnKkAEtSVKGDGhJkjJkQEuSlCEDWpKkDBnQkiRlyICWJClDk6pceUS8DLgCmAh8JqV0WZXtjcaKNQNcvnIdm7duZ9a0KVywYB6nHnVou7ulTvK5VxQ/z76xvf1ogVbWsrVZkffPLn5euKm9/dCIVRbQETER+DfgpUA/sCoibkgpra2qzZFasWaAC6/7Odt3PgLAwNbtXHjdzwH8IJAatLKWrU3pMVWOoJ8LrE8pbQCIiGXAKUDbA/rylese/QCo2b7zES5fuc4PAe1dbeR85w93vz9+R9Itq2VrswK1kfND9+9+35F09qo8Bn0oUP8O6C+n7SYizouI1RGxesuWLRV25zGbt24f1XSpy7Wslq1N6TFVjqBjiGnpcRNSuhK4EuCYY4553PwqzJo2hYEhCn7WtCmtaF6drjZSHv8j55qW1bK1WYHaSNmRc8epcgTdD8yuu98LbK6wvRG7YME8pkyeuNu0KZMncsGCeW3qkZS1ltWytSk9psoR9Crg8IiYAwwArwPOqLC9Easdy/JMUY3J+B8517Sslq3NCjly7jiVBXRK6eGIeAuwkuKrGZ9NKd1eVXujdepRh1r00gi0upatTalQ6fegU0rfBL5ZZRuSqmctS63nfxKTJClDBrQkSRkyoCVJypABLUlShgxoSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAS5KUIQNakqQMGdCSJGXIgJYkKUMGtCRJGTKgJUnKkAEtSVKGDGhJkjJkQEuSlCEDWpKkDBnQkiRlyICWJClDkVJqdx8eFRFbgDtb3OxM4N4Wt6nxox3vn8NSSge3uM1RaVItW5vN52vaXM14PfdYz1kFdDtExOqU0jHt7oc6k++f6vjaNp+vaXNV/Xq6i1uSpAwZ0JIkZciAhivb3QF1NN8/1fG1bT5f0+aq9PXs+mPQkiTlyBG0JEkZMqAlScpQ1wZ0RLwsItZFxPqIeGe7+6POEhGfjYh7IuIX7e7LeGNtNpfv1eaLiNkR8f2IuCMibo+IxZW0043HoCNiIvC/wEuBfmAV8PqU0tq2dkwdIyJeCDwAfCGldGS7+zNeWJvN53u1+SLiqcBTU0o/iYipwK3Aqc1+n3brCPq5wPqU0oaU0h+BZcApbe6TOkhK6Sbgd+3uxzhkbTaZ79XmSyn9NqX0k/L3bcAdwKHNbqdbA/pQYFPd/X4qeHEljZq1qY4SEX3AUcD/NHvd3RrQMcS07tvXL+XH2lTHiIgDgK8Bb00p3d/s9XdrQPcDs+vu9wKb29QXSY+xNtURImIyRTh/OaV0XRVtdGtArwIOj4g5EbEf8Drghjb3SZK1qQ4QEQFcBdyRUvpIVe10ZUCnlB4G3gKspDi4/9WU0u3t7ZU6SURcA/wYmBcR/RGxqN19Gg+szebzvVqJ5wNnAidExG3l7aRmN9KVX7OSJCl3XTmCliQpdwa0JEkZMqAlScqQAS1JUoYMaEmSMmRAa1gR8a2I2BoR32h3XySNjfXcWQxo7c3lFN/3k9T5rOcOYkALgIg4NiJ+FhE9EfHE8hqnR6aUvgtsa3f/JI2c9Tw+TGp3B5SHlNKqiLgBeC8wBfhSSskLvEsdyHoeHwxo1XsPxf9C3gGc3+a+SBob67nDuYtb9aYDBwBTgZ4290XS2FjPHc6AVr0rgYuALwMfaHNfJI2N9dzh3MUtACLijcDDKaWrI2Ii8KOIOAH4J+BPgAMioh9YlFJa2c6+Shqe9Tw+eDUrSZIy5C5uSZIyZEBLkpQhA1qSpAwZ0JIkZciAliQpQwa0JEkZMqAlScrQ/wOg9ANa9ZPnAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@Visualize feature spaces\n",
    "X1 = np.array(X[:,0])\n",
    "X2 = np.array(X[:,1])\n",
    "h1 = np.array(h[:,0])\n",
    "h2 = np.array(h[:,1])\n",
    "y = np.array(y)\n",
    "markers = ('o', '+')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "    ax1.scatter(X1[y == cl], X2[y == cl], marker = markers[idx], label='y = {}'.format(int(cl)))\n",
    "    ax2.scatter(h1[y == cl], h2[y == cl], marker = markers[idx], label='y = {}'.format(int(cl)))\n",
    "\n",
    "margin = 0.2\n",
    "ax1.set_title('X feature space')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.set_xticks([0,1])\n",
    "ax1.set_yticks([0,1])\n",
    "ax1.set_xlim([0-margin, 1+margin])\n",
    "ax1.set_ylim([0-margin, 1+margin])\n",
    "\n",
    "ax2.set_title('h feature space')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "ax2.set_xticks([0,1,2])\n",
    "ax2.set_yticks([0,1])\n",
    "ax2.set_xlim([0-margin, 2+margin])\n",
    "ax2.set_ylim([0-margin, 1+margin])\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual interpretation\n",
    "The goal of the following explanation is to provide an insight on what is the meaning of composing multiple neurons into a network, without any mathematical rigor. In order to obtain a nonlinear model we can take two sigmoid units, each of which assigns a probability to each data point, and sum those probabilities together, point by point, obtaining new values that have to be rescaled back between 0 and 1 (and that can be done with another sigmoid function). If we want to give more importance to one of the two units, we can take a linear combination of the two, instead of simply summing them together, and we also add a bias term (otherwise the resulting value would always be positive and, if fed to a sigmoid unit, would result in always being classified as positive).\n",
    "\n",
    "<img src=\"images/neural_networks/visual_network.png\" style=\"width:50em\" />\n",
    "\n",
    "which is equivalent this neural network:\n",
    "\n",
    "<img src=\"images/neural_networks/visual_network_2.jpg\" style=\"width:20em\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and vectorized notation\n",
    "From now on, I will use the following [notation](https://cs230.stanford.edu/files/Notation.pdf):\n",
    "- superscript $(i)$: i-th training example\n",
    "- superscript $[l]$: l-th layer\n",
    "- $m$: number of examples in the dataset\n",
    "- $n_x$: number of input units\n",
    "- $n_{h}^{[l]}$: number of units in the l-th hidden layer\n",
    "- $n_y$: number of output units (or number of classes)\n",
    "- $L$: number of layers in the network\n",
    "- $x^{(i)}\\in \\mathbb{R}^{n_x}$ is the i-th training example represented as a column vector\n",
    "- $X\\in \\mathbb{R}^{n_x \\times m}$: input matrix (the i-th column correspond to the i-th training example)\n",
    "- $Y\\in \\mathbb{R}^{n_y \\times m}$ is the label matrix\n",
    "- $y^{(i)}\\in \\mathbb{R}^{n_y}$ is the output label for the i-th example\n",
    "- $b^{[l]}\\in \\mathbb{R}^{n^{[l+1]}}$: bias vector in the l-th layer\n",
    "- $\\hat{y}\\in \\mathbb{R}^{n_y}$: predicted output vector\n",
    "- $x_{ji}$: input from unit $i$ into unit $j$\n",
    "- $w_{ji}$: weight from unit $i$ into unit $j$\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l+1]} \\times n^{[l-1]}}$: the weight matrix associated with the l-th layer\n",
    "- $a_j^{[l]}$: the output of the j-th neuron of the l-th hidden layer.\n",
    "\n",
    "### Example\n",
    "Consider this network with $n_x=3$ inputs in the input layer, 1 hidden layer with $n_h^{[1]}=2$ units and an output layer with $n_y=2$ output unit:\n",
    "\n",
    "<img src=\"images/neural_networks/notation.png\" style=\"width:25em\" />\n",
    "\n",
    "where \n",
    "\n",
    "$$W^{[1]}= \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix}, \\qquad \n",
    "W^{[2]}= \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix}, \\qquad\n",
    "a^{[1]} = \\begin{bmatrix} a_{1}^{[1]} \\\\ a_{2}^{[1]} \\end{bmatrix}, \\qquad\n",
    "a^{[2]} = a_1^{[2]}$$\n",
    "\n",
    "In this representation the bias terms are omitted and they have to be considered implicitly inside each neuron. For a single input data $x^{(i)}$ the computation will proceed as follows:\n",
    "\n",
    "$$\n",
    "a^{[1](i)} = \\phi\\left(W^{[1]}x^{(i)} + b^{[1]} \\right), \\qquad\n",
    "\\hat{y} = a^{[2](i)} = \\phi\\left(W^{[2]}a^{[1](i)} + b^{[2]} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We can use a vectorized notation to represent $m$ input data $x^{(i)}$ inside $X$:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "| & | & | \\\\\n",
    "x^{(1)} & \\dots & x^{(m)} \\\\\n",
    "| & | & | \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times m}\n",
    "$$\n",
    "\n",
    "The computation will proceed in this way:\n",
    "\n",
    "$$\n",
    "A^{[1]} = \\phi\\left(W^{[1]}X + b^{[1]} \\right) \\in \\mathbb{R}^{2}, \\qquad\n",
    "\\hat{y} = A^{[2]} = \\phi\\left(W^{[2]}A^{[1]} + b^{[2]} \\right) \\in \\mathbb{R}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation algorithm\n",
    "The main difference between linear models and neural netoworks is that the nonlinearity of neural network causes the loss function to become non-convex. Consequently, stochastic gradient descent applied to such types of functions has not convergence guarantee and is sensitive to the initial values of the parameters. Sticking with the previous example, the cost function will be:\n",
    "\n",
    "$$J\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} \\right) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)$$\n",
    "\n",
    "where, $\\mathcal{L}(\\cdot)$ is the loss function that, in case of a binary classification problem, can be the logistic cost function (or log-likelihood function) that we already defined in the chapter about logistic regression, and $\\hat{y}^{(i)} = a^{[2]}$ is the output predicted by the network associated to the i-th input.\n",
    "\n",
    "Proceed to minimize $J$ with the gradient descent algorithm that we already know: \n",
    "\n",
    "Repeat until the termination condition is met:\n",
    "- compute the prediction $\\hat{y}^{(i)}$ for each input ($i=1...m$)\n",
    "- compute the derivatives of the cost function with respect to the parameters: $\\text{d}W^{[1]}=\\frac{\\partial J}{\\partial W^{[1]}}$, $\\text{d}b^{[1]}=\\frac{\\partial J}{\\partial b^{[1]}}$, $\\text{d}W^{[2]}=\\frac{\\partial J}{\\partial W^{[2]}}$, $\\text{d}b^{[2]}=\\frac{\\partial J}{\\partial b^{[2]}}$.\n",
    "- update each parameter value according to the learning rate:  \n",
    "    - $W^{[1]} \\leftarrow W^{[1]} - \\eta \\text{d}W^{[1]}$\n",
    "    - $b^{[1]} \\leftarrow b^{[1]} - \\eta \\text{d}b^{[1]}$\n",
    "    - $W^{[2]} \\leftarrow W^{[2]} - \\eta \\text{d}W^{[2]}$\n",
    "    - $b^{[2]} \\leftarrow b^{[2]} - \\eta \\text{d}b^{[2]}$\n",
    "\n",
    "The tricky part is how to compute these partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
