{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vdMkvYVqFNw"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pietroventurini/machine-learning-notes/blob/master/6.1%20-%20Feedforward%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_dJPOYAprXX"
   },
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "## Contents\n",
    "1. [**Introduction**](#Introduction)  \n",
    "    1.1. The sigmoid unit  \n",
    "    1.2. The Rectified Linear Unit (ReLU)  \n",
    "2. [**Multi-layer neural networks**](#Multi-layer-neural-networks)  \n",
    "    2.1. Learning a XOR function  \n",
    "    2.2. Visual interpretation  \n",
    "    2.3. Generalization and vectorized notation  \n",
    "3. [**The Backpropagation Algorithm**](#The-Backpropagation-Algorithm)  \n",
    "    3.1. Computing the partial derivatives  \n",
    "    3.2. Backpropagation with vectorized input  \n",
    "    3.3. What happens in each layer?  \n",
    "    3.4. Initialization  \n",
    "    3.5. Best practices  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AycLYfB1prXZ"
   },
   "source": [
    "# Introduction\n",
    "A **feedforward neural network**, also called **multilayer perceptron** (MLP), is a type of **artificial neural network** (ANN) wherein connections between nodes do not form a cycle (differently from its descendant: the *recurrent neural network*). Its goal is to approximate some function $f^*$ by defining a mapping $y=f(x;w)$ and learning the value of the parameters $w$ that result in the best approximation of $f^*$. It is called *feedforward* because information flows from the input layer $x$, through the intermediate layers, to the output $y$, without any *feedback* connections where the outputs are fed back into the network. Before talking about networks, we have to introduce the units that will make up the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cg7_oxwXprXb"
   },
   "source": [
    "### The sigmoid unit\n",
    "Let's first introduce a \"building block\" for our neural network: the **sigmoid unit**. It is simply an artificial neuron in which the activation function is a sigmoid function (which has already been introduced in the chapter about Logistic Regression).\n",
    "\n",
    "<img src=\"images/neural_networks/sigmoid_unit.jpg\" style=\"width:45em; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "Differently from the perceptron, which doesn't distinguish between points that lie close to the boundary and points that lie far, the sigmoid neuron outputs a value between 0 and 1. So, if we think of a classification model composed by only one sigmoid unit, we can consider the output value as the probability of the input point to be classified as positive.\n",
    "\n",
    "The sigmoid function is monotonic and continuously differentiable, so we can use gradient descent methods to \"train\" a single sigmoid unit and an algorithm called **backpropagation** to train a network of units.\n",
    "\n",
    "The cost function is, again:\n",
    "\n",
    "$$J(w)=\\frac{1}{2}\\sum_{i}{\\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right)\\right)^2}$$\n",
    "\n",
    "$\\sigma(z)$ is the sigmoid function and its derivative is $\\frac{\\partial}{\\partial z}{\\sigma(z)} = \\sigma(z)\\left(1-\\sigma(z)\\right)$. Its argument, $z^{(i)}=w^\\top x^{(i)}$, is the linear combination between the weights vector $w$ and the *i*-th input $x^{(i)}$. The partial derivative of $z$ with respect to the *j*-th parameter $w_j$ is simply $\\frac{\\partial}{\\partial w_j}{z^{(i)}} = x_{j}^{(i)}$.\n",
    "\n",
    "The derivative of $J$ with respect to $w_j$ is then:\n",
    "\n",
    "$$\\begin{split}\n",
    "    \\frac{\\partial J}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2}\\sum_{i}{\\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right) \\right)^2} \\\\\n",
    "    &= \\frac{1}{2} \\sum_{i} 2\\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right)\\right) \\frac{\\partial}{\\partial w_j}\\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right)\\right) \\\\\n",
    "    &= \\sum_{i} \\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right)\\right) \\left(- \\frac{\\partial}{\\partial w_j} \\sigma\\left(z^{(i)}\\right) \\right) \\\\\n",
    "    &= \\sum_{i} \\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right)\\right) \\sigma(z^{(i)})\\left(1-\\sigma(z^{(i)})\\right) \\left(- \\frac{\\partial}{\\partial w_j} z^{(i)}\\right) \\\\\n",
    "    &= -\\sum_{i} \\left(y^{(i)}-\\sigma\\left(z^{(i)}\\right)\\right) \\sigma(z^{(i)})\\left(1-\\sigma(z^{(i)})\\right) x_j^{(i)}\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "This is the objective function to be minimized for a single neuron. In a multi-layer network, the inputs to each neuron depend from the outputs produced by the neurons of the previous layers. Note that the units of the *i*-th layer are directly connected to the units of the *(i+1)*-th layer, and not to the units of the subsequent layers.\n",
    "\n",
    "Although the nonlinear sigmoid neuron function overcomes some of the limitations of the perceptron, such as being able to converge even if the points are not linearly separable, in order to find nonlinear boundaries between the data we need to compose many sigmoid units together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiTgQfk3prXc"
   },
   "source": [
    "### The Rectified Linear Unit\n",
    "There are plenty of [activation functions](https://en.wikipedia.org/wiki/Activation_function) that can be used. One that's typically used is the **rectifier**, used to build the ReLU (Rectified Linear Unit). \n",
    "\n",
    "$$g(z)=\\max\\left\\{0,z\\right\\}$$\n",
    "\n",
    "its derivative is:\n",
    "\n",
    "$$g'(z) = \\begin{cases} 0 & \\text{if } z<0 \\\\ 1 & \\text{if } z>0 \\\\ \\text{undefined} & \\text{if } z=0 \\end{cases}$$\n",
    "\n",
    "When we implement the ReLU derivative in a computer, we can choose to assign either value 1 or 0 to the point $z=0$. The main advantage of the rectifier over the sigmoidal activation function is that it leads to fewer [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problems, since its derivative takes value 1, which is not close to zero for a large number of $z$ points, and rarely takes value 0 because the $z$ points will often be positive in a neural network, allowing the learning phase to speed up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6jqJ8G4prXe"
   },
   "outputs": [],
   "source": [
    "# load imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liISkGMUprXn",
    "outputId": "92c2ba8e-42c2-4153-ebdf-a7146b454913"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rectifier(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "z = np.arange(-6, 6, 0.1)\n",
    "g_z = rectifier(z)\n",
    "plt.plot(z, g_z)\n",
    "#plt.axvline(x=0.0, color='k', alpha=0.5)\n",
    "#plt.axhline(y=0.0, color='k', alpha=0.5)\n",
    "plt.title('Rectifier function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$g (z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLd102G_prXu"
   },
   "source": [
    "Applying this function to the output of a linear transformation yields a nonlinear function. However, the function remains close to be linear, in the sense that it is a **piecewise linear function**.\n",
    "\n",
    "Regarding the choice of the activation function, some properties are desirable:\n",
    "- **Nonlinear**: using a nonlinear activation function, then a two-layer neural network can be proven to be a universal function approximator ([Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)). The identity activation function $f(z)=z$ doesn't satisfy this property. When multiple layers use the identity activation function, then the entire network can be reduced to a single-layer model.\n",
    "- **Continuously differentiable**: in order to enable gradient-based optimization methods. For example, the binary step function (used by the Perceptron) is not differentiable at 0, so gradient methods can make no progress with it. The ReLU is also non-differentiable at 0, anyway it is differentiable anywhere else and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.\n",
    "- **Monotonic**: when the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.\n",
    "- **Smooth functions with a monotonic derivative**: these have been shown to generalize better in some cases.\n",
    "- **Approximates identity near the origin**: when activation functions have this property, the neural network will learn efficiently when its weights are initialized at small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.\n",
    "\n",
    "When building a neural network, avoid using the sigmoid activation function, except for the output layer if we are doing binary classification, since the hyperbolic tangent $g(z) = tanh(z)$ is strictly superior to it. The standard activation function which is typically used is the ReLU or, in some cases, the leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdRSnRhbprXv"
   },
   "source": [
    "# Multi-layer networks\n",
    "The term *network* comes to the fact that multiple functions are composed together in a model resulting into a **directed acyclic graph**. For example, we may have 3 functions $f^{(1)}, f^{(2)}, f^{(3)}$ composed together in a chain to form $f(\\mathbf{x}) = f^{(3)}\\left( f^{(2)} \\left( f^{(1)}(\\mathbf{x})\\right)\\right)$. The *i*-th function is called the *i*-th **layer** and the length of the chain gives the **depth** of the model. The final layer is the **output layer**. The training data provides us noisy examples of $f^*(\\mathbf{x})$ evaluated at different points. Those training examples specify what the output layer should produce for each input $\\mathbf{x}$, anyway, they don't specify what the behavior of the intermediate layers should be. It is the task of the learning algorithm to regulate their behaviour. Because the training data doesn't show the desired output of each of these layers, they are called **hidden layers**. A layer can be thought either as a vector-to-vector function or as many units acting in parallel, each representing a vector-to-scalar function (a single neuron).\n",
    "\n",
    "Linear models, such as linear regression or logistic regression can be fit efficiently, either in closed form or with convex optimization. Anyway, they have the defect to limit the model capacity to linear functions. In order to extend its capacity to nonlinear functions of $\\mathbf{x}$, instead of applying the linear model to $\\mathbf{x}$, we can apply it to a transformed input $g(\\mathbf{x})$ where $g$ is a nonlinear function defining a hidden layer and providing a new representation of $\\mathbf{x}$. In feedforward neural nets we have a model $y=f(x;\\theta, w) = g(x;\\theta)^\\top w$ where the parameters $\\theta$ are used to learn $g$ from a broad class of functions (that class has to be chosen and this is not trivial at all).\n",
    "\n",
    "Similarly to linear models, the optimizer, the cost function and the form of the output units still still have to be chosen. Furthermore, since feedforward networks introduced the concept of hidden layers, we must also choose the activation functions that will be used to compute the weights of the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcsHYGapprXx"
   },
   "source": [
    "The kind of functions that can be learned depends on the width and depth of the network:\n",
    "- **Boolean functions:** every boolean function can be represented with a 2 layers network (1 hidden layer + 1 output layer, where the output layer consists of one unit with boolean output). In the worst case the number of units can be exponential in the number of inputs.\n",
    "- **Continuous function:** Every bounded function can be approximated with arbitrarily small error by a network with two layers (1 hidden layer + 1 output layer, where the output is unthresholded) of units. The number of hidden units depends on the function.\n",
    "- **Arbitrary functions:** Any function can be approximated to arbitrary accuracy by a network with three layers (2 hidden layers + 1 output layer) of units and unthresholded output. The number of required units is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E5m5mumprXx"
   },
   "source": [
    "## Learning a XOR function\n",
    "The truth table of the XOR function is:\n",
    "\n",
    "| x1 | x2 | XOR |\n",
    "|:--:|:--:|:---:|\n",
    "|  0 |  0 |  0  |\n",
    "|  0 |  1 |  1  |\n",
    "|  1 |  0 |  1  |\n",
    "|  1 |  1 |  0  |\n",
    "\n",
    "We can treat this problem as a regression problem using the MSE as the loss function (in general, we should not use MSE as a cost function for modelling binary data, but prefer the _logit_ function as we've seen in the chapter about logistic regression).\n",
    "\n",
    "$$J(\\theta) = \\sum_{x\\in \\mathbb{X}}{\\left(f^*(x) - f(x;\\theta) \\right)^2}$$\n",
    "\n",
    "If we choose $f(x;\\theta)$ to be a linear model $f(x;\\theta) = \\mathbf{w}^\\top x + b$ with $\\mathbf{w}\\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$, and solve the normal equations, we would end up with $\\mathbf{w} = \\mathbf{0}$ and $b=\\frac{1}{2}$, that is a model that always predict $y = \\frac{1}{2}$. We have to use a nonlinear model that is able to learn a different feature space in which a linear model can represent the solution.\n",
    "\n",
    "Let's then introduce an intermediate layer containing two units resulting in a vector of hidden units $h$ that are computed by $h = f^{(1)}\\left(x; W,c\\right)$. The output layer will be a linear regression model $y = f^{(2)}\\left(h; w, b\\right) = w^\\top h + b$, but acting on $h$ instead of $x$, a representation of the input in a new feature space. The entire network is represented by $f\\left(\\mathbf{x}; \\mathbf{W},\\mathbf{c},\\mathbf{w},b\\right) = f^{(2)}\\left(f^{(1)}(\\mathbf{x})\\right)$ where $f^{(1)}$ has to be nonlinear, otherwise the network would result in a linear function of the input $x$, being unable to learn a XOR function.\n",
    "\n",
    "As we've seen before, a ReLU unit might come to our aid by combining the input in an affine trasformation (obtaining the net input) that will then be fed to a nonlinear function (the rectifier). Then, let $h=g\\left(W^\\top x + c\\right)$ where $W$ is a matrix containing the weights of the linear trasformation and $c$ the biases (which is a vector, because the affine transformation is from a vector $x$ to a vector $h$). The activation function is typically appliable element-wise, with $h_i = g\\left(x^\\top W_{:,i} + c_i \\right)$, we'll use the rectifier activation function $g(z)=\\max\\left\\{0,z\\right\\}$. The full network is:\n",
    "\n",
    "$$f\\left(\\mathbf{x}; \\mathbf{W},\\mathbf{c},\\mathbf{w},b\\right) = \\mathbf{w}^\\top \\max\\left\\{0,\\mathbf{W}^\\top \\mathbf{x} + \\mathbf{c}\\right\\} + b $$\n",
    "\n",
    "and it has the following structure:\n",
    "\n",
    "<img src=\"images/neural_networks/XOR_net.png\" style=\"width:15em; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "Now we can manually specify the solution to the XOR problem letting:\n",
    "\n",
    "$$W = \\begin{bmatrix}1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\qquad c = \\begin{bmatrix}0\\\\ -1 \\end{bmatrix} \\qquad  w=\\begin{bmatrix}1\\\\ -2 \\end{bmatrix} \\qquad b=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvaxKLPRprXy",
    "outputId": "a302b36e-58a0-409f-c47c-4a7020199a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[[0 0]], net_input=[[ 0. -1.]], h=[[0. 0.]], y=0.0\n",
      "x=[[0 1]], net_input=[[1. 0.]], h=[[1. 0.]], y=1.0\n",
      "x=[[1 0]], net_input=[[1. 0.]], h=[[1. 0.]], y=1.0\n",
      "x=[[1 1]], net_input=[[2. 1.]], h=[[2. 1.]], y=0.0\n"
     ]
    }
   ],
   "source": [
    "# inputs and parameters\n",
    "X = np.matrix([(0,0),(0,1),(1,0),(1,1)])\n",
    "W = np.ones((2,2))\n",
    "c = np.array([0,-1])\n",
    "w = np.array([1,-2])\n",
    "b = 0\n",
    "\n",
    "# computations of the network\n",
    "net_input = X @ W + c\n",
    "h = rectifier(net_input)\n",
    "y = h.dot(w).T + b\n",
    "\n",
    "# visualize outputs\n",
    "for x, combination, activation, output in zip (X,net_input, h, y):\n",
    "    print('x={}, net_input={}, h={}, y={}'.format(x, combination, activation, output.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3RhXEzSprX3"
   },
   "source": [
    "In real problems, instead of manually choosing parameters values, they are found by a gradient-based optimization algorithm.\n",
    "\n",
    "In the following plots we can see how the inputs are mapped from the original feature space, in which they weren't linearly separable, to a new feature space by the hidden layer computation, where they can be perfectly separated by a suitable linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWCghCeprX5",
    "outputId": "07c0a77a-54fc-49d8-de74-ad13afa9f313"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize feature spaces\n",
    "X1 = np.array(X[:,0])\n",
    "X2 = np.array(X[:,1])\n",
    "h1 = np.array(h[:,0])\n",
    "h2 = np.array(h[:,1])\n",
    "y = np.array(y)\n",
    "markers = ('o', '+')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "    ax1.scatter(X1[y == cl], X2[y == cl], marker = markers[idx], label='y = {}'.format(int(cl)))\n",
    "    ax2.scatter(h1[y == cl], h2[y == cl], marker = markers[idx], label='y = {}'.format(int(cl)))\n",
    "\n",
    "margin = 0.2\n",
    "ax1.set_title('X feature space')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.set_xticks([0,1])\n",
    "ax1.set_yticks([0,1])\n",
    "ax1.set_xlim([0-margin, 1+margin])\n",
    "ax1.set_ylim([0-margin, 1+margin])\n",
    "\n",
    "ax2.set_title('h feature space')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "ax2.set_xticks([0,1,2])\n",
    "ax2.set_yticks([0,1])\n",
    "ax2.set_xlim([0-margin, 2+margin])\n",
    "ax2.set_ylim([0-margin, 1+margin])\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buZXjIRVprX9"
   },
   "source": [
    "## Visual interpretation\n",
    "The goal of the following explanation is to provide an insight on what is the meaning of composing multiple neurons into a network, without any mathematical rigor. In order to obtain a nonlinear model we can take, for example, two sigmoid units, each of which assigns a probability to each data point, and sum those probabilities together, point by point, obtaining new values that have to be rescaled back between 0 and 1 (and that can be done with another sigmoid function). If we want to give more importance to one of the two units, we can take a linear combination of the two, instead of simply summing them together, and we also add a bias term (otherwise the resulting value would always be positive and, if fed to a sigmoid unit, would result in always being classified as positive).\n",
    "\n",
    "<img src=\"images/neural_networks/visual_network.png\" style=\"width:42em; display: block; margin-left: auto; margin-right: auto;\" />\n",
    "\n",
    "which is equivalent this neural network:\n",
    "\n",
    "<img src=\"images/neural_networks/visual_network_2.jpg\" style=\"width:20em; display: block; margin-left: auto; margin-right: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcCcgzKmprX_"
   },
   "source": [
    "## Generalization and vectorized notation\n",
    "From now on, I will use the following [notation](https://cs230.stanford.edu/files/Notation.pdf):\n",
    "- superscript $(i)$: *i*-th training example\n",
    "- superscript $[l]$: *l*-th layer\n",
    "- $m$: number of examples in the dataset\n",
    "- $n_x$: number of input units\n",
    "- $n_{h}^{[l]}$: number of units in the *l*-th hidden layer\n",
    "- $n_y$: number of output units (or number of classes)\n",
    "- $L$: number of layers in the network\n",
    "- $x^{(i)}\\in \\mathbb{R}^{n_x}$ is the i-th training example represented as a column vector\n",
    "- $X\\in \\mathbb{R}^{n_x \\times m}$: input matrix (the *i*-th column correspond to the i-th training example)\n",
    "- $Y\\in \\mathbb{R}^{n_y \\times m}$ is the label matrix\n",
    "- $y^{(i)}\\in \\mathbb{R}^{n_y}$ is the output label for the *i*-th example\n",
    "- $b^{[l]}\\in \\mathbb{R}^{n^{[l+1]}}$: bias vector in the *l*-th layer\n",
    "- $\\hat{y}\\in \\mathbb{R}^{n_y}$: predicted output vector\n",
    "- $w_{ji}^{[l]}$: single weight of the *l*-th layer, from unit i into unit j\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l+1]} \\times n^{[l-1]}}$: the weight matrix associated with the *l*-th layer\n",
    "- $a_j^{[l]}$: the output of the *j*-th neuron of the l-th hidden layer\n",
    "- $g^{[l]}$: the activation function of the *l*-th layer\n",
    "\n",
    "### Example\n",
    "Consider this network with $n_x=3$ inputs in the input layer, 1 hidden layer with $n_h^{[1]}=2$ units and an output layer with $n_y=1$ output unit:\n",
    "\n",
    "<img src=\"images/neural_networks/notation.png\" style=\"width:25em; display: block; margin-left: auto; margin-right: auto;\" />\n",
    "\n",
    "where \n",
    "\n",
    "$$W^{[1]}= \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix}, \\qquad \n",
    "W^{[2]}= \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix}, \\qquad\n",
    "a^{[1]} = \\begin{bmatrix} a_{1}^{[1]} \\\\ a_{2}^{[1]} \\end{bmatrix}, \\qquad\n",
    "a^{[2]} = \\begin{bmatrix} a_1^{[2]} \\end{bmatrix}$$\n",
    "\n",
    "In this representation the bias terms are omitted and they have to be considered implicitly inside each neuron. For a single input data $x^{(i)}$ the computation will proceed as follows:\n",
    "\n",
    "$$\n",
    "a^{[1](i)} = g\\left(W^{[1]}x^{(i)} + b^{[1]} \\right), \\qquad\n",
    "\\hat{y} = a^{[2](i)} = g\\left(W^{[2]}a^{[1](i)} + b^{[2]} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We can use a vectorized notation to represent $m$ input data $x^{(i)}$ inside $X$:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "| & | & | \\\\\n",
    "x^{(1)} & \\dots & x^{(m)} \\\\\n",
    "| & | & | \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times m}\n",
    "$$\n",
    "\n",
    "The computation will proceed in this way:\n",
    "\n",
    "$$\n",
    "A^{[1]} = g\\left(W^{[1]}X + b^{[1]} \\right) \\in \\mathbb{R}^{2\\times m}, \\qquad\n",
    "\\hat{y} = A^{[2]} = g\\left(W^{[2]}A^{[1]} + b^{[2]} \\right) \\in \\mathbb{R}^{1\\times m}\n",
    "$$\n",
    "\n",
    "**Note:** this is a slightly imprecise notation, since dimensions of $W^{[i]}A^{[i-1]}$ and $b^{[i]}$ are not compatible. We have to think at the vector $b^{[i]}$ being summed to each of the columns of the matrix $W^{[i]}A^{[i-1]}$. This is called **array broadcasting** and [numpy is able to handle that](https://numpy.org/doc/stable/user/theory.broadcasting.html#array-broadcasting-in-numpy) for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxqZMDPCprX_"
   },
   "source": [
    "# The Backpropagation Algorithm\n",
    "\n",
    "The main difference between linear models and neural networks is that the nonlinearity of neural network causes the loss function to become **non-convex**. Consequently, stochastic gradient descent applied to such type of functions has not convergence guarantee and is sensitive to the initial values of the parameters. Sticking to the previous example, the cost function will be:\n",
    "\n",
    "$$J\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} \\right) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(\\hat{y}, y\\right)$$\n",
    "\n",
    "where, $\\mathcal{L}$ is the *loss function* that, in case of a binary classification problem, can be the logistic cost function (or log-likelihood function) $\\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) = {-y^{(i)}\\ln{\\hat{y}^{(i)}} - (1-y^{(i)})\\ln{\\left(1-\\hat{y}^{(i)}\\right)}}$ that we already defined in the chapter about logistic regression, and $\\hat{y}^{(i)} = a^{[2]}$ is the output predicted by the network associated to the *i*-th input.\n",
    "\n",
    "Proceed to minimize $J$ with the gradient descent algorithm that we already know, using **backpropagation** or **reverse mode automatic differentiation (autodiff)**, in order to compute the gradients of the loss function: \n",
    "\n",
    "1. Inizialize weights to small random values.\n",
    "\n",
    "2. Repeat until the *termination condition* is met:  \n",
    "    - compute the prediction $\\hat{y}^{(i)}$ for each input ($i=1,...,m$)\n",
    "    - compute the gradient $\\nabla J=\\left(\\frac{\\partial J}{\\partial W^{[1]}}, \\frac{\\partial J}{\\partial b^{[1]}}, \\frac{\\partial J}{\\partial W^{[2]}}, \\frac{\\partial J}{\\partial b^{[2]}} \\right)$ of the cost function with respect to the parameters. Let then be: $\\text{d}W^{[1]}=\\frac{\\partial J}{\\partial W^{[1]}}$, $\\text{d}b^{[1]}=\\frac{\\partial J}{\\partial b^{[1]}}$, $\\text{d}W^{[2]}=\\frac{\\partial J}{\\partial W^{[2]}}$, $\\text{d}b^{[2]}=\\frac{\\partial J}{\\partial b^{[2]}}$.\n",
    "    - update each parameter value according to the learning rate $\\eta$:  \n",
    "        - $W^{[1]} \\leftarrow W^{[1]} - \\eta \\text{d}W^{[1]}$\n",
    "        - $b^{[1]} \\leftarrow b^{[1]} - \\eta \\text{d}b^{[1]}$\n",
    "        - $W^{[2]} \\leftarrow W^{[2]} - \\eta \\text{d}W^{[2]}$\n",
    "        - $b^{[2]} \\leftarrow b^{[2]} - \\eta \\text{d}b^{[2]}$\n",
    "\n",
    "Possible **termination conditions** are:\n",
    "- maximum number of iterations (also called **epochs**)\n",
    "- error below a threshold\n",
    "- error on a separate validation set below some threshold\n",
    "\n",
    "In practice, it is quite common to use stochastic or mini-batch gradient descent. Therefore, at each epoch, we may want to sample a single instances or a batch of fixed size of instances from the training set. In that case, the cost function has to be computed according to the method used (average over the number of training sample in the batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaXqwRElprYB"
   },
   "source": [
    "The tricky part is how to compute these partial derivatives because $\\hat{y}^{(i)}$ is obtained by a combination of functions. In order to compute them, we need to use the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) of calculus.\n",
    "\n",
    "$\\text{d}a^{[2]} = \\frac{\\partial J}{\\partial a^{[2]}} = \\frac{\\partial J}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}}$\n",
    "\n",
    "Assuming that $\\sigma(z^{[2]})$ is a sigmoid function, then its derivative with respect to $z^{[2]}$ is $\\sigma(z^{[2]})\\left(1-\\sigma(z^{[2]})\\right) = a^{[2]}\\left(1-a^{[2]}\\right) $.\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\text{d}z^{[2]} = \\frac{\\partial J}{\\partial z^{[2]}} = \\frac{\\partial J}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \n",
    "&= \\text{d}a^{[2]} \\cdot \\sigma(z^{[2]})\\left(1-\\sigma(z^{[2]})\\right) \\\\\n",
    "&= \\text{d}a^{[2]} \\cdot a^{[2]}\\left(1-a^{[2]}\\right) \\\\\n",
    "&= \\left( -\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}} \\right) a^{[2]}\\left(1-a^{[2]}\\right) \\\\\n",
    "&= a^{[2]} - y\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\text{d}W^{[2]} = \\frac{\\partial J}{\\partial W^{[2]}} = \\frac{\\partial J}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial W^{[2]}} &= \\text{d}z^{[2]} a^{[1]\\top} \\\\\n",
    "&= \\left(a^{[2]} - y\\right) a^{[1]\\top}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{d}b^{[2]} = \\frac{\\partial J}{\\partial b^{[2]}} = \\frac{\\partial J}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial b^{[2]}} = \\text{d}z^{[2]} = a^{[2]} - y\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{d}a^{[1]} = \\frac{\\partial J}{\\partial a^{[1]}} = \\frac{\\partial J}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} = W^{[2]\\top} \\text{d}z^{[2]} = W^{[2]\\top} \\left(a^{[2]} - y\\right)\n",
    "$\n",
    "\n",
    "Assuming that $g$ is a generic activation function:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\text{d}z^{[1]} = \\frac{\\partial J}{\\partial z^{[1]}} = \\frac{\\partial J}{\\partial a^{[1]}} \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} &= \\text{d}a^{[1]} * g' \\left(z^{[1]}\\right) \\\\\n",
    "&= W^{[2]\\top} \\text{d}z^{[2]} * g' \\left(z^{[1]}\\right)\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "where $*$ is an element-wise product.\n",
    "\n",
    "$\n",
    "\\text{d}W^{[1]} = \\frac{\\partial J}{\\partial W^{[1]}} = \\frac{\\partial J}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial W^{[1]}} = \\text{d}z^{[1]} x^\\top\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{d}b^{[1]} = \\frac{\\partial J}{\\partial b^{[1]}} = \\frac{\\partial J}{\\partial z^{[1]}} \\frac{\\partial z^{[1]}}{\\partial b^{[1]}} = \\text{d}z^{[1]}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX6bMwXyprYB"
   },
   "source": [
    "## Backpropagation with vectorized input\n",
    "Now let's consider the input training examples all at once in the input matrix $X\\in \\mathbb{R}^{3\\times m}$ (please, note that, in practice, we rarely will consider the entire training set all at once, but we will consider mini batches of training instances). The computations of the hidden layer become $Z^{[1]}=W^{[1]} X + b^{[1]}$ and $A^{[1]}=g\\left(Z^{[1]}\\right)$. \n",
    "\n",
    "The partial derivatives become:\n",
    "\n",
    "- $\\text{d}Z^{[2]} = A^{[2]} - Y$\n",
    "- $\\text{d}W^{[2]} = \\frac{1}{m} \\text{d}Z^{[2]} A^{[1]\\top} $\n",
    "- $\\text{d}b^{[2]} = \\frac{1}{m} \\texttt{np.sum}\\left(\\text{d}Z^{[2]}, \\texttt{axis}=1, \\texttt{keepdims}=True \\right)$\n",
    "- $\\text{d}Z^{[1]} = W^{[2]\\top} \\text{d}Z^{[2]} * g' \\left(Z^{[1]}\\right)$ (in which $*$ is the-element wise product between element of two matrices with dimension $n^{[1]}\\times m$)\n",
    "- $\\text{d}W^{[1]} = \\frac{1}{m} \\text{d}Z^{[1]} X^\\top $\n",
    "- $\\text{d}b^{[1]} = \\frac{1}{m} \\texttt{np.sum}\\left(\\text{d}Z^{[1]}, \\texttt{axis}=1, \\texttt{keepdims}=True \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ucutimUprYD"
   },
   "source": [
    "## What happens in each layer?\n",
    "More generally, at every iteration, during the *forward propagation*, the *l*-th layer receive as input and compute as output these quantities:\n",
    "- **Input:** \n",
    "    - $A^{[l-1]}$\n",
    "- **Output:**  \n",
    "    - $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\\quad$ (this is a sort of cached quantity that will return useful during backpropagation)\n",
    "    - $A^{[l]} = g^{[l]}\\left(Z^{[l]}\\right)$\n",
    "    \n",
    "During the *backpropagation* process, the *l*-th layer receive as input and compute as output these quantities:\n",
    "- **Input:** \n",
    "    - $\\text{d}A^{[l]}$\n",
    "- **Output:**  \n",
    "    - $\\text{d}Z^{[l]} = \\text{d}A^{[l]} * {g'}^{[l]} \\left(Z^{[l]}\\right)\\quad$ (where $*$ stands for the element wise product)\n",
    "    - $\\text{d}W^{[l]} = \\frac{1}{m} \\text{d}Z^{[l]} \\cdot A^{[l-1]\\top}$\n",
    "    - $\\text{d}b^{[l]} = \\frac{1}{m} \\texttt{np.sum}\\left(\\text{d}Z^{[l]}, \\texttt{axis}=1, \\texttt{keepdims}=True\\right)$\n",
    "    - $\\text{d}A^{[l-1]} = W^{[l]\\top} \\cdot \\text{d}Z^{[l]}$\n",
    "    \n",
    "<img src=\"images/neural_networks/iteration.png\" style=\"width:18em; display: block; margin-left: auto; margin-right: auto;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjKrdEdmprYD"
   },
   "source": [
    "## Initialization\n",
    "Weights of a neural network should not be initialized to zero, but to small random number. I'll try to illustrate it with an example. Consider this 2-layers neural network.\n",
    "\n",
    "<img src=\"images/neural_networks/initialization.png\" style=\"width:25em; display: block; margin-left: auto; margin-right: auto;\" />\n",
    "\n",
    "Suppose that the weights are initialized at zero:\n",
    "\n",
    "$$W^{[1]} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}, \\qquad b^{[1]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\qquad W^{[2]} = \\begin{bmatrix} 0 & 0  \\end{bmatrix}, \\qquad b^{[2]} = 0$$\n",
    "\n",
    "Initializing $b^{[i]}$ at zero is not a problem, but it is a problem if $W^{[i]}$ is. The first thing we may notice is that $a_1^{[1]} = a_2^{[1]}$. When computing backpropagation, it will happen that also $\\text{d}z_1^{[1]} = \\text{d}z_2^{[1]}$ and so, both the units of the hidden layer will stay always equal. This can be proven by induction since $\\text{d}W^{[1]}$, at every iteration, will look like:\n",
    "\n",
    "$$\\text{d}W^{[1]} = \\begin{bmatrix} u & v \\\\ u & v \\end{bmatrix}$$\n",
    "\n",
    "leaving $W^{[1]}$ being a matrix with two identical rows. For this reason, having more than 1 hidden unit wouldn't make any sense, since they would all be computing the same value, no matter how long we let gradient descent run.\n",
    "\n",
    "We can randomly initialize the weights in this way:\n",
    "\n",
    "```python\n",
    "W1 = np.random.randn((2,2)) * 0.01\n",
    "b1 = np.zeros((2,1))\n",
    "W2 = np.random.randn((1,2)) * 0.01\n",
    "b2 = 0\n",
    "```\n",
    "\n",
    "The reason why it is preferrable to initialize weights to small values rather than large ones, is that if we're using an activation function like the sigmoid or the hyperbolic tangent, then is better if its input (a weighted sum) is not too large, otherwise we would find ourselves in a region of the activation function where its slope (its derivative) is close to zero, causing the convergence of the gradient descent process to slow down (since gradient descent would take very short steps towards a minimum point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZeG8eSmprYE"
   },
   "source": [
    "## Best practices\n",
    "Backpropagation can fail in some cases, for example:\n",
    "- **Vanishing gradients**: the gradients for the layers closer the input can become very small, near zero, leading to the multiplication of many small terms between them. If that's the case, then the training process for these layers become very slow. Using **ReLU** units instead of sigmoid units **can help prevent vanishing gradients**.\n",
    "- **Exploding gradients**: this is the opposite problem, in which the gradients of the lower layers can become very large  not being able to converge. Lowering the learning rate or adopting [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) can help prevent exploding gradients.\n",
    "- **Dead ReLU neurons**: if the input (which is a weighted sum) to a ReLU unit falls under 0, then the unit can get stuck because it keeps outputting 0, contributing nothing to the network's final prediction and gradients not being able to flow through it during backpropagation. By cutting off some gradients, the input to the ReLU unit may not ever change enough to bring the weighted sum back above 0. Lowering the learning rate can prevent the death of ReLU units.\n",
    "- **Dropout regularization**: [dropout regularization](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) is a form of regularization which is useful for neural networks. It works by removing a random selection of a fixed number of a network layer's units for a single gradient step. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. A regularization factor of 0 correspond to no regularization at all, while a regularization factor of 1 correspond to dropping out every unit, in this case the model learns nothing at all. So, we will choose values between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDl7-AMNprYE"
   },
   "source": [
    "### Euristics to escape local minima\n",
    "\n",
    "There exist several euristics that can be used to avoid getting stuck in local minima. For example we can include a **weight momentum** in the weight update:\n",
    "\n",
    "$$W^{[1]} \\leftarrow W^{[1]} - \\eta \\text{d}W^{[1]} - \\alpha \\text{d}W^{[1]}(t-1)$$\n",
    "\n",
    "where the last term $\\text{d}W^{[1]}(t-1)$ represents the weight update term of the previous iteration, rescaled by a factor $\\alpha$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6.1 - Feedforward Neural Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
