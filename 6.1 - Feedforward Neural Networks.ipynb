{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "## Contents\n",
    "1. Introduction  \n",
    "    1.1. The sigmoid unit  \n",
    "    1.2. The Rectified Linear Unit (ReLU)  \n",
    "2. Multi-layer neural networks  \n",
    "    2.1. Learning a XOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A **feedforward neural network**, also called **multilayer perceptron** (MLP), is a type of **artificial neural network** (ANN) wherein connections between nodes do not form a cycle (differently from its descendant: the *recurrent neural networks*). Its goal is to approximate some function $f^*$ by defining a mapping $y=f(x;w)$ and learning the value of the parameters $w$ that result in the best approximation. It is called feedforward because information flows from the input layer $x$, through the intermediate layers, to the output $y$, without any *feedback* connections where the outputs are fed back into the network. Before talking about networks, we have to introduce the units that will make up the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid unit\n",
    "Let's first introduce a \"building block\" for our neural network: the **sigmoid unit**. It is simply an artificial neuron in which the activation function is a sigmoid function (which has already been introduced in the chapter on Logistic Regression).\n",
    "\n",
    "<img src=\"images/neural_networks/sigmoid_unit.jpg\" style=\"width:60%\"/>\n",
    "\n",
    "The sigmoid function is monotonic and continuously differentiable, so we can use gradient descent methods to \"train\" a single sigmoid unit and an algorithm called **backpropagation** to train a network of units.\n",
    "\n",
    "The cost function is, again:\n",
    "\n",
    "$$J(w)=\\frac{1}{2}\\sum_{i}{\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right)^2}$$\n",
    "\n",
    "$\\phi(z)$ is the sigmoid function and its derivative is $\\frac{\\partial}{\\partial z}{\\phi(z)} = \\phi(z)\\left(1-\\phi(z)\\right)$.\n",
    "\n",
    "The derivative of $J$ is then:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2}\\sum_{i}{\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right) \\right)^2} \\\\\n",
    "    &= \\frac{1}{2} \\sum_{i} 2\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\frac{\\partial}{\\partial w_j}\\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\\\\n",
    "    &= \\sum_{i} \\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\left(- \\frac{\\partial}{\\partial w_j} \\phi\\left(z^{(i)}\\right) \\right) \\\\\n",
    "    &= \\sum_{i} \\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\phi(z^{(i)})\\left(1-\\phi(z^{(i)})\\right) \\left(- \\frac{\\partial}{\\partial w_j} z^{(i)}\\right) \\\\\n",
    "    &= -\\sum_{i} \\left(y^{(i)}-\\phi\\left(z^{(i)}\\right)\\right) \\phi(z^{(i)})\\left(1-\\phi(z^{(i)})\\right) x_j^{(i)}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "This is the objective function to be minimized for a single neuron. In a multi-layer network, the inputs to each neuron depends from the outputs produced by the neurons of the previous layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rectified Linear Unit\n",
    "There are plenty of [activation functions](https://en.wikipedia.org/wiki/Activation_function) that can be used. One that's typically used is the **rectifier**, used to build ReLU (Rectified Linear Unit). \n",
    "\n",
    "$$\\phi(z)=\\max\\left\\{0,z\\right\\}$$\n",
    "\n",
    "Its main advantage over the sigmoidal activation function is that it leads to fewer [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rectifier(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "z = np.arange(-6, 6, 0.1)\n",
    "phi_z = rectifier(z)\n",
    "plt.plot(z, phi_z)\n",
    "#plt.axvline(x=0.0, color='k', alpha=0.5)\n",
    "#plt.axhline(y=0.0, color='k', alpha=0.5)\n",
    "plt.title('Rectifier function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\phi (z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this function to the output of a linear transformation yelds a nonlinear function. However, the function remains close to be linear, in the sense that it is a **piecewise linear function**.\n",
    "\n",
    "Regarding the choice of the activation function, some properties are desirable:\n",
    "- **Nonlinear**: using a nonlinear activation function, then a two-layer neural network can be proven to be a universal function approximator ([Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)). The identity activation function $f(z)=z$ doesn't satisfy this property. When multiple layers use the identity activation function, then the entire network can be reduced to a single-layer model.\n",
    "- **Continuously differentiable**: in order to enable gradient-based optimization methods. For example the binary step function (used by the Perceptron) is not differentiable at 0, so gradient methods can make no progress with it. The ReLU is also non-differentiable at 0, anyway it is differentiable anywhere else and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.\n",
    "- **Monotonic**: when the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.\n",
    "- **Smooth functions with a monotonic derivative**: these have been shown to generalize better in some cases.\n",
    "- **Approximates identity near the origin**: when activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer networks\n",
    "The term *network* comes to the fact that multiple functions are composed together in a model resulting into a directed acyclic graph. For example we may have 3 functions $f^{(1)}, f^{(2)}, f^{(3)}$ composed together in a chain to form $f(\\mathbf{x}) = f^{(3)}\\left( f^{(2)} \\left( f^{(1)}(\\mathbf{x})\\right)\\right)$. The i-th function is called the i-th **layer** and the length of the chain gives the **depth** of the model. The final layer is the **output layer**. The training data provides us noisy examples of $f^*(\\mathbf{x})$ evaluated at different points. Those training examples specify what the output layer should produce for each input $\\mathbf{x}$, anyway, they don't specify what the behavior of the intermediate layers should be. It is the task of the learning algorithm to regulate their behaviour. Because the training data doesn't show the desired output of each of these layers, they are called **hidden layers**. A layer can be thought either as a vector-to-vector function or as many units acting in parallel, each representing a vector-to-scalar function (a single neuron).\n",
    "\n",
    "Linear models, such as linear regression or logistic regression can be fit efficiently, either in closed form or with convex optimization. Anyway, they have the defect to limit the model capacity to linear functions. In order to extend its capacity to nonlinear functions of $\\mathbf{x}$, instead of applying the linear model to $\\mathbf{x}$, we can apply it to a transformed input $\\phi(\\mathbf{x})$ where $\\phi$ is a nonlinear function defining a hidden layer and providing a new representation of $\\mathbf{x}$. In feedforward neural nets we have a model $y=f(x;\\theta, w) = \\phi(x;\\theta)^\\top w$ where the parameters $\\theta$ are used to learn $\\phi$ from a broad class of functions (that class has to be chosen and this is not trivial at all).\n",
    "\n",
    "Similarly to linear models, the optimizer, the cost function and the form of the output units still still have to be chosen. Furthermore, since feedforward networks introduced the concept of hidden layers, we must also choose the activation functions that will be used to compute the weights of the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a XOR function\n",
    "The truth table of the XOR function is:\n",
    "\n",
    "| x1 | x2 | XOR |\n",
    "|:--:|:--:|:---:|\n",
    "|  0 |  0 |  0  |\n",
    "|  0 |  1 |  1  |\n",
    "|  1 |  0 |  1  |\n",
    "|  1 |  1 |  0  |\n",
    "\n",
    "We can treat this problem as a regression problem using the MSE as the loss function (in general, we should not use MSE as a cost function for modelling binary data, but prefer the logit function as we've seen in the chapter on logistic regression).\n",
    "\n",
    "$$J(\\theta) = \\sum_{x\\in \\mathbb{X}}{\\left(f^*(x) - f(x;\\theta) \\right)^2}$$\n",
    "\n",
    "If we choose $f(x;\\theta)$ to be a linear model $f(x;\\theta) = \\mathbf{w}^\\top x + b$ with $\\mathbf{w}\\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$, and solve the normal equations, we would end up with $\\mathbf{w} = \\mathbf{0}$ and $b=\\frac{1}{2}$, that is a model that always predict $y = \\frac{1}{2}$. We have to use a nonlinear model that is able to learn a different feature space in which a linear model can represent the solution.\n",
    "\n",
    "Let's then introduce an intermediate layer containing two units resulting in a vector of hidden units $h$ that are computed by $h = f^{(1)}\\left(x; W,c\\right)$. The output layer will be a linear regression model $y = f^{(2)}\\left(h, w, b\\right)$, but acting on $h$ instead of $x$, a representation of the input in a new feature space. The entire network is represented by $y = f^{(2)}\\left(f^{(1)}(x)\\right)$ where $f^{(1)}$ has to be nonlinear, otherwise the network would result in a linear function of the input $x$, being unable to learn a XOR function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
